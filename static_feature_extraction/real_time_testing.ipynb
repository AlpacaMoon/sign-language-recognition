{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Time Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # required to have python 3.9.2 to install mediapipe\n",
    "# ! pip install cvzone -U\n",
    "# ! pip install opencv-python -U\n",
    "# ! pip install numpy -U\n",
    "# ! pip install mediapipe -U\n",
    "# ! pip install tensorflow -U\n",
    "# ! pip install wordninja -U\n",
    "# ! pip install transformer -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Eng Lip\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from time import time\n",
    "from cvzone import FPS\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from time import time\n",
    "\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "from cvzone.FaceDetectionModule import FaceDetector\n",
    "from cvzone.PoseModule import PoseDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Feature Extraction (Hand+Face+Pose Detection)\n",
    "# Flatten a 2d np array into 1d array\n",
    "def flatten2dList(arr, dataType=int):\n",
    "    return np.fromiter(chain.from_iterable(arr), dataType)\n",
    "\n",
    "# Get the largest absolute value in an np array\n",
    "def getAbsLargestVal(arr):\n",
    "    return np.max(np.abs(arr))\n",
    "\n",
    "# Offset and normalize the landmark list\n",
    "# Returns a 1d numpy array\n",
    "def preprocess_landmarks(landmark_list):    \n",
    "    landmark_list = np.array(landmark_list, dtype=float)\n",
    "    origin = landmark_list[0]\n",
    "    \n",
    "    # Offset every point with respect to the first point\n",
    "    # Convert to 1D-array\n",
    "    new_landmark_list = (landmark_list - origin).ravel()\n",
    "    \n",
    "    # Get highest absolute value\n",
    "    largest_value = getAbsLargestVal(new_landmark_list)\n",
    "    \n",
    "    # Normalization\n",
    "    if largest_value != 0:\n",
    "        return new_landmark_list / largest_value\n",
    "    return new_landmark_list\n",
    "\n",
    "# Offset and normalize a BBOX list (BBOX = Bounding Box, used in face and hand detection)\n",
    "# Returns a 1d numpy array\n",
    "def preprocess_bbox(bbox, frameSize):\n",
    "    bbox = np.array(bbox, dtype=float)\n",
    "    # Convert 3rd and 4th element into coordinates instead of width/height\n",
    "    bbox[2] = bbox[0] + bbox[2]\n",
    "    bbox[3] = bbox[1] + bbox[3]\n",
    "\n",
    "    # Normalize against frame size\n",
    "    bbox[0] /= frameSize[0]\n",
    "    bbox[1] /= frameSize[1]\n",
    "    bbox[2] /= frameSize[0]\n",
    "    bbox[3] /= frameSize[1]\n",
    "\n",
    "    return bbox\n",
    "\n",
    "# Normalize a center vertex (a list of 2 elements)\n",
    "# Returns a 1d numpy array\n",
    "def preprocess_center(center, frameSize):\n",
    "    center = np.array(center, dtype=float)\n",
    "    center[0] /= frameSize[0]\n",
    "    center[1] /= frameSize[1]\n",
    "    return center\n",
    "\n",
    "# Preprocess (Offset and normalize) the body's landmark list, bbox and center\n",
    "def preprocess_body_part(bodyPart, frameSize):\n",
    "    bodyPart['lmList'] = preprocess_landmarks(bodyPart['lmList'])\n",
    "    bodyPart['bbox'] = preprocess_bbox(bodyPart['bbox'], frameSize)\n",
    "    bodyPart['center'] = preprocess_center(bodyPart['center'], frameSize)\n",
    "    return bodyPart\n",
    "\n",
    "# Function to generate empty/placeholder data for a hand \n",
    "# Used when a hand is not detected in frame\n",
    "def generate_empty_hand(type):\n",
    "    return {\n",
    "        'lmList': np.zeros(21 * 3, dtype=int), \n",
    "        'bbox': np.zeros(4, dtype=float), \n",
    "        'center': np.zeros(2, dtype=float), \n",
    "        'type': type\n",
    "    }\n",
    "\n",
    "# Select the best matching face, aka the one with the best score (clarity)\n",
    "# and closest to the center of the screen\n",
    "# Since the Neural network will be design to only accept one face\n",
    "def select_best_matching_face(faces, frameSize):\n",
    "    if not faces or len(faces) == 0:\n",
    "        return False\n",
    "    elif len(faces) == 1:\n",
    "        return faces[0]\n",
    "    \n",
    "    def difference(a, b):\n",
    "        return ((a[0] - b[0])**2) + ((a[1] - b[1])**2)\n",
    "    \n",
    "    frameCenter = (frameSize[0] / 2, frameSize[1] / 2)\n",
    "\n",
    "    best_score = faces[0]\n",
    "    best_center = faces[0]\n",
    "    center_diff = difference(faces[0]['center'], frameCenter)\n",
    "\n",
    "    for each in faces:\n",
    "        if difference(each['center'], frameCenter) < center_diff:\n",
    "            best_center = each\n",
    "        if each['score'][0] > best_score['score'][0]:\n",
    "            best_score = each\n",
    "    \n",
    "    if best_center['score'][0] > 0.5:\n",
    "        return best_center\n",
    "    return best_score\n",
    "\n",
    "# Flatten everything\n",
    "def flattenDetectionResult(obj):\n",
    "    # return np.fromiter(chain.from_iterable([obj['lmList'], obj['bbox'], obj['center']]), float)\n",
    "    return np.concatenate([obj['lmList'], obj['bbox'], obj['center']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detects hands, face & pose, \n",
    "# convert them into normalized landmark/keypoint coordinates in a 1D-array, \n",
    "# and also returns the frame with the landmark connections drawn onto it\n",
    "\n",
    "# Improved/Parallelised version\n",
    "def featureExtractionV3(handDetector, faceDetector, poseDetector, frame, draw=True):\n",
    "    def detectHands(handDetector, frame, frameSize, draw):\n",
    "        results = None\n",
    "        # Hand Detection\n",
    "        if (draw):\n",
    "            results, frame = handDetector.findHands(frame, draw=draw)\n",
    "        else:\n",
    "            results = handDetector.findHands(frame, draw=draw)\n",
    "\n",
    "        if not results:\n",
    "            results = [generate_empty_hand('Left'), generate_empty_hand('Right')]\n",
    "        elif len(results) == 1:\n",
    "            if (results[0]['type'] == 'Left'):\n",
    "                results[0] = preprocess_body_part(results[0], frameSize)\n",
    "                results.append(generate_empty_hand('Right'))\n",
    "            else:\n",
    "                results[0] = preprocess_body_part(results[0], frameSize)\n",
    "                results.insert(0, generate_empty_hand('Left'))                         \n",
    "        else:\n",
    "            results[0] = preprocess_body_part(results[0], frameSize)\n",
    "            results[1] = preprocess_body_part(results[1], frameSize)\n",
    "        return results\n",
    "\n",
    "    # Pose Detection\n",
    "    # **We only use the first 23 out of the total 33 landmark points \n",
    "    #   as those represent the lower half body and are irrelevant to sign language interpretation\n",
    "    def detectPose(poseDetector, frame, draw):\n",
    "        frame = poseDetector.findPose(frame, draw=draw)\n",
    "        results, _ = poseDetector.findPosition(frame, bboxWithHands=False)\n",
    "        if results:\n",
    "            results = preprocess_landmarks(results[:23])\n",
    "        else:\n",
    "            results = np.zeros(23, dtype=int)\n",
    "        return results\n",
    "    \n",
    "    # Face Detection\n",
    "    def detectFace(faceDetector, frame, frameSize, draw):\n",
    "        frame, results = faceDetector.findFaces(frame, draw=draw)\n",
    "        if results:\n",
    "            results = select_best_matching_face(results, frameSize)\n",
    "            results['bbox'] = preprocess_bbox(results['bbox'], frameSize)\n",
    "            results['center'] = preprocess_center(results['center'], frameSize)\n",
    "        else:\n",
    "            results = {\n",
    "                'bbox': np.zeros(4, dtype=float), \n",
    "                'center': np.zeros(2, dtype=float)\n",
    "            }\n",
    "        return results\n",
    "\n",
    "    frameSize = (frame.shape[1], frame.shape[0])\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        t1 = executor.submit(detectHands, handDetector, frame, frameSize, draw)\n",
    "        t2 = executor.submit(detectPose, poseDetector, frame, draw)\n",
    "        t3 = executor.submit(detectFace, faceDetector, frame, frameSize, draw)\n",
    "        \n",
    "        # Convert results into 1D-array\n",
    "        detectionResults = flatten2dList([\n",
    "            flattenDetectionResult(t1.result()[0]), \n",
    "            flattenDetectionResult(t1.result()[1]), \n",
    "            t2.result(), \n",
    "            t3.result()['bbox'],\n",
    "            t3.result()['center'],\n",
    "            t3.result()['center'] - t1.result()[0]['center'],\n",
    "            t3.result()['center'] - t1.result()[1]['center']\n",
    "        ], dataType=float)\n",
    "\n",
    "        return detectionResults, frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detects hands, face & pose, \n",
    "# convert them into normalized landmark/keypoint coordinates in a 1D-array, \n",
    "# and also returns the frame with the landmark connections drawn onto it\n",
    "\n",
    "# Improved/Parallelised version\n",
    "def staticFeatureExtraction(handDetector, frame, draw=True):\n",
    "    def detectHands(handDetector, frame, frameSize, draw):\n",
    "        results = None\n",
    "        # Hand Detection\n",
    "        if (draw):\n",
    "            results, frame = handDetector.findHands(frame, draw=draw)\n",
    "        else:\n",
    "            results = handDetector.findHands(frame, draw=draw)\n",
    "\n",
    "        if not results:\n",
    "            results = [generate_empty_hand('Left'), generate_empty_hand('Right')]\n",
    "        elif len(results) == 1:\n",
    "            if (results[0]['type'] == 'Left'):\n",
    "                results[0] = preprocess_body_part(results[0], frameSize)\n",
    "                results.append(generate_empty_hand('Right'))\n",
    "            else:\n",
    "                results[0] = preprocess_body_part(results[0], frameSize)\n",
    "                results.insert(0, generate_empty_hand('Left'))                         \n",
    "        else:\n",
    "            results[0] = preprocess_body_part(results[0], frameSize)\n",
    "            results[1] = preprocess_body_part(results[1], frameSize)\n",
    "        return results\n",
    "\n",
    "    frameSize = (frame.shape[1], frame.shape[0])\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        t1 = executor.submit(detectHands, handDetector, frame, frameSize, draw)\n",
    "        # t2 = executor.submit(detectPose, poseDetector, frame, draw)\n",
    "        # t3 = executor.submit(detectFace, faceDetector, frame, frameSize, draw)\n",
    "        \n",
    "        # Convert results into 1D-array\n",
    "        detectionResults = flatten2dList([\n",
    "            flattenDetectionResult(t1.result()[0]), \n",
    "            flattenDetectionResult(t1.result()[1]), \n",
    "            # t2.result(), \n",
    "            # t3.result()['bbox'],\n",
    "            # t3.result()['center'],\n",
    "            # t3.result()['center'] - t1.result()[0]['center'],\n",
    "            # t3.result()['center'] - t1.result()[1]['center']\n",
    "        ], dataType=float)\n",
    "\n",
    "        return detectionResults, frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Eng Lip\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 136, 32)           128       \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1  (None, 68, 32)            0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 66, 64)            6208      \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPoolin  (None, 33, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 31, 128)           24704     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3968)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               1016064   \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 256)               1024      \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 36)                9252      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1057380 (4.03 MB)\n",
      "Trainable params: 1056868 (4.03 MB)\n",
      "Non-trainable params: 512 (2.00 KB)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " '10']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from files_io import readActionLabels\n",
    "\n",
    "# Load the model from the H5 file\n",
    "# model = keras.models.load_model('../static_recognition/models/static_model_V1.h5')\n",
    "model = keras.models.load_model('../static_recognition/models/static_model.keras')\n",
    "\n",
    "# Print the summary before loading\n",
    "model.summary()\n",
    "\n",
    "static_labels = readActionLabels()\n",
    "static_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "cam = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "cam.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cam.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "# Detectors\n",
    "handDetector = HandDetector(detectionCon=0.5, maxHands=2)\n",
    "faceDetector = FaceDetector(minDetectionCon=0.5)\n",
    "poseDetector = PoseDetector(detectionCon=0.5)\n",
    "fpsReader = FPS.FPS()\n",
    "\n",
    "timeStats = []\n",
    "\n",
    "try:\n",
    "    predictionHistory = deque()\n",
    "    detectionThreshold = 0.999    \n",
    "    predictionCooldown = 0.5\n",
    "    appendCooldown = 1.0\n",
    "    # the first time append should eliminate predictionCooldown\n",
    "    lastAppendTime = time() + appendCooldown\n",
    "    lastDetectTime = time() + predictionCooldown\n",
    "\n",
    "    while True:\n",
    "        startTime = time()\n",
    "\n",
    "        # Read from camera\n",
    "        success, frame = cam.read()\n",
    "        if not success:\n",
    "            raise Exception(\"No Frames Read\")\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # Hand Detection\n",
    "        detectionResults, frame = staticFeatureExtraction(\n",
    "            handDetector, frame)\n",
    "        \n",
    "        # detectionResults, frame = featureExtractionV3(\n",
    "        #     handDetector, faceDetector, poseDetector, frame)\n",
    "        \n",
    "        detectionResults = np.expand_dims(\n",
    "            detectionResults, axis=0) # reshape detection result\n",
    "\n",
    "        if time() <= lastDetectTime + predictionCooldown:\n",
    "            pass\n",
    "        else:\n",
    "            predictionResults = model.predict(\n",
    "                x=detectionResults,\n",
    "                verbose=0,\n",
    "                use_multiprocessing=True,\n",
    "                workers=4\n",
    "            )[0]\n",
    "            lastDetectTime = time()\n",
    "            \n",
    "            # Get predicted character and accuracy\n",
    "            predCharacter = static_labels[np.argmax(predictionResults)]\n",
    "            predAccuracy = predictionResults[np.argmax(predictionResults)]\n",
    "        \n",
    "            cv2.putText(frame, (predCharacter + \":\"+ str(predAccuracy)), (15, 70),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 3)          \n",
    "            \n",
    "            if predAccuracy >= detectionThreshold:            \n",
    "                # If predictionHistory is not empty\n",
    "                # If predCharacter is the same as the last appended character\n",
    "                # Check if 0.5 seconds have passed since the last append\n",
    "                if predictionHistory and predCharacter == predictionHistory[-1] and time() <= lastAppendTime + appendCooldown:\n",
    "                    # Do nothing, don't append\n",
    "                    pass\n",
    "                else:\n",
    "                    predictionHistory.append(predCharacter)\n",
    "                    # Reset the timestamp when a new character is detected\n",
    "                    lastAppendTime = time()\n",
    "\n",
    "                if len(predictionHistory) > 5:\n",
    "                        predictionHistory.popleft()  \n",
    "                        \n",
    "        # cv2.putText(frame, (predCharacter + \":\"+ str(predAccuracy)), (15, 70),\n",
    "        #             cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 3)\n",
    "                        \n",
    "        cv2.putText(frame, str(predictionHistory), (15, 120),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 3)\n",
    "        \n",
    "\n",
    "        fps, frame = fpsReader.update(frame, pos=(\n",
    "            950, 80), textColor=(0, 255, 0), bgColor=(),scale=5, thickness=5)\n",
    "\n",
    "        # fps, frame = fpsReader.update(frame, pos=(\n",
    "        #     950, 80), color=(0, 255, 0), scale=5, thickness=5)\n",
    "\n",
    "        \n",
    "        # Show resulting frame\n",
    "        # cv2.putText(frame, f'Training #{training + 1} for \\'{action}\\'', (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "        cv2.imshow(\"Sign Language Recognition Prototype\", frame)\n",
    "\n",
    "        timeStats.append(time() - startTime)\n",
    "\n",
    "        keyPressed = cv2.waitKey(1)\n",
    "        # Stop Program when pressed 'Esc'\n",
    "        if keyPressed == 27:\n",
    "            raise Exception(\"Finished\")\n",
    "        elif keyPressed == ord('c'):\n",
    "            predictionHistory.clear()\n",
    "            lastAppendTime = time() + predictionCooldown\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(e)\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeStats[10:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
