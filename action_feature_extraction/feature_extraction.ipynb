{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries / Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "import traceback\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvzone.HandTrackingModule import HandDetector\n",
    "from cvzone.FaceDetectionModule import FaceDetector\n",
    "from cvzone.PoseModule import PoseDetector\n",
    "from cvzone import FPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real Time Testing \n",
    "from collections import deque\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Extraction (Hand+Face+Pose Detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten a 2d np array into 1d array\n",
    "def flatten2dList(arr, dataType=int):\n",
    "    return np.fromiter(chain.from_iterable(arr), dataType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the largest absolute value in an np array\n",
    "def getAbsLargestVal(arr):\n",
    "    return np.max(np.abs(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offset and normalize the landmark list\n",
    "# Returns a 1d numpy array\n",
    "def preprocess_landmarks(landmark_list):    \n",
    "    landmark_list = np.array(landmark_list, dtype=float)\n",
    "    origin = landmark_list[0]\n",
    "    \n",
    "    # Offset every point with respect to the first point\n",
    "    # Convert to 1D-array\n",
    "    new_landmark_list = (landmark_list - origin).ravel()\n",
    "    \n",
    "    # Get highest absolute value\n",
    "    largest_value = getAbsLargestVal(new_landmark_list)\n",
    "    \n",
    "    # Normalization\n",
    "    if largest_value != 0:\n",
    "        return new_landmark_list / largest_value\n",
    "    return new_landmark_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offset and normalize a BBOX list (BBOX = Bounding Box, used in face and hand detection)\n",
    "# Returns a 1d numpy array\n",
    "def preprocess_bbox(bbox, frameSize):\n",
    "    bbox = np.array(bbox, dtype=float)\n",
    "    # Convert 3rd and 4th element into coordinates instead of width/height\n",
    "    bbox[2] = bbox[0] + bbox[2]\n",
    "    bbox[3] = bbox[1] + bbox[3]\n",
    "\n",
    "    # Normalize against frame size\n",
    "    bbox[0] /= frameSize[0]\n",
    "    bbox[1] /= frameSize[1]\n",
    "    bbox[2] /= frameSize[0]\n",
    "    bbox[3] /= frameSize[1]\n",
    "\n",
    "    return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize a center vertex (a list of 2 elements)\n",
    "# Returns a 1d numpy array\n",
    "def preprocess_center(center, frameSize):\n",
    "    center = np.array(center, dtype=float)\n",
    "    center[0] /= frameSize[0]\n",
    "    center[1] /= frameSize[1]\n",
    "    return center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess (Offset and normalize) the body's landmark list, bbox and center\n",
    "def preprocess_body_part(bodyPart, frameSize):\n",
    "    bodyPart['lmList'] = preprocess_landmarks(bodyPart['lmList'])\n",
    "    bodyPart['bbox'] = preprocess_bbox(bodyPart['bbox'], frameSize)\n",
    "    bodyPart['center'] = preprocess_center(bodyPart['center'], frameSize)\n",
    "    return bodyPart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate empty/placeholder data for a hand \n",
    "# Used when a hand is not detected in frame\n",
    "def generate_empty_hand(type):\n",
    "    return {\n",
    "        'lmList': np.zeros(21 * 3, dtype=int), \n",
    "        'bbox': np.zeros(4, dtype=float), \n",
    "        'center': np.zeros(2, dtype=float), \n",
    "        'type': type\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best matching face, aka the one with the best score (clarity)\n",
    "# and closest to the center of the screen\n",
    "# Since the Neural network will be design to only accept one face\n",
    "def select_best_matching_face(faces, frameSize):\n",
    "    if not faces or len(faces) == 0:\n",
    "        return False\n",
    "    elif len(faces) == 1:\n",
    "        return faces[0]\n",
    "    \n",
    "    def difference(a, b):\n",
    "        return ((a[0] - b[0])**2) + ((a[1] - b[1])**2)\n",
    "    \n",
    "    frameCenter = (frameSize[0] / 2, frameSize[1] / 2)\n",
    "\n",
    "    best_score = faces[0]\n",
    "    best_center = faces[0]\n",
    "    center_diff = difference(faces[0]['center'], frameCenter)\n",
    "\n",
    "    for each in faces:\n",
    "        if difference(each['center'], frameCenter) < center_diff:\n",
    "            best_center = each\n",
    "        if each['score'][0] > best_score['score'][0]:\n",
    "            best_score = each\n",
    "    \n",
    "    if best_center['score'][0] > 0.5:\n",
    "        return best_center\n",
    "    return best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten everything\n",
    "def flattenDetectionResult(obj):\n",
    "    # return np.fromiter(chain.from_iterable([obj['lmList'], obj['bbox'], obj['center']]), float)\n",
    "    return np.concatenate([obj['lmList'], obj['bbox'], obj['center']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparation for Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "from constants import TRAININGS_PER_LABEL, FRAMES_PER_TRAINING, KEYPOINTS_PER_FRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 'hello',\n",
       " '1': 'good/thank you',\n",
       " '2': 'help',\n",
       " '3': 'I/me',\n",
       " '4': 'please',\n",
       " '5': 'sorry',\n",
       " '6': 'welcome',\n",
       " '7': 'welcome',\n",
       " '8': 'ok',\n",
       " '9': 'what',\n",
       " '10': 'what',\n",
       " '11': 'can',\n",
       " '12': 'thank you very much',\n",
       " '13': 'deaf',\n",
       " '14': 'do not',\n",
       " '15': 'feel',\n",
       " '16': 'eat/food',\n",
       " '17': 'eat a lot',\n",
       " '18': 'tired',\n",
       " '19': 'because',\n",
       " '20': 'sick',\n",
       " '21': 'drink',\n",
       " '22': 'drink',\n",
       " '23': 'apple',\n",
       " '24': 'banana',\n",
       " '25': 'drive',\n",
       " '26': 'again',\n",
       " '27': 'also',\n",
       " '28': 'ask',\n",
       " '29': 'yes',\n",
       " '30': 'no',\n",
       " '31': 'man',\n",
       " '32': 'man',\n",
       " '33': 'woman',\n",
       " '34': 'woman',\n",
       " '35': 'he/she',\n",
       " '36': 'bad',\n",
       " '37': 'have/has/had',\n",
       " '38': 'have/has/had',\n",
       " '39': 'when',\n",
       " '40': 'where',\n",
       " '41': 'which',\n",
       " '42': 'who',\n",
       " '43': 'why',\n",
       " '44': 'how',\n",
       " '45': 'you',\n",
       " '46': 'boy',\n",
       " '47': 'girl',\n",
       " '48': 'friend',\n",
       " '49': 'finish/complete',\n",
       " '50': 'find',\n",
       " '51': 'other',\n",
       " '52': 'forget',\n",
       " '53': 'give',\n",
       " '54': 'give you',\n",
       " '55': 'give me',\n",
       " '56': 'go',\n",
       " '57': 'get',\n",
       " '58': 'understand/comprehend',\n",
       " '59': 'use',\n",
       " '60': 'will',\n",
       " '61': 'with',\n",
       " '62': 'wait',\n",
       " '63': 'work',\n",
       " '64': 'they',\n",
       " '65': 'their',\n",
       " '66': 'school',\n",
       " '67': 'write',\n",
       " '68': 'send text/message',\n",
       " '69': 'email',\n",
       " '70': 'email',\n",
       " '71': 'home',\n",
       " '72': 'but',\n",
       " '73': 'should',\n",
       " '74': 'not',\n",
       " '75': 'my',\n",
       " '76': 'name',\n",
       " '77': 'like',\n",
       " '78': 'say',\n",
       " '79': 'cold',\n",
       " '80': 'hot',\n",
       " '81': 'family',\n",
       " '82': 'mother',\n",
       " '83': 'father',\n",
       " '84': 'many',\n",
       " '85': 'few',\n",
       " '86': 'now',\n",
       " '87': 'later',\n",
       " '88': 'time',\n",
       " '89': 'tomorrow',\n",
       " '90': 'yesterday',\n",
       " '91': 'same/also',\n",
       " '92': 'remember',\n",
       " '93': 'your',\n",
       " '94': 'more',\n",
       " '95': 'meet',\n",
       " '96': 'see',\n",
       " '97': 'slow',\n",
       " '98': 'fast/quick',\n",
       " '99': 'some',\n",
       " '100': 'store/shop',\n",
       " '101': 'take',\n",
       " '102': 'take/bring me',\n",
       " '103': 'tell',\n",
       " '104': 'think',\n",
       " '105': 'want',\n",
       " '106': 'inexpensive',\n",
       " '107': 'expensive',\n",
       " '108': 'that',\n",
       " '109': 'this',\n",
       " '110': 'here',\n",
       " '111': 'near',\n",
       " '112': 'far',\n",
       " '113': 'cat',\n",
       " '114': 'dog',\n",
       " '115': 'morning',\n",
       " '116': 'night',\n",
       " '117': 'beautiful',\n",
       " '118': 'open',\n",
       " '119': 'close/shut',\n",
       " '120': 'close/shut',\n",
       " '121': 'NONE'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from files_io import readActionLabels, initActionLabelFolders\n",
    "\n",
    "action_labels = readActionLabels()\n",
    "initActionLabelFolders(action_labels)\n",
    "action_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Feature Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize cam as global object\n",
    "cam = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detects hands, face & pose, \n",
    "# convert them into normalized landmark/keypoint coordinates in a 1D-array, \n",
    "# and also returns the frame with the landmark connections drawn onto it\n",
    "\n",
    "# Improved/Parallelised version\n",
    "def featureExtraction(handDetector, faceDetector, poseDetector, frame, draw=True):\n",
    "    def detectHands(handDetector, frame, frameSize, draw):\n",
    "        results = None\n",
    "        # Hand Detection\n",
    "        if (draw):\n",
    "            results, frame = handDetector.findHands(frame, draw=draw, flipType=False)\n",
    "        else:\n",
    "            results = handDetector.findHands(frame, draw=draw, flipType=False)\n",
    "\n",
    "        if not results:\n",
    "            results = [generate_empty_hand('Left'), generate_empty_hand('Right')]\n",
    "        elif len(results) == 1:\n",
    "            if (results[0]['type'] == 'Left'):\n",
    "                results[0] = preprocess_body_part(results[0], frameSize)\n",
    "                results.append(generate_empty_hand('Right'))\n",
    "            else:\n",
    "                results[0] = preprocess_body_part(results[0], frameSize)\n",
    "                results.insert(0, generate_empty_hand('Left'))                         \n",
    "        else:\n",
    "            results[0] = preprocess_body_part(results[0], frameSize)\n",
    "            results[1] = preprocess_body_part(results[1], frameSize)\n",
    "        return results\n",
    "\n",
    "    # Pose Detection\n",
    "    # **We only use the first 23 out of the total 33 landmark points \n",
    "    #   as those represent the lower half body and are irrelevant to sign language interpretation\n",
    "    def detectPose(poseDetector, frame, draw):\n",
    "        frame = poseDetector.findPose(frame, draw=draw)\n",
    "        results, _ = poseDetector.findPosition(frame, bboxWithHands=False)\n",
    "        if results:\n",
    "            results = preprocess_landmarks(results[:23])\n",
    "        else:\n",
    "            results = np.zeros(23, dtype=int)\n",
    "        return results\n",
    "    \n",
    "    # Face Detection\n",
    "    def detectFace(faceDetector, frame, frameSize, draw):\n",
    "        frame, results = faceDetector.findFaces(frame, draw=draw)\n",
    "        if results:\n",
    "            results = select_best_matching_face(results, frameSize)\n",
    "            results['bbox'] = preprocess_bbox(results['bbox'], frameSize)\n",
    "            results['center'] = preprocess_center(results['center'], frameSize)\n",
    "        else:\n",
    "            results = {\n",
    "                'bbox': np.zeros(4, dtype=float), \n",
    "                'center': np.zeros(2, dtype=float)\n",
    "            }\n",
    "        return results\n",
    "\n",
    "    frameSize = (frame.shape[1], frame.shape[0])\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        t1 = executor.submit(detectHands, handDetector, frame, frameSize, draw)\n",
    "        t2 = executor.submit(detectPose, poseDetector, frame, draw)\n",
    "        t3 = executor.submit(detectFace, faceDetector, frame, frameSize, draw)\n",
    "        \n",
    "        # Convert results into 1D-array\n",
    "        detectionResults = flatten2dList([\n",
    "            flattenDetectionResult(t1.result()[0]), \n",
    "            flattenDetectionResult(t1.result()[1]), \n",
    "            t2.result(), \n",
    "            t3.result()['bbox'],\n",
    "            t3.result()['center'],\n",
    "            t3.result()['center'] - t1.result()[0]['center'],\n",
    "            t3.result()['center'] - t1.result()[1]['center']\n",
    "        ], dataType=float)\n",
    "\n",
    "        return detectionResults, frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Details of Keypoints collected**\n",
    "\n",
    "detectHands => {\n",
    "    lmList: array(21, 3), \n",
    "    bbox: array(4,), \n",
    "    center: array(2,)\n",
    "}\n",
    "(Total 69 for each hand)\n",
    "\n",
    "detectPose => [\n",
    "    array(23, 4)\n",
    "]\n",
    "(Total 92)\n",
    "\n",
    "detectFace => {\n",
    "    bbox: array(4,),\n",
    "    center: array(2,)\n",
    "}\n",
    "(Total 6)\n",
    "\n",
    "Extra: Distance difference between face and hands => (\n",
    "    face.center - hands[0].center = array(2,),\n",
    "    face.center - hands[1].center = array(2,)\n",
    ")\n",
    "(Total 4)\n",
    "\n",
    "#### **Final Total Number of Keypoints:**\n",
    "69 + 69 + 92 + 6 + 4 = 240 keypoints\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2  UI Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read one frame from camera\n",
    "def readFrame():\n",
    "    success, frame = cam.read()\n",
    "    if not success: \n",
    "        raise Exception(\"No Frames Read\")\n",
    "    return cv2.flip(frame, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pause recording upon \"Space\"\n",
    "def pauseWhenSpace(trainingNum, actionStr):\n",
    "    while True:\n",
    "        frame = readFrame()\n",
    "        cv2.putText(frame, f'Training #{trainingNum + 1} for \\'{actionStr}\\'', (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "        cv2.putText(frame, f'Pausing...', (40, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (20, 255, 125), 3)\n",
    "        cv2.imshow(\"Sign Language Recognition Prototype\", frame)\n",
    "\n",
    "        # If pressed resume, do countdown\n",
    "        keyPressed = cv2.waitKey(10)\n",
    "        if keyPressed == 32:    # 32 == Space\n",
    "            pause_again = False\n",
    "            \n",
    "            for i in range(3):\n",
    "                for _ in range(10):\n",
    "                    temp_frame = readFrame()\n",
    "                    cv2.putText(temp_frame, f'Training #{trainingNum + 1} for \\'{actionStr}\\'', (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "                    cv2.putText(temp_frame, f'Resuming in {3 - i}', (20, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 120), 3)\n",
    "                    cv2.imshow(\"Sign Language Recognition Prototype\", temp_frame)\n",
    "                    tempKey = cv2.waitKey(100)\n",
    "\n",
    "                    if (tempKey == 27):\n",
    "                        raise Exception(\"Finished\")\n",
    "                    # If pressed paused again, stop resuming and continue pausing\n",
    "                    elif tempKey == 32:\n",
    "                        pause_again = True\n",
    "                        break\n",
    "                    elif tempKey == 122:    # Pressed z\n",
    "                        trainingNum -= 1\n",
    "                        pause_again = True\n",
    "                        break\n",
    "                    elif tempKey == 120:    # Pressed x\n",
    "                        trainingNum += 1\n",
    "                        pause_again = True\n",
    "                        break\n",
    "                if pause_again:\n",
    "                    break\n",
    "\n",
    "            if not pause_again:\n",
    "                return trainingNum\n",
    "            \n",
    "        elif keyPressed == 27:\n",
    "            raise Exception(\"Finished\")\n",
    "        elif keyPressed == 122:    # Pressed z\n",
    "            trainingNum -= 1\n",
    "        elif keyPressed == 120:    # Pressed x\n",
    "            trainingNum += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display countdown (1, 2, 3)\n",
    "def countdownFromThree(trainingNum, actionStr):\n",
    "    # Count down 3 seconds on every new training\n",
    "    for i in range(2):\n",
    "        for _ in range(6):\n",
    "            frame = readFrame()\n",
    "            \n",
    "            cv2.putText(frame, f'Training #{trainingNum + 1} for \\'{actionStr}\\'', (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "            cv2.putText(frame, f'Next Training in {3 - i}', (20, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 3)\n",
    "            cv2.imshow(\"Sign Language Recognition Prototype\", frame)\n",
    "            \n",
    "            tempKey = cv2.waitKey(100)\n",
    "            if (tempKey == 27):     # Pressed Esc\n",
    "                raise Exception(\"Finished\")\n",
    "            elif tempKey == 32:     # Pressed Space\n",
    "                return pauseWhenSpace(trainingNum, actionStr)\n",
    "            elif tempKey == 122:    # Pressed z\n",
    "                trainingNum -= 1\n",
    "            elif tempKey == 120:    # Pressed x\n",
    "                trainingNum += 1\n",
    "                \n",
    "    return trainingNum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Recording Label (Create Training Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from files_io import saveKeypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NONE'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify which action to record\n",
    "selected_i = 121\n",
    "action = action_labels[f\"{selected_i}\"]\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detectors\n",
    "handDetector = HandDetector(detectionCon=0.5, maxHands=2)\n",
    "faceDetector = FaceDetector(minDetectionCon=0.5)\n",
    "poseDetector = PoseDetector(detectionCon=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cam = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "cam.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cam.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "try:\n",
    "    trainingResults = np.zeros((TRAININGS_PER_LABEL, FRAMES_PER_TRAINING, 240))\n",
    "    \n",
    "    training_num = 0\n",
    "    while training_num < TRAININGS_PER_LABEL:\n",
    "        \n",
    "        frame_num = 0\n",
    "        while frame_num < FRAMES_PER_TRAINING:\n",
    "\n",
    "            # Countdown\n",
    "            if frame_num == 0:\n",
    "                training_num = countdownFromThree(training_num, action)\n",
    "        \n",
    "            # Read from camera\n",
    "            frame = readFrame()\n",
    "\n",
    "            detectionResults, frame = featureExtraction(\n",
    "                handDetector, faceDetector, poseDetector, frame)\n",
    "            \n",
    "            # Show resulting frame\n",
    "            cv2.putText(frame, f'Training #{training_num + 1} for \\'{action}\\'', (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "            cv2.imshow(\"Sign Language Recognition Prototype\", frame)     \n",
    "\n",
    "            # Save the results\n",
    "            trainingResults[training_num][frame_num] = detectionResults\n",
    "\n",
    "            keyPressed = cv2.waitKey(10)\n",
    "            # Stop Program when pressed 'Esc'\n",
    "            if (keyPressed == 27):\n",
    "                raise Exception(\"Finished\")\n",
    "            \n",
    "            frame_num += 1\n",
    "        \n",
    "        training_num += 1\n",
    "\n",
    "        if training_num >= TRAININGS_PER_LABEL:\n",
    "            training_num = countdownFromThree(training_num, action)\n",
    "\n",
    "    # After all frames are finished for each training:\n",
    "    # save as .npy\n",
    "    \n",
    "    # IMPORTANT: \n",
    "    # Enable it ONLY during data collection or it may OVERWRITE EXISTING DATA\n",
    "    # saveKeypoints(f\"{selected_i},{action}\", \"0-99\", trainingResults)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    traceback.print_exc()\n",
    "\n",
    "finally:\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
