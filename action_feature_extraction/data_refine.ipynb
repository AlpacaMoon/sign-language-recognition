{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "from cvzone.FaceDetectionModule import FaceDetector\n",
    "from cvzone.PoseModule import PoseDetector\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'good/thank you',\n",
       " 'help',\n",
       " 'I/me',\n",
       " 'please',\n",
       " 'sorry',\n",
       " 'welcome',\n",
       " 'welcome',\n",
       " 'ok',\n",
       " 'what',\n",
       " 'what',\n",
       " 'can',\n",
       " 'thank you very much',\n",
       " 'deaf',\n",
       " 'do not',\n",
       " 'feel',\n",
       " 'eat/food',\n",
       " 'eat a lot',\n",
       " 'tired',\n",
       " 'because',\n",
       " 'sick',\n",
       " 'drink',\n",
       " 'drink',\n",
       " 'apple',\n",
       " 'banana',\n",
       " 'drive',\n",
       " 'again',\n",
       " 'also',\n",
       " 'ask',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'man',\n",
       " 'man',\n",
       " 'woman',\n",
       " 'woman',\n",
       " 'he/she',\n",
       " 'bad',\n",
       " 'have/has/had',\n",
       " 'have/has/had',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'who',\n",
       " 'why',\n",
       " 'how',\n",
       " 'you',\n",
       " 'boy',\n",
       " 'girl',\n",
       " 'friend',\n",
       " 'finish/complete',\n",
       " 'find',\n",
       " 'other',\n",
       " 'forget',\n",
       " 'give',\n",
       " 'give you',\n",
       " 'give me',\n",
       " 'go',\n",
       " 'get',\n",
       " 'understand/comprehend',\n",
       " 'use',\n",
       " 'will',\n",
       " 'with',\n",
       " 'wait',\n",
       " 'work',\n",
       " 'they',\n",
       " 'their',\n",
       " 'school',\n",
       " 'write',\n",
       " 'send text/message',\n",
       " 'email',\n",
       " 'email',\n",
       " 'home',\n",
       " 'but',\n",
       " 'should',\n",
       " 'not',\n",
       " 'my',\n",
       " 'name',\n",
       " 'like',\n",
       " 'say',\n",
       " 'cold',\n",
       " 'hot',\n",
       " 'family',\n",
       " 'mother',\n",
       " 'father',\n",
       " 'many',\n",
       " 'few',\n",
       " 'now',\n",
       " 'later',\n",
       " 'time',\n",
       " 'tomorrow',\n",
       " 'yesterday',\n",
       " 'same/also',\n",
       " 'remember',\n",
       " 'your',\n",
       " 'more',\n",
       " 'meet',\n",
       " 'see',\n",
       " 'slow',\n",
       " 'fast/quick',\n",
       " 'some',\n",
       " 'store/shop',\n",
       " 'take',\n",
       " 'take/bring me',\n",
       " 'tell',\n",
       " 'think',\n",
       " 'want',\n",
       " 'inexpensive',\n",
       " 'expensive',\n",
       " 'that',\n",
       " 'this',\n",
       " 'here',\n",
       " 'near',\n",
       " 'far',\n",
       " 'cat',\n",
       " 'dog',\n",
       " 'morning',\n",
       " 'night',\n",
       " 'beautiful',\n",
       " 'open',\n",
       " 'close/shut',\n",
       " 'close/shut',\n",
       " 'NONE']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "LIST_PATH = \"action_recognition/action_labels.csv\"\n",
    "\n",
    "import os\n",
    "import csv\n",
    "action_labels = []\n",
    "with open(os.path.join(\"../\", LIST_PATH)) as f:\n",
    "    csv_reader = csv.reader(f, delimiter=\",\")\n",
    "    action_labels = [i[1] for i in csv_reader]\n",
    "action_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "DATA_PATH = \"action_recognition/keypoints_data\"\n",
    "\n",
    "current_i = 0\n",
    "rawNpy = np.load(os.path.join(\"../\", DATA_PATH, f\"{current_i},{action_labels[current_i]}\", \"0-99.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FeatureExtractionModule():\n",
    "    def __init__(self, **kwargs):\n",
    "        # Detectors\n",
    "        self.handDetector = HandDetector(detectionCon=0.5, maxHands=2)\n",
    "        self.faceDetector = FaceDetector(minDetectionCon=0.5)\n",
    "        self.poseDetector = PoseDetector(detectionCon=0.5)\n",
    "\n",
    "    def detectHands(self, handDetector, frame, frameSize, draw):\n",
    "        results = [0, 0]\n",
    "        tempResults = []\n",
    "        # Hand Detection\n",
    "        if draw:\n",
    "            tempResults, frame = handDetector.findHands(frame, draw=draw, flipType=False)\n",
    "        else:\n",
    "            tempResults = handDetector.findHands(frame, draw=draw, flipType=False)\n",
    "\n",
    "        if not tempResults:\n",
    "            results = [self.generate_empty_hand(\"Left\"), self.generate_empty_hand(\"Right\")]\n",
    "        elif len(tempResults) == 1:\n",
    "            if tempResults[0][\"type\"] == \"Left\":\n",
    "                results = [self.preprocess_body_part(tempResults[0], frameSize), self.generate_empty_hand(\"Right\")]\n",
    "            else:\n",
    "                results = [self.generate_empty_hand(\"Left\"), self.preprocess_body_part(tempResults[0], frameSize)]\n",
    "        else:\n",
    "            if tempResults[0]['type'] == 'Right' and tempResults[1]['type'] == 'Left':\n",
    "                results[0] = tempResults[1]\n",
    "                results[1] = tempResults[0]\n",
    "            elif tempResults[0]['type'] == 'Left' and tempResults[1]['type'] == 'Right':\n",
    "                results[0] = tempResults[0]\n",
    "                results[1] = tempResults[1]\n",
    "\n",
    "            # If both detected hands are both left or both right\n",
    "            elif tempResults[0]['center'][0] > tempResults[1]['center'][0]:\n",
    "                results[0] = tempResults[1]\n",
    "                results[1] = tempResults[0]\n",
    "            else:\n",
    "                results[0] = tempResults[0]\n",
    "                results[1] = tempResults[1]\n",
    "\n",
    "            results[0] = self.preprocess_body_part(results[0], frameSize)\n",
    "            results[1] = self.preprocess_body_part(results[1], frameSize)\n",
    "\n",
    "        return results\n",
    "\n",
    "    # Pose Detection\n",
    "    # **We only use the first 23 out of the total 33 landmark points\n",
    "    #   as those represent the lower half body and are irrelevant to sign language interpretation\n",
    "    def detectPose(self, poseDetector, frame, frameSize, draw):\n",
    "        frame = poseDetector.findPose(frame, draw=draw)\n",
    "        if poseDetector.results.pose_landmarks:\n",
    "            results = np.array([[i.x, i.y, i.z, i.visibility] for i in poseDetector.results.pose_landmarks.landmark[:23]])\n",
    "            return results.ravel()\n",
    "\n",
    "        # frame = poseDetector.findPose(frame, draw=draw)\n",
    "        # results, _ = poseDetector.findPosition(frame, bboxWithHands=False)\n",
    "        # print('---------------')\n",
    "        # print('e1', np.array(results)[:, -1])\n",
    "        # if results:\n",
    "        #     return np.array(results).flatten()\n",
    "        #     # return self.preprocess_landmarks(results[:23], frameSize)\n",
    "        # print('e2', results)\n",
    "        return np.zeros(92, dtype=float)\n",
    "        \n",
    "\n",
    "    # Face Detection\n",
    "    def detectFace(self, faceDetector, frame, frameSize, draw):\n",
    "        frame, results = faceDetector.findFaces(frame, draw=draw)\n",
    "        if results:\n",
    "            results = self.select_best_matching_face(results, frameSize)\n",
    "            results[\"bbox\"] = self.preprocess_bbox(results[\"bbox\"], frameSize)\n",
    "            results[\"center\"] = self.preprocess_center(results[\"center\"], frameSize)\n",
    "            return results\n",
    "\n",
    "        return {\n",
    "            \"bbox\": np.zeros(4, dtype=float),\n",
    "            \"center\": np.zeros(2, dtype=float),\n",
    "        }\n",
    "\n",
    "    # Detects hands, face & pose,\n",
    "    # convert them into normalized landmark/keypoint coordinates in a 1D-array,\n",
    "    # and also returns the frame with the landmark connections drawn onto it\n",
    "    def parallelFeatureExtraction(\n",
    "        self, handDetector, faceDetector, poseDetector, frame, draw=True\n",
    "    ):\n",
    "        frameSize = (frame.shape[1], frame.shape[0])\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            t1 = executor.submit(self.detectHands, handDetector, frame, frameSize, draw)\n",
    "            t2 = executor.submit(self.detectPose, poseDetector, frame, frameSize, draw)\n",
    "            t3 = executor.submit(self.detectFace, faceDetector, frame, frameSize, draw)\n",
    "\n",
    "            # Convert results into 1D-array\n",
    "            detectionResults = self.flatten2dList(\n",
    "                [\n",
    "                    self.flattenDetectionResult(t1.result()[0]),\n",
    "                    self.flattenDetectionResult(t1.result()[1]),\n",
    "                    t2.result(),\n",
    "                    t3.result()[\"bbox\"],\n",
    "                    t3.result()[\"center\"],\n",
    "                    t3.result()[\"center\"] - t1.result()[0][\"center\"],\n",
    "                    t3.result()[\"center\"] - t1.result()[1][\"center\"],\n",
    "                ],\n",
    "                dataType=float,\n",
    "            )\n",
    "\n",
    "            return detectionResults, frame\n",
    "\n",
    "    # Offset and normalize the landmark list\n",
    "    # Returns a 1d numpy array\n",
    "    def preprocess_landmarks(self, landmark_list, frameSize):\n",
    "        np_landmark_list = np.array(landmark_list, dtype=float)\n",
    "        np_frameSize = np.array([frameSize[0], frameSize[1], frameSize[0]])\n",
    "        return (np_landmark_list / np_frameSize).ravel()\n",
    "\n",
    "\n",
    "    # Offset and normalize a BBOX list (BBOX = Bounding Box, used in face and hand detection)\n",
    "    # Returns a 1d numpy array\n",
    "    def preprocess_bbox(self, bbox, frameSize):\n",
    "        bbox = np.array(bbox, dtype=float)\n",
    "        # Convert 3rd and 4th element into coordinates instead of width/height\n",
    "        bbox[2] = bbox[0] + bbox[2]\n",
    "        bbox[3] = bbox[1] + bbox[3]\n",
    "\n",
    "        # Normalize against frame size\n",
    "        bbox[0] /= frameSize[0]\n",
    "        bbox[1] /= frameSize[1]\n",
    "        bbox[2] /= frameSize[0]\n",
    "        bbox[3] /= frameSize[1]\n",
    "\n",
    "        return bbox\n",
    "\n",
    "\n",
    "    # Normalize a center vertex (a list of 2 elements)\n",
    "    # Returns a 1d numpy array\n",
    "    def preprocess_center(self, center, frameSize):\n",
    "        center = np.array(center, dtype=float)\n",
    "        center[0] /= frameSize[0]\n",
    "        center[1] /= frameSize[1]\n",
    "        return center\n",
    "\n",
    "\n",
    "    # Preprocess (Offset and normalize) the body's landmark list, bbox and center\n",
    "    def preprocess_body_part(self, bodyPart, frameSize):\n",
    "        bodyPart[\"lmList\"] = self.preprocess_landmarks(bodyPart[\"lmList\"], frameSize)\n",
    "        bodyPart[\"bbox\"] = self.preprocess_bbox(bodyPart[\"bbox\"], frameSize)\n",
    "        bodyPart[\"center\"] = self.preprocess_center(bodyPart[\"center\"], frameSize)\n",
    "        return bodyPart\n",
    "\n",
    "\n",
    "    # Function to generate empty/placeholder data for a hand\n",
    "    # Used when a hand is not detected in frame\n",
    "    def generate_empty_hand(self, type):\n",
    "        return {\n",
    "            \"lmList\": np.zeros(63, dtype=float),\n",
    "            \"bbox\": np.zeros(4, dtype=float),\n",
    "            \"center\": np.zeros(2, dtype=float),\n",
    "            \"type\": type,\n",
    "        }\n",
    "\n",
    "\n",
    "    # Select the best matching face, aka the one with the best score (clarity)\n",
    "    # and closest to the center of the screen\n",
    "    # Since the Neural network will be design to only accept one face\n",
    "    def select_best_matching_face(self, faces, frameSize):\n",
    "        if not faces or len(faces) == 0:\n",
    "            return False\n",
    "        elif len(faces) == 1:\n",
    "            return faces[0]\n",
    "\n",
    "        def difference(a, b):\n",
    "            return ((a[0] - b[0]) ** 2) + ((a[1] - b[1]) ** 2)\n",
    "\n",
    "        frameCenter = (frameSize[0] / 2, frameSize[1] / 2)\n",
    "\n",
    "        best_score = faces[0]\n",
    "        best_center = faces[0]\n",
    "        center_diff = difference(faces[0][\"center\"], frameCenter)\n",
    "\n",
    "        for each in faces[1:]:\n",
    "            if difference(each[\"center\"], frameCenter) < center_diff:\n",
    "                best_center = each\n",
    "            if each[\"score\"][0] > best_score[\"score\"][0]:\n",
    "                best_score = each\n",
    "\n",
    "        if best_center[\"score\"][0] > 0.5:\n",
    "            return best_center\n",
    "        return best_score\n",
    "\n",
    "    # Flatten a 2d np array into 1d array\n",
    "    def flatten2dList(self, arr, dataType=float):\n",
    "        return np.fromiter(chain.from_iterable(arr), dataType)\n",
    "\n",
    "    # Flatten everything\n",
    "    def flattenDetectionResult(self, obj):\n",
    "        return np.concatenate([obj[\"lmList\"], obj[\"bbox\"], obj[\"center\"]])\n",
    "\n",
    "\n",
    "    def extractFeatures(self, frame):\n",
    "        detectionResults, frame = self.parallelFeatureExtraction(\n",
    "            self.handDetector, self.faceDetector, self.poseDetector, frame\n",
    "        )\n",
    "\n",
    "        return detectionResults, frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cam = None\n",
    "# Read one frame from camera\n",
    "\n",
    "def readFrame():\n",
    "    success, frame = cam.read()\n",
    "    if not success: \n",
    "        raise Exception(\"No Frames Read\")\n",
    "    return cv2.flip(frame, 1)\n",
    "\n",
    "\n",
    "cam = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "cam.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cam.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "try:\n",
    "    femodule = FeatureExtractionModule()\n",
    "    \n",
    "    while True:\n",
    "        # Read from camera\n",
    "        rawframe = readFrame()\n",
    "\n",
    "        frame = np.copy(rawframe)\n",
    "\n",
    "        detectionResults, frame = femodule.extractFeatures(frame)\n",
    "        \n",
    "        # Show resulting frame\n",
    "        cv2.imshow(\"Sign Language Recognition Prototype\", frame)     \n",
    "\n",
    "        keyPressed = cv2.waitKey(10)\n",
    "        # Stop Program when pressed 'Esc'\n",
    "        if (keyPressed == 27):\n",
    "            raise Exception(\"Finished\")\n",
    "\n",
    "        break\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    traceback.print_exc()\n",
    "\n",
    "finally:\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    del femodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240,)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detectionResults.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(detectionResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999569654464722"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(detectionResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.55139923,  0.73399454, -0.74569803],\n",
       "       [ 0.56648535,  0.68524396, -0.70646536],\n",
       "       [ 0.57641351,  0.68570024, -0.70679051],\n",
       "       [ 0.58612144,  0.68697423, -0.70683694],\n",
       "       [ 0.53418308,  0.68727589, -0.7128588 ],\n",
       "       [ 0.52298796,  0.68843925, -0.71283257],\n",
       "       [ 0.51305228,  0.69001383, -0.71318281],\n",
       "       [ 0.60445964,  0.71692556, -0.40919787],\n",
       "       [ 0.5005421 ,  0.71843433, -0.43512335],\n",
       "       [ 0.57334667,  0.78898698, -0.62870717],\n",
       "       [ 0.53335941,  0.79032856, -0.63620675],\n",
       "       [ 0.68944788,  0.9606145 , -0.20965974],\n",
       "       [ 0.43566662,  0.96137846, -0.25402993],\n",
       "       [ 0.75933588,  1.21941566, -0.30540067],\n",
       "       [ 0.39738399,  1.2676121 , -0.33917922],\n",
       "       [ 0.72616225,  1.33728361, -0.68819159],\n",
       "       [ 0.42404801,  1.39239919, -0.77118653],\n",
       "       [ 0.7201255 ,  1.39245105, -0.80496472],\n",
       "       [ 0.42453066,  1.45391953, -0.86561853],\n",
       "       [ 0.7049216 ,  1.36649776, -0.79739189],\n",
       "       [ 0.43603382,  1.42126441, -0.89484537],\n",
       "       [ 0.69912648,  1.34275317, -0.76160091],\n",
       "       [ 0.44017926,  1.39484775, -0.7962063 ]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detectionResults[138:207].reshape(23, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertX(normalized_x):\n",
    "    return int(normalized_x * 1280)\n",
    "def convertY(normalized_y):\n",
    "    return int(normalized_y * 720)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0)\n",
      "(0, 0)\n",
      "0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "dr = detectionResults\n",
    "print((convertX(dr[63]), convertY(dr[64])))\n",
    "print((convertX(dr[65]), convertY(dr[66])))\n",
    "print(dr[65], dr[66])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_45200\\1325288025.py\", line 24, in <module>\n",
      "    raise Exception(\"Finished\")\n",
      "Exception: Finished\n"
     ]
    }
   ],
   "source": [
    "modFrame = np.copy(rawframe)\n",
    "dr = detectionResults\n",
    "\n",
    "# Draw left hand center\n",
    "cv2.circle(modFrame, (convertX(dr[67]), convertY(dr[68])), 5, (0, 0, 255), 5)\n",
    "\n",
    "# Draw left hand bbox\n",
    "cv2.rectangle(modFrame, \n",
    "              (convertX(dr[63]), convertY(dr[64])),\n",
    "              (convertX(dr[65]), convertY(dr[66])),\n",
    "              (0, 0, 255), 3\n",
    "              )\n",
    "\n",
    "cv2.circle(modFrame, (1200, 700), 5, (0, 255, 255), 5)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        cv2.imshow(\"dsa\", modFrame)\n",
    "        # cv2.imshow(\"dsa\", frame)\n",
    "        \n",
    "        keyPressed = cv2.waitKey(10)\n",
    "        # Stop Program when pressed 'Esc'\n",
    "        if (keyPressed == 27):\n",
    "            raise Exception(\"Finished\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    traceback.print_exc()\n",
    "\n",
    "finally:\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 'hello',\n",
       " '1': 'good/thank you',\n",
       " '2': 'help',\n",
       " '3': 'I/me',\n",
       " '4': 'please',\n",
       " '5': 'sorry',\n",
       " '6': 'welcome',\n",
       " '7': 'welcome',\n",
       " '8': 'ok',\n",
       " '9': 'what',\n",
       " '10': 'what',\n",
       " '11': 'can',\n",
       " '12': 'thank you very much',\n",
       " '13': 'deaf',\n",
       " '14': 'do not',\n",
       " '15': 'feel',\n",
       " '16': 'eat/food',\n",
       " '17': 'eat a lot',\n",
       " '18': 'tired',\n",
       " '19': 'because',\n",
       " '20': 'sick',\n",
       " '21': 'drink',\n",
       " '22': 'drink',\n",
       " '23': 'apple',\n",
       " '24': 'banana',\n",
       " '25': 'drive',\n",
       " '26': 'again',\n",
       " '27': 'also',\n",
       " '28': 'ask',\n",
       " '29': 'yes',\n",
       " '30': 'no',\n",
       " '31': 'man',\n",
       " '32': 'man',\n",
       " '33': 'woman',\n",
       " '34': 'woman',\n",
       " '35': 'he/she',\n",
       " '36': 'bad',\n",
       " '37': 'have/has/had',\n",
       " '38': 'have/has/had',\n",
       " '39': 'when',\n",
       " '40': 'where',\n",
       " '41': 'which',\n",
       " '42': 'who',\n",
       " '43': 'why',\n",
       " '44': 'how',\n",
       " '45': 'you',\n",
       " '46': 'boy',\n",
       " '47': 'girl',\n",
       " '48': 'friend',\n",
       " '49': 'finish/complete',\n",
       " '50': 'find',\n",
       " '51': 'other',\n",
       " '52': 'forget',\n",
       " '53': 'give',\n",
       " '54': 'give you',\n",
       " '55': 'give me',\n",
       " '56': 'go',\n",
       " '57': 'get',\n",
       " '58': 'understand/comprehend',\n",
       " '59': 'use',\n",
       " '60': 'will',\n",
       " '61': 'with',\n",
       " '62': 'wait',\n",
       " '63': 'work',\n",
       " '64': 'they',\n",
       " '65': 'their',\n",
       " '66': 'school',\n",
       " '67': 'write',\n",
       " '68': 'send text/message',\n",
       " '69': 'email',\n",
       " '70': 'email',\n",
       " '71': 'home',\n",
       " '72': 'but',\n",
       " '73': 'should',\n",
       " '74': 'not',\n",
       " '75': 'my',\n",
       " '76': 'name',\n",
       " '77': 'like',\n",
       " '78': 'say',\n",
       " '79': 'cold',\n",
       " '80': 'hot',\n",
       " '81': 'family',\n",
       " '82': 'mother',\n",
       " '83': 'father',\n",
       " '84': 'many',\n",
       " '85': 'few',\n",
       " '86': 'now',\n",
       " '87': 'later',\n",
       " '88': 'time',\n",
       " '89': 'tomorrow',\n",
       " '90': 'yesterday',\n",
       " '91': 'same/also',\n",
       " '92': 'remember',\n",
       " '93': 'your',\n",
       " '94': 'more',\n",
       " '95': 'meet',\n",
       " '96': 'see',\n",
       " '97': 'slow',\n",
       " '98': 'fast/quick',\n",
       " '99': 'some',\n",
       " '100': 'store/shop',\n",
       " '101': 'take',\n",
       " '102': 'take/bring me',\n",
       " '103': 'tell',\n",
       " '104': 'think',\n",
       " '105': 'want',\n",
       " '106': 'inexpensive',\n",
       " '107': 'expensive',\n",
       " '108': 'that',\n",
       " '109': 'this',\n",
       " '110': 'here',\n",
       " '111': 'near',\n",
       " '112': 'far',\n",
       " '113': 'cat',\n",
       " '114': 'dog',\n",
       " '115': 'morning',\n",
       " '116': 'night',\n",
       " '117': 'beautiful',\n",
       " '118': 'open',\n",
       " '119': 'close/shut',\n",
       " '120': 'close/shut',\n",
       " '121': 'NONE'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from files_io import readActionLabels, initActionLabelFolders\n",
    "\n",
    "action_labels = readActionLabels()\n",
    "initActionLabelFolders(action_labels)\n",
    "action_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read one frame from camera\n",
    "def readFrame():\n",
    "    success, frame = cam.read()\n",
    "    if not success: \n",
    "        raise Exception(\"No Frames Read\")\n",
    "    return cv2.flip(frame, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pause recording upon \"Space\"\n",
    "def pauseWhenSpace(trainingNum, actionStr):\n",
    "    while True:\n",
    "        frame = readFrame()\n",
    "        cv2.putText(frame, f'Training #{trainingNum + 1} for \\'{actionStr}\\'', (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "        cv2.putText(frame, f'Pausing...', (40, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (20, 255, 125), 3)\n",
    "        cv2.imshow(\"Sign Language Recognition Prototype\", frame)\n",
    "\n",
    "        # If pressed resume, do countdown\n",
    "        keyPressed = cv2.waitKey(10)\n",
    "        if keyPressed == 32:    # 32 == Space\n",
    "            pause_again = False\n",
    "            \n",
    "            for i in range(3):\n",
    "                for _ in range(10):\n",
    "                    temp_frame = readFrame()\n",
    "                    cv2.putText(temp_frame, f'Training #{trainingNum + 1} for \\'{actionStr}\\'', (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "                    cv2.putText(temp_frame, f'Resuming in {3 - i}', (20, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 120), 3)\n",
    "                    cv2.imshow(\"Sign Language Recognition Prototype\", temp_frame)\n",
    "                    tempKey = cv2.waitKey(100)\n",
    "\n",
    "                    if (tempKey == 27):\n",
    "                        raise Exception(\"Finished\")\n",
    "                    # If pressed paused again, stop resuming and continue pausing\n",
    "                    elif tempKey == 32:\n",
    "                        pause_again = True\n",
    "                        break\n",
    "                    elif tempKey == 122:    # Pressed z\n",
    "                        trainingNum -= 1\n",
    "                        pause_again = True\n",
    "                        break\n",
    "                    elif tempKey == 120:    # Pressed x\n",
    "                        trainingNum += 1\n",
    "                        pause_again = True\n",
    "                        break\n",
    "                if pause_again:\n",
    "                    break\n",
    "\n",
    "            if not pause_again:\n",
    "                return trainingNum\n",
    "            \n",
    "        elif keyPressed == 27:\n",
    "            raise Exception(\"Finished\")\n",
    "        elif keyPressed == 122:    # Pressed z\n",
    "            trainingNum -= 1\n",
    "        elif keyPressed == 120:    # Pressed x\n",
    "            trainingNum += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display countdown (1, 2, 3)\n",
    "def countdownFromThree(trainingNum, actionStr):\n",
    "    # Count down 3 seconds on every new training\n",
    "    for i in range(1):\n",
    "        for _ in range(6):\n",
    "            frame = readFrame()\n",
    "            \n",
    "            cv2.putText(frame, f'Training #{trainingNum + 1} for \\'{actionStr}\\'', (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "            cv2.putText(frame, f'Next Training in {3 - i}', (20, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 3)\n",
    "            cv2.imshow(\"Sign Language Recognition Prototype\", frame)\n",
    "            \n",
    "            tempKey = cv2.waitKey(100)\n",
    "            if (tempKey == 27):     # Pressed Esc\n",
    "                raise Exception(\"Finished\")\n",
    "            elif tempKey == 32:     # Pressed Space\n",
    "                return pauseWhenSpace(trainingNum, actionStr)\n",
    "            elif tempKey == 122:    # Pressed z\n",
    "                trainingNum -= 1\n",
    "            elif tempKey == 120:    # Pressed x\n",
    "                trainingNum += 1\n",
    "                \n",
    "    return trainingNum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Recording Label (Create Training Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from files_io import saveKeypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fem = FeatureExtractionModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NONE'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify which action to record\n",
    "selected_i = 121\n",
    "action = action_labels[f\"{selected_i}\"]\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAININGS_PER_LABEL = 100\n",
    "FRAMES_PER_TRAINING = 20\n",
    "\n",
    "cam = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "cam.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cam.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "try:\n",
    "    trainingResults = np.zeros((TRAININGS_PER_LABEL, FRAMES_PER_TRAINING, 240))\n",
    "    \n",
    "    training_num = 0\n",
    "    while training_num < TRAININGS_PER_LABEL:\n",
    "        \n",
    "        frame_num = 0\n",
    "        while frame_num < FRAMES_PER_TRAINING:\n",
    "\n",
    "            # Countdown\n",
    "            if frame_num == 0:\n",
    "                training_num = countdownFromThree(training_num, action)\n",
    "        \n",
    "            # Read from camera\n",
    "            frame = readFrame()\n",
    "\n",
    "            detectionResults, frame = fem.extractFeatures(frame)\n",
    "            \n",
    "            # Show resulting frame\n",
    "            cv2.putText(frame, f'Training #{training_num + 1} for \\'{action}\\'', (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "            cv2.imshow(\"Sign Language Recognition Prototype\", frame)     \n",
    "\n",
    "            # Save the results\n",
    "            trainingResults[training_num][frame_num] = detectionResults\n",
    "\n",
    "            keyPressed = cv2.waitKey(10)\n",
    "            # Stop Program when pressed 'Esc'\n",
    "            if (keyPressed == 27):\n",
    "                raise Exception(\"Finished\")\n",
    "            \n",
    "            frame_num += 1\n",
    "        \n",
    "        training_num += 1\n",
    "\n",
    "        if training_num >= TRAININGS_PER_LABEL:\n",
    "            training_num = countdownFromThree(training_num, action)\n",
    "\n",
    "    # After all frames are finished for each training:\n",
    "    # save as .npy\n",
    "    \n",
    "    # IMPORTANT: \n",
    "    # Enable it ONLY during data collection or it may OVERWRITE EXISTING DATA\n",
    "    saveKeypoints(f\"{selected_i},{action}\", \"0-99\", trainingResults)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    traceback.print_exc()\n",
    "\n",
    "finally:\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
