{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "from cvzone.FaceDetectionModule import FaceDetector\n",
    "from cvzone.PoseModule import PoseDetector\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'good/thank you',\n",
       " 'help',\n",
       " 'I/me',\n",
       " 'please',\n",
       " 'sorry',\n",
       " 'welcome',\n",
       " 'welcome',\n",
       " 'ok',\n",
       " 'what',\n",
       " 'what',\n",
       " 'can',\n",
       " 'thank you very much',\n",
       " 'deaf',\n",
       " 'do not',\n",
       " 'feel',\n",
       " 'eat/food',\n",
       " 'eat a lot',\n",
       " 'tired',\n",
       " 'because',\n",
       " 'sick',\n",
       " 'drink',\n",
       " 'drink',\n",
       " 'apple',\n",
       " 'banana',\n",
       " 'drive',\n",
       " 'again',\n",
       " 'also',\n",
       " 'ask',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'man',\n",
       " 'man',\n",
       " 'woman',\n",
       " 'woman',\n",
       " 'he/she',\n",
       " 'bad',\n",
       " 'have/has/had',\n",
       " 'have/has/had',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'who',\n",
       " 'why',\n",
       " 'how',\n",
       " 'you',\n",
       " 'boy',\n",
       " 'girl',\n",
       " 'friend',\n",
       " 'finish/complete',\n",
       " 'find',\n",
       " 'other',\n",
       " 'forget',\n",
       " 'give',\n",
       " 'give you',\n",
       " 'give me',\n",
       " 'go',\n",
       " 'get',\n",
       " 'understand/comprehend',\n",
       " 'use',\n",
       " 'will',\n",
       " 'with',\n",
       " 'wait',\n",
       " 'work',\n",
       " 'they',\n",
       " 'their',\n",
       " 'school',\n",
       " 'write',\n",
       " 'send text/message',\n",
       " 'email',\n",
       " 'email',\n",
       " 'home',\n",
       " 'but',\n",
       " 'should',\n",
       " 'not',\n",
       " 'my',\n",
       " 'name',\n",
       " 'like',\n",
       " 'say',\n",
       " 'cold',\n",
       " 'hot',\n",
       " 'family',\n",
       " 'mother',\n",
       " 'father',\n",
       " 'many',\n",
       " 'few',\n",
       " 'now',\n",
       " 'later',\n",
       " 'time',\n",
       " 'tomorrow',\n",
       " 'yesterday',\n",
       " 'same/also',\n",
       " 'remember',\n",
       " 'your',\n",
       " 'more',\n",
       " 'meet',\n",
       " 'see',\n",
       " 'slow',\n",
       " 'fast/quick',\n",
       " 'some',\n",
       " 'store/shop',\n",
       " 'take',\n",
       " 'take/bring me',\n",
       " 'tell',\n",
       " 'think',\n",
       " 'want',\n",
       " 'inexpensive',\n",
       " 'expensive',\n",
       " 'that',\n",
       " 'this',\n",
       " 'here',\n",
       " 'near',\n",
       " 'far',\n",
       " 'cat',\n",
       " 'dog',\n",
       " 'morning',\n",
       " 'night',\n",
       " 'beautiful',\n",
       " 'open',\n",
       " 'close/shut',\n",
       " 'close/shut']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "LIST_PATH = \"action_recognition/action_labels.csv\"\n",
    "\n",
    "import os\n",
    "import csv\n",
    "action_labels = []\n",
    "with open(os.path.join(\"../\", LIST_PATH)) as f:\n",
    "    csv_reader = csv.reader(f, delimiter=\",\")\n",
    "    action_labels = [i[1] for i in csv_reader]\n",
    "action_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "DATA_PATH = \"action_recognition/keypoints_data\"\n",
    "\n",
    "current_i = 0\n",
    "rawNpy = np.load(os.path.join(\"../\", DATA_PATH, f\"{current_i},{action_labels[current_i]}\", \"0-99.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FeatureExtractionModule():\n",
    "    def __init__(self, **kwargs):\n",
    "        # Detectors\n",
    "        self.handDetector = HandDetector(detectionCon=0.5, maxHands=2)\n",
    "        self.faceDetector = FaceDetector(minDetectionCon=0.5)\n",
    "        self.poseDetector = PoseDetector(detectionCon=0.5)\n",
    "\n",
    "    def detectHands(self, handDetector, frame, frameSize, draw):\n",
    "        results = [0, 0]\n",
    "        tempResults = []\n",
    "        # Hand Detection\n",
    "        if draw:\n",
    "            tempResults, frame = handDetector.findHands(frame, draw=draw, flipType=False)\n",
    "        else:\n",
    "            tempResults = handDetector.findHands(frame, draw=draw, flipType=False)\n",
    "\n",
    "        if not tempResults:\n",
    "            results = [self.generate_empty_hand(\"Left\"), self.generate_empty_hand(\"Right\")]\n",
    "        elif len(tempResults) == 1:\n",
    "            if tempResults[0][\"type\"] == \"Left\":\n",
    "                results = [self.preprocess_body_part(tempResults[0], frameSize), self.generate_empty_hand(\"Right\")]\n",
    "            else:\n",
    "                results = [self.generate_empty_hand(\"Left\"), self.preprocess_body_part(tempResults[0], frameSize)]\n",
    "        else:\n",
    "            if tempResults[0]['type'] == 'Right' and tempResults[1]['type'] == 'Left':\n",
    "                results[0] = tempResults[1]\n",
    "                results[1] = tempResults[0]\n",
    "            elif tempResults[0]['type'] == 'Left' and tempResults[1]['type'] == 'Right':\n",
    "                results[0] = tempResults[0]\n",
    "                results[1] = tempResults[1]\n",
    "\n",
    "            # If both detected hands are both left or both right\n",
    "            elif tempResults[0]['center'][0] > tempResults[1]['center'][0]:\n",
    "                results[0] = tempResults[1]\n",
    "                results[1] = tempResults[0]\n",
    "            else:\n",
    "                results[0] = tempResults[0]\n",
    "                results[1] = tempResults[1]\n",
    "\n",
    "            results[0] = self.preprocess_body_part(results[0], frameSize)\n",
    "            results[1] = self.preprocess_body_part(results[1], frameSize)\n",
    "\n",
    "        return results\n",
    "\n",
    "    # Pose Detection\n",
    "    # **We only use the first 23 out of the total 33 landmark points\n",
    "    #   as those represent the lower half body and are irrelevant to sign language interpretation\n",
    "    def detectPose(self, poseDetector, frame, frameSize, draw):\n",
    "        # frame = poseDetector.findPose(frame, draw=draw)\n",
    "        # if poseDetector.results.pose_landmarks:\n",
    "        #     results = np.array([[i.x, i.y, i.z] for i in poseDetector.results.pose_landmarks.landmark[:23]])\n",
    "        #     return results.ravel()\n",
    "\n",
    "        frame = poseDetector.findPose(frame, draw=draw)\n",
    "        results, _ = poseDetector.findPosition(frame, bboxWithHands=False)\n",
    "        print('---------------')\n",
    "        print('e1', results)\n",
    "        if results:\n",
    "            return self.preprocess_landmarks(results[:23], frameSize)\n",
    "        print('e2', results)\n",
    "        return np.zeros(69, dtype=float)\n",
    "        \n",
    "\n",
    "    # Face Detection\n",
    "    def detectFace(self, faceDetector, frame, frameSize, draw):\n",
    "        frame, results = faceDetector.findFaces(frame, draw=draw)\n",
    "        if results:\n",
    "            results = self.select_best_matching_face(results, frameSize)\n",
    "            results[\"bbox\"] = self.preprocess_bbox(results[\"bbox\"], frameSize)\n",
    "            results[\"center\"] = self.preprocess_center(results[\"center\"], frameSize)\n",
    "            return results\n",
    "\n",
    "        return {\n",
    "            \"bbox\": np.zeros(4, dtype=float),\n",
    "            \"center\": np.zeros(2, dtype=float),\n",
    "        }\n",
    "\n",
    "    # Detects hands, face & pose,\n",
    "    # convert them into normalized landmark/keypoint coordinates in a 1D-array,\n",
    "    # and also returns the frame with the landmark connections drawn onto it\n",
    "    def parallelFeatureExtraction(\n",
    "        self, handDetector, faceDetector, poseDetector, frame, draw=True\n",
    "    ):\n",
    "        frameSize = (frame.shape[1], frame.shape[0])\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            t1 = executor.submit(self.detectHands, handDetector, frame, frameSize, draw)\n",
    "            t2 = executor.submit(self.detectPose, poseDetector, frame, frameSize, draw)\n",
    "            t3 = executor.submit(self.detectFace, faceDetector, frame, frameSize, draw)\n",
    "\n",
    "            # Convert results into 1D-array\n",
    "            detectionResults = self.flatten2dList(\n",
    "                [\n",
    "                    self.flattenDetectionResult(t1.result()[0]),\n",
    "                    self.flattenDetectionResult(t1.result()[1]),\n",
    "                    t2.result(),\n",
    "                    t3.result()[\"bbox\"],\n",
    "                    t3.result()[\"center\"],\n",
    "                    t3.result()[\"center\"] - t1.result()[0][\"center\"],\n",
    "                    t3.result()[\"center\"] - t1.result()[1][\"center\"],\n",
    "                ],\n",
    "                dataType=float,\n",
    "            )\n",
    "\n",
    "            return detectionResults, frame\n",
    "\n",
    "    # Offset and normalize the landmark list\n",
    "    # Returns a 1d numpy array\n",
    "    def preprocess_landmarks(self, landmark_list, frameSize):\n",
    "        np_landmark_list = np.array(landmark_list, dtype=float)\n",
    "        np_frameSize = np.array([frameSize[0], frameSize[1], frameSize[0]])\n",
    "        return (np_landmark_list / np_frameSize).ravel()\n",
    "\n",
    "\n",
    "    # Offset and normalize a BBOX list (BBOX = Bounding Box, used in face and hand detection)\n",
    "    # Returns a 1d numpy array\n",
    "    def preprocess_bbox(self, bbox, frameSize):\n",
    "        bbox = np.array(bbox, dtype=float)\n",
    "        # Convert 3rd and 4th element into coordinates instead of width/height\n",
    "        bbox[2] = bbox[0] + bbox[2]\n",
    "        bbox[3] = bbox[1] + bbox[3]\n",
    "\n",
    "        # Normalize against frame size\n",
    "        bbox[0] /= frameSize[0]\n",
    "        bbox[1] /= frameSize[1]\n",
    "        bbox[2] /= frameSize[0]\n",
    "        bbox[3] /= frameSize[1]\n",
    "\n",
    "        return bbox\n",
    "\n",
    "\n",
    "    # Normalize a center vertex (a list of 2 elements)\n",
    "    # Returns a 1d numpy array\n",
    "    def preprocess_center(self, center, frameSize):\n",
    "        center = np.array(center, dtype=float)\n",
    "        center[0] /= frameSize[0]\n",
    "        center[1] /= frameSize[1]\n",
    "        return center\n",
    "\n",
    "\n",
    "    # Preprocess (Offset and normalize) the body's landmark list, bbox and center\n",
    "    def preprocess_body_part(self, bodyPart, frameSize):\n",
    "        bodyPart[\"lmList\"] = self.preprocess_landmarks(bodyPart[\"lmList\"], frameSize)\n",
    "        bodyPart[\"bbox\"] = self.preprocess_bbox(bodyPart[\"bbox\"], frameSize)\n",
    "        bodyPart[\"center\"] = self.preprocess_center(bodyPart[\"center\"], frameSize)\n",
    "        return bodyPart\n",
    "\n",
    "\n",
    "    # Function to generate empty/placeholder data for a hand\n",
    "    # Used when a hand is not detected in frame\n",
    "    def generate_empty_hand(self, type):\n",
    "        return {\n",
    "            \"lmList\": np.zeros(63, dtype=float),\n",
    "            \"bbox\": np.zeros(4, dtype=float),\n",
    "            \"center\": np.zeros(2, dtype=float),\n",
    "            \"type\": type,\n",
    "        }\n",
    "\n",
    "\n",
    "    # Select the best matching face, aka the one with the best score (clarity)\n",
    "    # and closest to the center of the screen\n",
    "    # Since the Neural network will be design to only accept one face\n",
    "    def select_best_matching_face(self, faces, frameSize):\n",
    "        if not faces or len(faces) == 0:\n",
    "            return False\n",
    "        elif len(faces) == 1:\n",
    "            return faces[0]\n",
    "\n",
    "        def difference(a, b):\n",
    "            return ((a[0] - b[0]) ** 2) + ((a[1] - b[1]) ** 2)\n",
    "\n",
    "        frameCenter = (frameSize[0] / 2, frameSize[1] / 2)\n",
    "\n",
    "        best_score = faces[0]\n",
    "        best_center = faces[0]\n",
    "        center_diff = difference(faces[0][\"center\"], frameCenter)\n",
    "\n",
    "        for each in faces[1:]:\n",
    "            if difference(each[\"center\"], frameCenter) < center_diff:\n",
    "                best_center = each\n",
    "            if each[\"score\"][0] > best_score[\"score\"][0]:\n",
    "                best_score = each\n",
    "\n",
    "        if best_center[\"score\"][0] > 0.5:\n",
    "            return best_center\n",
    "        return best_score\n",
    "\n",
    "    # Flatten a 2d np array into 1d array\n",
    "    def flatten2dList(self, arr, dataType=float):\n",
    "        return np.fromiter(chain.from_iterable(arr), dataType)\n",
    "\n",
    "    # Flatten everything\n",
    "    def flattenDetectionResult(self, obj):\n",
    "        return np.concatenate([obj[\"lmList\"], obj[\"bbox\"], obj[\"center\"]])\n",
    "\n",
    "\n",
    "    def extractFeatures(self, frame):\n",
    "        detectionResults, frame = self.parallelFeatureExtraction(\n",
    "            self.handDetector, self.faceDetector, self.poseDetector, frame\n",
    "        )\n",
    "\n",
    "        return detectionResults, frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "femodule = FeatureExtractionModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "e1 [[0, 679, 470, -707], [1, 697, 440, -654], [2, 708, 442, -653], [3, 718, 444, -653], [4, 659, 444, -644], [5, 645, 448, -644], [6, 632, 452, -644], [7, 742, 469, -371], [8, 612, 477, -316], [9, 703, 513, -605], [10, 652, 514, -592], [11, 821, 640, -264], [12, 536, 645, -222], [13, 1005, 744, -781], [14, 365, 733, -703], [15, 815, 543, -1132], [16, 533, 553, -1163], [17, 808, 464, -1212], [18, 545, 467, -1259], [19, 782, 464, -1071], [20, 563, 474, -1147], [21, 771, 497, -1091], [22, 580, 508, -1141], [23, 768, 1040, -59], [24, 557, 1035, 64], [25, 756, 1340, -77], [26, 551, 1333, 151], [27, 741, 1609, 363], [28, 544, 1597, 458], [29, 743, 1665, 386], [30, 540, 1649, 480], [31, 717, 1695, -16], [32, 566, 1696, 61]]\n",
      "operands could not be broadcast together with shapes (23,4) (3,) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_45016\\876110533.py\", line 25, in <module>\n",
      "    detectionResults, frame = femodule.extractFeatures(frame)\n",
      "  File \"C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_45016\\2034679685.py\", line 197, in extractFeatures\n",
      "    detectionResults, frame = self.parallelFeatureExtraction(\n",
      "  File \"C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_45016\\2034679685.py\", line 95, in parallelFeatureExtraction\n",
      "    t2.result(),\n",
      "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\concurrent\\futures\\_base.py\", line 438, in result\n",
      "    return self.__get_result()\n",
      "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\concurrent\\futures\\_base.py\", line 390, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\concurrent\\futures\\thread.py\", line 52, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_45016\\2034679685.py\", line 59, in detectPose\n",
      "    results = self.preprocess_landmarks(results[:23], frameSize)\n",
      "  File \"C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_45016\\2034679685.py\", line 111, in preprocess_landmarks\n",
      "    return (np_landmark_list / np_frameSize).ravel()\n",
      "ValueError: operands could not be broadcast together with shapes (23,4) (3,) \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import traceback\n",
    "\n",
    "cam = None\n",
    "# Read one frame from camera\n",
    "def readFrame():\n",
    "    success, frame = cam.read()\n",
    "    if not success: \n",
    "        raise Exception(\"No Frames Read\")\n",
    "    return cv2.flip(frame, 1)\n",
    "\n",
    "\n",
    "cam = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "cam.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cam.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "try:\n",
    "    \n",
    "    while True:\n",
    "        # Read from camera\n",
    "        rawframe = readFrame()\n",
    "\n",
    "        frame = np.copy(rawframe)\n",
    "\n",
    "        detectionResults, frame = femodule.extractFeatures(frame)\n",
    "        \n",
    "        # Show resulting frame\n",
    "        cv2.imshow(\"Sign Language Recognition Prototype\", frame)     \n",
    "\n",
    "        keyPressed = cv2.waitKey(10)\n",
    "        # Stop Program when pressed 'Esc'\n",
    "        if (keyPressed == 27):\n",
    "            raise Exception(\"Finished\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    traceback.print_exc()\n",
    "\n",
    "finally:\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    # del femodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(detectionResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.55139923,  0.73399454, -0.74569803],\n",
       "       [ 0.56648535,  0.68524396, -0.70646536],\n",
       "       [ 0.57641351,  0.68570024, -0.70679051],\n",
       "       [ 0.58612144,  0.68697423, -0.70683694],\n",
       "       [ 0.53418308,  0.68727589, -0.7128588 ],\n",
       "       [ 0.52298796,  0.68843925, -0.71283257],\n",
       "       [ 0.51305228,  0.69001383, -0.71318281],\n",
       "       [ 0.60445964,  0.71692556, -0.40919787],\n",
       "       [ 0.5005421 ,  0.71843433, -0.43512335],\n",
       "       [ 0.57334667,  0.78898698, -0.62870717],\n",
       "       [ 0.53335941,  0.79032856, -0.63620675],\n",
       "       [ 0.68944788,  0.9606145 , -0.20965974],\n",
       "       [ 0.43566662,  0.96137846, -0.25402993],\n",
       "       [ 0.75933588,  1.21941566, -0.30540067],\n",
       "       [ 0.39738399,  1.2676121 , -0.33917922],\n",
       "       [ 0.72616225,  1.33728361, -0.68819159],\n",
       "       [ 0.42404801,  1.39239919, -0.77118653],\n",
       "       [ 0.7201255 ,  1.39245105, -0.80496472],\n",
       "       [ 0.42453066,  1.45391953, -0.86561853],\n",
       "       [ 0.7049216 ,  1.36649776, -0.79739189],\n",
       "       [ 0.43603382,  1.42126441, -0.89484537],\n",
       "       [ 0.69912648,  1.34275317, -0.76160091],\n",
       "       [ 0.44017926,  1.39484775, -0.7962063 ]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detectionResults[138:207].reshape(23, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertX(normalized_x):\n",
    "    return int(normalized_x * 1280)\n",
    "def convertY(normalized_y):\n",
    "    return int(normalized_y * 720)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0)\n",
      "(0, 0)\n",
      "0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "dr = detectionResults\n",
    "print((convertX(dr[63]), convertY(dr[64])))\n",
    "print((convertX(dr[65]), convertY(dr[66])))\n",
    "print(dr[65], dr[66])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_45200\\1325288025.py\", line 24, in <module>\n",
      "    raise Exception(\"Finished\")\n",
      "Exception: Finished\n"
     ]
    }
   ],
   "source": [
    "modFrame = np.copy(rawframe)\n",
    "dr = detectionResults\n",
    "\n",
    "# Draw left hand center\n",
    "cv2.circle(modFrame, (convertX(dr[67]), convertY(dr[68])), 5, (0, 0, 255), 5)\n",
    "\n",
    "# Draw left hand bbox\n",
    "cv2.rectangle(modFrame, \n",
    "              (convertX(dr[63]), convertY(dr[64])),\n",
    "              (convertX(dr[65]), convertY(dr[66])),\n",
    "              (0, 0, 255), 3\n",
    "              )\n",
    "\n",
    "cv2.circle(modFrame, (1200, 700), 5, (0, 255, 255), 5)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        cv2.imshow(\"dsa\", modFrame)\n",
    "        # cv2.imshow(\"dsa\", frame)\n",
    "        \n",
    "        keyPressed = cv2.waitKey(10)\n",
    "        # Stop Program when pressed 'Esc'\n",
    "        if (keyPressed == 27):\n",
    "            raise Exception(\"Finished\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    traceback.print_exc()\n",
    "\n",
    "finally:\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.03531983e-04,  1.88927253e-03, -6.35702629e-05,  3.80028854e-04,\n",
       "        1.93832931e-03,  6.46637927e-05,  4.97945677e-04,  2.53801528e-03,\n",
       "        4.99524933e-05,  3.78504442e-04,  2.56094701e-03,  2.35871039e-04,\n",
       "        4.99892421e-04,  3.11529769e-03,  3.51638114e-04,  3.72882606e-04,\n",
       "        3.13743684e-03,  4.90312185e-04,  5.06585138e-04,  3.20153369e-03,\n",
       "        3.66787123e-04,  3.68840550e-04,  3.21989391e-03,  5.10521932e-04,\n",
       "        4.83982125e-04,  3.31390103e-03,  1.10713672e-04,  3.94875929e-04,\n",
       "        3.34577362e-03,  2.33860523e-04])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mf = np.copy(rawframe)\n",
    "fem = FeatureExtractionModule()\n",
    "fem.detectPose(fem.poseDetector, mf, (1280, 720), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.64296875,  0.81805556, -0.0296875 ],\n",
       "       [ 0.6484375 ,  0.81805556, -0.01875   ],\n",
       "       [ 0.6484375 ,  0.82083333, -0.01875   ],\n",
       "       [ 0.6484375 ,  0.82361111, -0.01875   ],\n",
       "       [ 0.64921875,  0.80833333, -0.03359375],\n",
       "       [ 0.64921875,  0.80416667, -0.03359375],\n",
       "       [ 0.64921875,  0.79861111, -0.03359375],\n",
       "       [ 0.64375   ,  0.81805556,  0.04296875],\n",
       "       [ 0.64453125,  0.78611111, -0.02109375],\n",
       "       [ 0.634375  ,  0.82222222, -0.00390625],\n",
       "       [ 0.63515625,  0.80833333, -0.02265625],\n",
       "       [ 0.60703125,  0.84166667,  0.08203125],\n",
       "       [ 0.6125    ,  0.75138889, -0.046875  ],\n",
       "       [ 0.57734375,  0.87222222,  0.0421875 ],\n",
       "       [ 0.5640625 ,  0.73472222, -0.078125  ],\n",
       "       [ 0.59453125,  0.82222222, -0.0375    ],\n",
       "       [ 0.53203125,  0.79166667, -0.0578125 ],\n",
       "       [ 0.59375   ,  0.81111111, -0.053125  ],\n",
       "       [ 0.5171875 ,  0.80138889, -0.06640625],\n",
       "       [ 0.6       ,  0.80277778, -0.04453125],\n",
       "       [ 0.52109375,  0.81388889, -0.06875   ],\n",
       "       [ 0.5984375 ,  0.80555556, -0.03671875],\n",
       "       [ 0.525     ,  0.80972222, -0.05625   ],\n",
       "       [ 0.5046875 ,  0.81805556,  0.03984375],\n",
       "       [ 0.50546875,  0.75694444, -0.03984375],\n",
       "       [ 0.50625   ,  0.92083333, -0.0765625 ],\n",
       "       [ 0.5140625 ,  0.79861111, -0.20390625],\n",
       "       [ 0.4265625 ,  0.90277778, -0.053125  ],\n",
       "       [ 0.4421875 ,  0.83472222, -0.1265625 ],\n",
       "       [ 0.41875   ,  0.8875    , -0.05078125],\n",
       "       [ 0.434375  ,  0.82916667, -0.11484375],\n",
       "       [ 0.39375   ,  0.93333333, -0.0890625 ],\n",
       "       [ 0.4140625 ,  0.86805556, -0.1390625 ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array(thisn)[:, 1:] / np.array([1280, 720, 1280]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.64309895,  0.81921208, -0.02971784],\n",
       "       [ 0.64914638,  0.81839812, -0.01926368],\n",
       "       [ 0.64897245,  0.82129753, -0.01930911],\n",
       "       [ 0.64877284,  0.82423294, -0.01935484],\n",
       "       [ 0.64952326,  0.80892742, -0.03362121],\n",
       "       [ 0.64965653,  0.80432618, -0.03364923],\n",
       "       [ 0.64978689,  0.79971641, -0.033786  ],\n",
       "       [ 0.64440036,  0.81944329,  0.04302012],\n",
       "       [ 0.64520204,  0.78625327, -0.02152   ],\n",
       "       [ 0.63484842,  0.82259595, -0.00439513],\n",
       "       [ 0.63522363,  0.80940622, -0.02327977],\n",
       "       [ 0.6076892 ,  0.84276557,  0.08261181],\n",
       "       [ 0.61270493,  0.75246572, -0.04764055],\n",
       "       [ 0.57774049,  0.87272847,  0.04224254],\n",
       "       [ 0.56469619,  0.73545492, -0.07852706],\n",
       "       [ 0.5946523 ,  0.82358366, -0.03812525],\n",
       "       [ 0.53232223,  0.79264587, -0.05852298],\n",
       "       [ 0.59378326,  0.811351  , -0.05325757],\n",
       "       [ 0.51767898,  0.80220228, -0.06679534],\n",
       "       [ 0.60058713,  0.80354351, -0.04516131],\n",
       "       [ 0.52152324,  0.81429291, -0.06877722],\n",
       "       [ 0.59910578,  0.80657774, -0.03737321],\n",
       "       [ 0.52545208,  0.81056494, -0.05659753],\n",
       "       [ 0.50517339,  0.81934482,  0.04039843],\n",
       "       [ 0.50591964,  0.75799835, -0.04045757],\n",
       "       [ 0.50666577,  0.9210282 , -0.0769569 ],\n",
       "       [ 0.51484066,  0.79961812, -0.20422256],\n",
       "       [ 0.42731434,  0.90346867, -0.05348158],\n",
       "       [ 0.44266313,  0.83508056, -0.12671612],\n",
       "       [ 0.41895705,  0.88752705, -0.05140147],\n",
       "       [ 0.43479276,  0.83009762, -0.11507098],\n",
       "       [ 0.39400652,  0.93339974, -0.08978383],\n",
       "       [ 0.41450897,  0.86819082, -0.13914554]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[i.x, i.y, i.z] for i in pd.results.pose_landmarks.landmark])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd = PoseDetector(detectionCon=0.5)\n",
    "def f1():\n",
    "    pd.results = None\n",
    "    mf2 = pd.findPose(mf, draw=False)\n",
    "    thisn, thisn2 = pd.findPosition(mf2, bboxWithHands=False)\n",
    "    return (np.array(thisn)[:, 1:] / np.array([1280, 720, 1280]))\n",
    "\n",
    "def f2():\n",
    "    pd.results = None\n",
    "    mf2 = pd.findPose(mf, draw=False)\n",
    "    return np.array([[i.x, i.y, i.z] for i in pd.results.pose_landmarks.landmark])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Documents\\TARC\\FINAL YEAR PROJECT\\Code\\sign-language-recognition\\action_feature_extraction\\data_refine.ipynb Cell 13\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/TARC/FINAL%20YEAR%20PROJECT/Code/sign-language-recognition/action_feature_extraction/data_refine.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtime\u001b[39;00m \u001b[39mimport\u001b[39;00m time\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/TARC/FINAL%20YEAR%20PROJECT/Code/sign-language-recognition/action_feature_extraction/data_refine.ipynb#X30sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m st \u001b[39m=\u001b[39m time()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Documents/TARC/FINAL%20YEAR%20PROJECT/Code/sign-language-recognition/action_feature_extraction/data_refine.ipynb#X30sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m f1()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/TARC/FINAL%20YEAR%20PROJECT/Code/sign-language-recognition/action_feature_extraction/data_refine.ipynb#X30sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(time() \u001b[39m-\u001b[39m st)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/TARC/FINAL%20YEAR%20PROJECT/Code/sign-language-recognition/action_feature_extraction/data_refine.ipynb#X30sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m st \u001b[39m=\u001b[39m time()\n",
      "\u001b[1;32md:\\Documents\\TARC\\FINAL YEAR PROJECT\\Code\\sign-language-recognition\\action_feature_extraction\\data_refine.ipynb Cell 13\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/TARC/FINAL%20YEAR%20PROJECT/Code/sign-language-recognition/action_feature_extraction/data_refine.ipynb#X30sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m mf2 \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mfindPose(mf, draw\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/TARC/FINAL%20YEAR%20PROJECT/Code/sign-language-recognition/action_feature_extraction/data_refine.ipynb#X30sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m thisn, thisn2 \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mfindPosition(mf2, bboxWithHands\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Documents/TARC/FINAL%20YEAR%20PROJECT/Code/sign-language-recognition/action_feature_extraction/data_refine.ipynb#X30sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mreturn\u001b[39;00m (np\u001b[39m.\u001b[39;49marray(thisn)[:, \u001b[39m1\u001b[39;49m:] \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39m1280\u001b[39m, \u001b[39m720\u001b[39m, \u001b[39m1280\u001b[39m]))\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "st = time()\n",
    "f1()\n",
    "print(time() - st)\n",
    "\n",
    "st = time()\n",
    "f2()\n",
    "print(time() - st)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
