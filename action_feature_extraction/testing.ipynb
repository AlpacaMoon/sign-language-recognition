{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from time import time\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "import traceback\n",
    "from cvzone import FPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvzone.HandTrackingModule import HandDetector\n",
    "from cvzone.FaceDetectionModule import FaceDetector\n",
    "from cvzone.PoseModule import PoseDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten a 2d np array into 1d array\n",
    "def flatten2dList(arr, dataType=int):\n",
    "    return np.fromiter(chain.from_iterable(arr), dataType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the largest absolute value in an np array\n",
    "def getAbsLargestVal(arr):\n",
    "    return np.max(np.abs(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offset and normalize the landmark list\n",
    "# Returns a 1d numpy array\n",
    "def preprocess_landmarks(landmark_list):    \n",
    "    landmark_list = np.array(landmark_list, dtype=float)\n",
    "    origin = landmark_list[0]\n",
    "    \n",
    "    # Offset every point with respect to the first point\n",
    "    # Convert to 1D-array\n",
    "    new_landmark_list = (landmark_list - origin).ravel()\n",
    "    \n",
    "    # Get highest absolute value\n",
    "    largest_value = getAbsLargestVal(new_landmark_list)\n",
    "    \n",
    "    # Normalization\n",
    "    if largest_value != 0:\n",
    "        return new_landmark_list / largest_value\n",
    "    return new_landmark_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offset and normalize a BBOX list (BBOX = Bounding Box, used in face and hand detection)\n",
    "# Returns a 1d numpy array\n",
    "def preprocess_bbox(bbox, frameSize):\n",
    "    bbox = np.array(bbox, dtype=float)\n",
    "    # Convert 3rd and 4th element into coordinates instead of width/height\n",
    "    bbox[2] = bbox[0] + bbox[2]\n",
    "    bbox[3] = bbox[1] + bbox[3]\n",
    "\n",
    "    # Normalize against frame size\n",
    "    bbox[0] /= frameSize[0]\n",
    "    bbox[1] /= frameSize[1]\n",
    "    bbox[2] /= frameSize[0]\n",
    "    bbox[3] /= frameSize[1]\n",
    "\n",
    "    return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize a center vertex (a list of 2 elements)\n",
    "# Returns a 1d numpy array\n",
    "def preprocess_center(center, frameSize):\n",
    "    center = np.array(center, dtype=float)\n",
    "    center[0] /= frameSize[0]\n",
    "    center[1] /= frameSize[1]\n",
    "    return center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess (Offset and normalize) the body's landmark list, bbox and center\n",
    "def preprocess_body_part(bodyPart, frameSize):\n",
    "    bodyPart['lmList'] = preprocess_landmarks(bodyPart['lmList'])\n",
    "    bodyPart['bbox'] = preprocess_bbox(bodyPart['bbox'], frameSize)\n",
    "    bodyPart['center'] = preprocess_center(bodyPart['center'], frameSize)\n",
    "    return bodyPart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate empty/placeholder data for a hand \n",
    "# Used when a hand is not detected in frame\n",
    "def generate_empty_hand(type):\n",
    "    return {\n",
    "        'lmList': np.zeros(21 * 3, dtype=int), \n",
    "        'bbox': np.zeros(4, dtype=float), \n",
    "        'center': np.zeros(2, dtype=float), \n",
    "        'type': type\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best matching face, aka the one with the best score (clarity)\n",
    "# and closest to the center of the screen\n",
    "# Since the Neural network will be design to only accept one face\n",
    "def select_best_matching_face(faces, frameSize):\n",
    "    if not faces or len(faces) == 0:\n",
    "        return False\n",
    "    elif len(faces) == 1:\n",
    "        return faces[0]\n",
    "    \n",
    "    def difference(a, b):\n",
    "        return ((a[0] - b[0])**2) + ((a[1] - b[1])**2)\n",
    "    \n",
    "    frameCenter = (frameSize[0] / 2, frameSize[1] / 2)\n",
    "\n",
    "    best_score = faces[0]\n",
    "    best_center = faces[0]\n",
    "    center_diff = difference(faces[0]['center'], frameCenter)\n",
    "\n",
    "    for each in faces:\n",
    "        if difference(each['center'], frameCenter) < center_diff:\n",
    "            best_center = each\n",
    "        if each['score'][0] > best_score['score'][0]:\n",
    "            best_score = each\n",
    "    \n",
    "    if best_center['score'][0] > 0.5:\n",
    "        return best_center\n",
    "    return best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten everything\n",
    "def flattenDetectionResult(obj):\n",
    "    return np.concatenate([obj['lmList'], obj['bbox'], obj['center']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Detects hands, face & pose, \n",
    "# convert them into normalized landmark/keypoint coordinates in a 1D-array, \n",
    "# and also returns the frame with the landmark connections drawn onto it\n",
    "\n",
    "# Serial/Unparallelised version (Old version)\n",
    "def featureExtractionV3(handDetector, faceDetector, poseDetector, frame, draw=True):\n",
    "    def detectHands(handDetector, frame, frameSize, draw):\n",
    "        results = None\n",
    "        # Hand Detection\n",
    "        if (draw):\n",
    "            results, frame = handDetector.findHands(frame, draw=draw)\n",
    "        else:\n",
    "            results = handDetector.findHands(frame, draw=draw)\n",
    "\n",
    "        if not results:\n",
    "            results = [generate_empty_hand('Left'), generate_empty_hand('Right')]\n",
    "        elif len(results) == 1:\n",
    "            if (results[0]['type'] == 'Left'):\n",
    "                results[0] = preprocess_body_part(results[0], frameSize)\n",
    "                results.append(generate_empty_hand('Right'))\n",
    "            else:\n",
    "                results[0] = preprocess_body_part(results[0], frameSize)\n",
    "                results.insert(0, generate_empty_hand('Left'))                         \n",
    "        else:\n",
    "            results[0] = preprocess_body_part(results[0], frameSize)\n",
    "            results[1] = preprocess_body_part(results[1], frameSize)\n",
    "        return results\n",
    "\n",
    "    # Pose Detection\n",
    "    # **We only use the first 23 out of the total 33 landmark points \n",
    "    #   as those represent the lower half body and are irrelevant to sign language interpretation\n",
    "    def detectPose(poseDetector, frame, draw):\n",
    "        frame = poseDetector.findPose(frame, draw=draw)\n",
    "        results, _ = poseDetector.findPosition(frame, bboxWithHands=False)\n",
    "        if results:\n",
    "            results = preprocess_landmarks(results[:23])\n",
    "        else:\n",
    "            results = np.zeros(23, dtype=int)\n",
    "        return results\n",
    "    \n",
    "    # Face Detection\n",
    "    def detectFace(faceDetector, frame, frameSize, draw):\n",
    "        frame, results = faceDetector.findFaces(frame, draw=draw)\n",
    "        if results:\n",
    "            results = select_best_matching_face(results, frameSize)\n",
    "            results['bbox'] = preprocess_bbox(results['bbox'], frameSize)\n",
    "            results['center'] = preprocess_center(results['center'], frameSize)\n",
    "        else:\n",
    "            results = {\n",
    "                'bbox': np.zeros(4, dtype=float), \n",
    "                'center': np.zeros(2, dtype=float)\n",
    "            }\n",
    "        return results\n",
    "\n",
    "    frameSize = (frame.shape[1], frame.shape[0])\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        t1 = executor.submit(detectHands, handDetector, frame, frameSize, draw)\n",
    "        t2 = executor.submit(detectPose, poseDetector, frame, draw)\n",
    "        t3 = executor.submit(detectFace, faceDetector, frame, frameSize, draw)\n",
    "        \n",
    "        # Convert results into 1D-array\n",
    "        detectionResults = flatten2dList([\n",
    "            flattenDetectionResult(t1.result()[0]), \n",
    "            flattenDetectionResult(t1.result()[1]), \n",
    "            t2.result(), \n",
    "            t3.result()['bbox'],\n",
    "            t3.result()['center'],\n",
    "            t3.result()['center'] - t1.result()[0]['center'],\n",
    "            t3.result()['center'] - t1.result()[1]['center']\n",
    "        ], dataType=float)\n",
    "\n",
    "        return detectionResults, frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detectors\n",
    "handDetector = HandDetector(detectionCon=0.5, maxHands=2)\n",
    "faceDetector = FaceDetector(minDetectionCon=0.5)\n",
    "poseDetector = PoseDetector(detectionCon=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import FRAMES_PER_TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24372\\1185570273.py\", line 29, in <module>\n",
      "    raise Exception(\"Finished\")\n",
      "Exception: Finished\n"
     ]
    }
   ],
   "source": [
    "cam = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "cam.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cam.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "fpsReader = FPS()\n",
    "\n",
    "\n",
    "try:\n",
    "    keypointsHistory = deque()\n",
    "    predictionHistory = deque()\n",
    "    timestampHistory = deque()\n",
    "\n",
    "    detectionThreshold = 1.0\n",
    "\n",
    "    lastPredictionTime = time()\n",
    "    predictionCooldown = 1\n",
    "    \n",
    "    while True:\n",
    "\n",
    "        # Read from camera\n",
    "        success, frame = cam.read()\n",
    "\n",
    "        # Pose Detection\n",
    "        detectionResults, frame = featureExtractionV3(\n",
    "            handDetector, faceDetector, poseDetector, frame)\n",
    "        \n",
    "        timestampHistory.append(int(time() * 1000))\n",
    "        \n",
    "        # Semantic Prediction\n",
    "        keypointsHistory.append(detectionResults)\n",
    "        if len(keypointsHistory) > FRAMES_PER_TRAINING:\n",
    "            keypointsHistory.popleft()\n",
    "            timestampHistory.popleft()\n",
    "            # Run prediction\n",
    "        \n",
    "        cv2.putText(frame, ', '.join(predictionHistory), (15, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 3)\n",
    "        \n",
    "        fps, frame = fpsReader.update(frame,pos=(50,80),color=(0,255,0),scale=5,thickness=5)\n",
    "\n",
    "        cv2.imshow(\"Sign Language Recognition Prototype\", frame)     \n",
    "\n",
    "\n",
    "        keyPressed = cv2.waitKey(10)\n",
    "        if (keyPressed == 27):\n",
    "            raise Exception(\"Finished\")\n",
    "        \n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(720, 1280, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hand Tracking Module\n",
    "By: Computer Vision Zone\n",
    "Website: https://www.computervision.zone/\n",
    "\"\"\"\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import math\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "class MyHandDetector:\n",
    "    \"\"\"\n",
    "    Finds Hands using the mediapipe library. Exports the landmarks\n",
    "    in pixel format. Adds extra functionalities like finding how\n",
    "    many fingers are up or the distance between two fingers. Also\n",
    "    provides bounding box info of the hand found.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mode=False, maxHands=2, detectionCon=0.5, minTrackCon=0.5):\n",
    "        \"\"\"\n",
    "        :param mode: In static mode, detection is done on each image: slower\n",
    "        :param maxHands: Maximum number of hands to detect\n",
    "        :param detectionCon: Minimum Detection Confidence Threshold\n",
    "        :param minTrackCon: Minimum Tracking Confidence Threshold\n",
    "        \"\"\"\n",
    "        self.mode = mode\n",
    "        self.maxHands = maxHands\n",
    "        self.detectionCon = detectionCon\n",
    "        self.minTrackCon = minTrackCon\n",
    "\n",
    "        self.mpHands = mp.solutions.hands\n",
    "        self.hands = self.mpHands.Hands(static_image_mode=self.mode, max_num_hands=self.maxHands,\n",
    "                                        min_detection_confidence=self.detectionCon,\n",
    "                                        min_tracking_confidence=self.minTrackCon)\n",
    "        self.mpDraw = mp.solutions.drawing_utils\n",
    "        self.tipIds = [4, 8, 12, 16, 20]\n",
    "        self.fingers = []\n",
    "        self.lmList = []\n",
    "\n",
    "    def findHands(self, img, draw=True, flipType=True):\n",
    "        \"\"\"\n",
    "        Finds hands in a BGR image.\n",
    "        :param img: Image to find the hands in.\n",
    "        :param draw: Flag to draw the output on the image.\n",
    "        :return: Image with or without drawings\n",
    "        \"\"\"\n",
    "\n",
    "        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        self.results = self.hands.process(imgRGB)\n",
    "        allHands = []\n",
    "        h, w, c = img.shape\n",
    "\n",
    "        if self.results.multi_hand_landmarks:\n",
    "            for handType, handLms in zip(self.results.multi_handedness, self.results.multi_hand_landmarks):\n",
    "                myHand = {}\n",
    "                ## lmList\n",
    "                mylmList = []\n",
    "                xList = []\n",
    "                yList = []\n",
    "                for id, lm in enumerate(handLms.landmark):\n",
    "                    px, py, pz = int(lm.x * w), int(lm.y * h), int(lm.z * w)\n",
    "                    mylmList.append([px, py, pz])\n",
    "                    xList.append(px)\n",
    "                    yList.append(py)\n",
    "\n",
    "                ## bbox\n",
    "                xmin, xmax = min(xList), max(xList)\n",
    "                ymin, ymax = min(yList), max(yList)\n",
    "                boxW, boxH = xmax - xmin, ymax - ymin\n",
    "                bbox = xmin, ymin, boxW, boxH\n",
    "                cx, cy = bbox[0] + (bbox[2] // 2), \\\n",
    "                         bbox[1] + (bbox[3] // 2)\n",
    "\n",
    "                myHand[\"lmList\"] = mylmList\n",
    "                myHand[\"bbox\"] = bbox\n",
    "                myHand[\"center\"] = (cx, cy)\n",
    "\n",
    "                if flipType:\n",
    "                    if handType.classification[0].label == \"Right\":\n",
    "                        myHand[\"type\"] = \"Left\"\n",
    "                    else:\n",
    "                        myHand[\"type\"] = \"Right\"\n",
    "                else:\n",
    "                    myHand[\"type\"] = handType.classification[0].label\n",
    "                allHands.append(myHand)\n",
    "\n",
    "                ## draw\n",
    "                if draw:\n",
    "\n",
    "                    global x, y\n",
    "                    x = handLms\n",
    "                    y = self.mpHands.HAND_CONNECTIONS\n",
    "                    self.mpDraw.draw_landmarks(img, handLms,\n",
    "                                               self.mpHands.HAND_CONNECTIONS)\n",
    "                    cv2.rectangle(img, (bbox[0] - 20, bbox[1] - 20),\n",
    "                                  (bbox[0] + bbox[2] + 20, bbox[1] + bbox[3] + 20),\n",
    "                                  (255, 0, 255), 2)\n",
    "                    cv2.putText(img, myHand[\"type\"], (bbox[0] - 30, bbox[1] - 30), cv2.FONT_HERSHEY_PLAIN,\n",
    "                                2, (255, 0, 255), 2)\n",
    "        \n",
    "        if draw:\n",
    "            return allHands, img\n",
    "        else:\n",
    "            return allHands\n",
    "        \n",
    "\n",
    "    def fingersUp(self, myHand):\n",
    "        \"\"\"\n",
    "        Finds how many fingers are open and returns in a list.\n",
    "        Considers left and right hands separately\n",
    "        :return: List of which fingers are up\n",
    "        \"\"\"\n",
    "        myHandType = myHand[\"type\"]\n",
    "        myLmList = myHand[\"lmList\"]\n",
    "        if self.results.multi_hand_landmarks:\n",
    "            fingers = []\n",
    "            # Thumb\n",
    "            if myHandType == \"Right\":\n",
    "                if myLmList[self.tipIds[0]][0] > myLmList[self.tipIds[0] - 1][0]:\n",
    "                    fingers.append(1)\n",
    "                else:\n",
    "                    fingers.append(0)\n",
    "            else:\n",
    "                if myLmList[self.tipIds[0]][0] < myLmList[self.tipIds[0] - 1][0]:\n",
    "                    fingers.append(1)\n",
    "                else:\n",
    "                    fingers.append(0)\n",
    "\n",
    "            # 4 Fingers\n",
    "            for id in range(1, 5):\n",
    "                if myLmList[self.tipIds[id]][1] < myLmList[self.tipIds[id] - 2][1]:\n",
    "                    fingers.append(1)\n",
    "                else:\n",
    "                    fingers.append(0)\n",
    "        return fingers\n",
    "\n",
    "    def findDistance(self, p1, p2, img=None):\n",
    "        \"\"\"\n",
    "        Find the distance between two landmarks based on their\n",
    "        index numbers.\n",
    "        :param p1: Point1\n",
    "        :param p2: Point2\n",
    "        :param img: Image to draw on.\n",
    "        :param draw: Flag to draw the output on the image.\n",
    "        :return: Distance between the points\n",
    "                 Image with output drawn\n",
    "                 Line information\n",
    "        \"\"\"\n",
    "\n",
    "        x1, y1 = p1\n",
    "        x2, y2 = p2\n",
    "        cx, cy = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "        length = math.hypot(x2 - x1, y2 - y1)\n",
    "        info = (x1, y1, x2, y2, cx, cy)\n",
    "        if img is not None:\n",
    "            cv2.circle(img, (x1, y1), 15, (255, 0, 255), cv2.FILLED)\n",
    "            cv2.circle(img, (x2, y2), 15, (255, 0, 255), cv2.FILLED)\n",
    "            cv2.line(img, (x1, y1), (x2, y2), (255, 0, 255), 3)\n",
    "            cv2.circle(img, (cx, cy), 15, (255, 0, 255), cv2.FILLED)\n",
    "            return length, info, img\n",
    "        else:\n",
    "            return length, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hd = MyHandDetector(detectionCon=0.5, maxHands=2)\n",
    "global x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_13996\\1663665762.py\", line 30, in <module>\n",
      "    raise Exception(\"Finished\")\n",
      "Exception: Finished\n"
     ]
    }
   ],
   "source": [
    "cam = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "cam.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cam.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "fpsReader = FPS()\n",
    "\n",
    "timeStats = []\n",
    "\n",
    "try:\n",
    "    initialTime = time()\n",
    "    while True:\n",
    "        startTime = time()\n",
    "\n",
    "        # Read from camera\n",
    "        success, frame = cam.read()\n",
    "        nFrame = np.copy(frame)\n",
    "        \n",
    "        # Pose Detection\n",
    "        hands, frame = hd.findHands(frame, draw=True)\n",
    "        \n",
    "        \n",
    "        fps, frame = fpsReader.update(frame,pos=(50,80),color=(0,255,0),scale=5,thickness=5)\n",
    "\n",
    "        cv2.imshow(\"Sign Language Recognition Prototype\", frame)     \n",
    "\n",
    "        timeStats.append(time() - startTime)\n",
    "\n",
    "        keyPressed = cv2.waitKey(10)\n",
    "        if (keyPressed == 27):\n",
    "            raise Exception(\"Finished\")\n",
    "        \n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mediapipe.framework.formats.landmark_pb2.NormalizedLandmarkList"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,hello\n",
      "1,good/thank\n",
      "2,help\n",
      "3,I/me\n",
      "4,please\n",
      "5,sorry\n",
      "6,welcome\n",
      "7,welcome\n",
      "8,ok\n",
      "9,what\n",
      "10,what\n",
      "11,what\n",
      "12,thank you very much\n",
      "13,deaf\n",
      "14,do not\n",
      "15,feel\n",
      "16,eat/food\n",
      "17,eat a lot\n",
      "18,tired\n",
      "19,because\n",
      "20,sick\n",
      "21,drink\n",
      "22,drink\n",
      "23,apple\n",
      "24,banana\n",
      "25,drive\n",
      "26,again\n",
      "27,also\n",
      "28,ask\n",
      "29,yes\n",
      "30,no\n",
      "31,man\n",
      "32,man\n",
      "33,woman\n",
      "34,woman\n",
      "35,he/she\n",
      "36,bad\n",
      "37,have/has/had (formal)\n",
      "38,have/has/had (informal)\n",
      "39,when\n",
      "40,where\n",
      "41,which\n",
      "42,who\n",
      "43,why\n",
      "44,how\n",
      "45,you\n",
      "46,boy\n",
      "47,girl\n",
      "48,friend\n",
      "49,finish/complete\n",
      "50,finish/complete\n",
      "51,forget\n",
      "52,forget\n",
      "53,give\n",
      "54,give you\n",
      "55,give me\n",
      "56,go\n",
      "57,get\n",
      "58,understand/comprehend\n",
      "59,use\n",
      "60,will\n",
      "61,with\n",
      "62,wait\n",
      "63,work\n",
      "64,they\n",
      "65,their\n",
      "66,school\n",
      "67,write\n",
      "68,send text/message\n",
      "69,email\n",
      "70,email\n",
      "71,home\n",
      "72,but\n",
      "73,should\n",
      "74,not\n",
      "75,my\n",
      "76,name\n",
      "77,like\n",
      "78,say\n",
      "79,cold\n",
      "80,hot\n",
      "81,family\n",
      "82,mother\n",
      "83,father\n",
      "84,many\n",
      "85,few\n",
      "86,now\n",
      "87,later\n",
      "88,today\n",
      "89,tomorrow\n",
      "90,yesterday\n",
      "91,same/also\n",
      "92,remember\n",
      "93,your\n",
      "94,more\n",
      "95,meet\n",
      "96,see\n",
      "97,slow\n",
      "98,fast/quick\n",
      "99,some\n",
      "100,store/shop\n",
      "101,take\n",
      "102,take/bring me\n",
      "103,tell\n",
      "104,think\n",
      "105,want\n",
      "106,inexpensive\n",
      "107,expensive\n",
      "108,that\n",
      "109,this\n",
      "110,here\n",
      "111,near\n",
      "112,far\n",
      "113,cat\n",
      "114,dog\n",
      "115,morning\n",
      "116,night\n",
      "117,beautiful\n",
      "118,open\n",
      "119,close/shut\n",
      "120,close/shut\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('../reference/list.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    \n",
    "    # Iterate through each row in the CSV file\n",
    "    for row in reader:\n",
    "        # Each row is a list of values\n",
    "        print(f\"{row[0]},{row[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16/11/2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from constants import KEYPOINTS_PATH\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "foundEmpty = 0\n",
    "\n",
    "for each in os.listdir(KEYPOINTS_PATH):\n",
    "    thisDir = os.path.join(KEYPOINTS_PATH, each)\n",
    "    \n",
    "    for f in os.listdir(thisDir):\n",
    "        thisF = os.path.join(thisDir, f)\n",
    "        if os.path.splitext(thisF)[1].lower() == '.npy':\n",
    "\n",
    "            load_data = np.load(thisF)\n",
    "            for i, x in enumerate(load_data):\n",
    "                if (np.sum(x) >= -0.00001 and np.sum(x) <= 0.00001):\n",
    "                    foundEmpty += 1\n",
    "                    print(each, i)\n",
    "                    print(x)\n",
    "            \n",
    "\n",
    "foundEmpty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22/11/223"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ogFrame = np.load('sampleRawFrame.npy')\n",
    "newFrame = np.load('../test_img_2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "\n",
    "    cv2.imshow(\"Title\", cv2.flip(newFrame, 1))\n",
    "    \n",
    "    keyPressed = cv2.waitKey(10)\n",
    "    # Stop Program when pressed 'Esc'\n",
    "    if (keyPressed == 27):\n",
    "        cv2.destroyAllWindows() \n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(720, 1280, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ogFrame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 640, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newFrame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Future at 0x2300f7d7b80 state=finished returned NoneType>\n",
      "<class 'concurrent.futures._base.Future'>\n",
      "{<Thread(ThreadPoolExecutor-2_0, started 22684)>}\n",
      "<class 'threading.Thread'> <Thread(ThreadPoolExecutor-2_0, started 22684)>\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def func():\n",
    "    for i in range(10):\n",
    "        ...\n",
    "    return\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    t1 = executor.submit(func)\n",
    "    print(t1)\n",
    "    print(type(t1))\n",
    "    print(executor._threads)\n",
    "    for t in executor._threads:\n",
    "        print(type(t), t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'threading.Thread'> <Thread(Thread-5, stopped 19944)>\n"
     ]
    }
   ],
   "source": [
    "from threading import Thread\n",
    "\n",
    "t = Thread(target=func)\n",
    "t.start()\n",
    "\n",
    "print(type(t), t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
