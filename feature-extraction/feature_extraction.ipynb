{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries / Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvzone.HandTrackingModule import HandDetector\n",
    "from cvzone.FaceDetectionModule import FaceDetector\n",
    "from cvzone.PoseModule import PoseDetector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Extraction (Hand+Face+Pose Detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten a 2d np array into 1d array\n",
    "def flatten2dList(arr, dataType=int):\n",
    "    return np.fromiter(chain.from_iterable(arr), dataType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the largest absolute value in an np array\n",
    "def getAbsLargestVal(arr):\n",
    "    return max(np.max(arr), abs(np.min(arr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offset and normalize the landmark list\n",
    "def preprocess_landmarks(landmark_list):\n",
    "    if not landmark_list:\n",
    "        return []\n",
    "    \n",
    "    # Offset every point with respect to the first point\n",
    "    new_landmark_list = []\n",
    "    origin_x = landmark_list[0][0] \n",
    "    origin_y = landmark_list[0][1]\n",
    "    origin_z = landmark_list[0][2]\n",
    "    for each in landmark_list:\n",
    "        updated_point = [\n",
    "            each[0] - origin_x, \n",
    "            each[1] - origin_y, \n",
    "            each[2] - origin_z\n",
    "        ]\n",
    "        new_landmark_list.append(updated_point)\n",
    "    \n",
    "    # Convert to 1D-array\n",
    "    new_landmark_list = flatten2dList(new_landmark_list)\n",
    "    \n",
    "    # Get highest absolute value\n",
    "    largest_value = getAbsLargestVal(new_landmark_list)\n",
    "    \n",
    "    # Normalization\n",
    "    return new_landmark_list / largest_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offset and normalize a BBOX list (BBOX = Bounding Box, used in face and hand detection)\n",
    "def preprocess_bbox(bbox, frameSize):\n",
    "    bbox = np.array(bbox, dtype=float)\n",
    "    # Convert 3rd and 4th element into coordinates instead of width/height\n",
    "    bbox[2] = bbox[0] + bbox[2]\n",
    "    bbox[3] = bbox[1] + bbox[3]\n",
    "\n",
    "    # Normalize against frame size\n",
    "    bbox[0] /= frameSize[0]\n",
    "    bbox[2] /= frameSize[0]\n",
    "    bbox[1] /= frameSize[1]\n",
    "    bbox[3] /= frameSize[1]\n",
    "\n",
    "    return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize a center vertex (a list of 2 elements)\n",
    "def preprocess_center(center, frameSize):\n",
    "    center = np.array(center)\n",
    "    center[0] /= frameSize[0]\n",
    "    center[1] /= frameSize[1]\n",
    "    return center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess (Offset and normalize) the body's landmark list, bbox and center\n",
    "def preprocess_body_part(bodyPart, frameSize):\n",
    "    bodyPart['lmList'] = preprocess_landmarks(bodyPart['lmList'])\n",
    "    bodyPart['bbox'] = preprocess_bbox(bodyPart['bbox'], frameSize)\n",
    "    bodyPart['center'] = preprocess_center(bodyPart['center'], frameSize)\n",
    "    return bodyPart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate empty/placeholder data for a hand \n",
    "# Used when a hand is not detected in frame\n",
    "def generate_empty_hand(type):\n",
    "    return {\n",
    "        'lmList': np.zeros(21 * 3, dtype=int), \n",
    "        'bbox': np.zeros(4, dtype=float), \n",
    "        'center': np.zeros(2, dtype=float), \n",
    "        'type': type\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best matching face, aka the one with the best score (clarity)\n",
    "# and closest to the center of the screen\n",
    "# Since the Neural network will be design to only accept one face\n",
    "def select_best_matching_face(faces, frameSize):\n",
    "    if not faces:\n",
    "        return False\n",
    "    elif len(faces) == 1:\n",
    "        return faces[0]\n",
    "    \n",
    "    def difference(a, b):\n",
    "        return (a[0] - b[0])**2 + (a[1] - b[1])**2\n",
    "    \n",
    "    frameCenter = (frameSize[0] / 2, frameSize[1] / 2)\n",
    "\n",
    "    best_score = faces[0]\n",
    "    best_center = faces[0]\n",
    "    center_diff = difference(faces[0]['center'], frameCenter)\n",
    "\n",
    "    for each in faces:\n",
    "        if difference(each['center'], frameCenter) < center_diff:\n",
    "            best_center = each\n",
    "        if each['score'][0] > best_score['score'][0]:\n",
    "            best_score = each\n",
    "    \n",
    "    if best_center['score'][0] > 0.5:\n",
    "        return best_center\n",
    "    return best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten everything\n",
    "def flattenDetectionResult(obj):\n",
    "    return np.concatenate([obj['lmList'], obj['bbox'], obj['center']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototype of pose detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "# Detectors\n",
    "handDetector = HandDetector(detectionCon=0.5, maxHands=2)\n",
    "faceDetector = FaceDetector(minDetectionCon=0.5)\n",
    "poseDetector = PoseDetector(detectionCon=0.5)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Read from camera\n",
    "        success, frame = cam.read()\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        results = {}\n",
    "        frameSize = (frame.shape[1], frame.shape[0])\n",
    "\n",
    "        # Hand Detection\n",
    "        results['hands'], frame = handDetector.findHands(frame, draw=True)\n",
    "        if not results['hands']:\n",
    "            results['hands'] = [generate_empty_hand('Left'), generate_empty_hand('Right')]\n",
    "        elif len(results['hands']) == 1:\n",
    "            if (results['hands'][0]['type'] == 'Left'):\n",
    "                results['hands'][0] = preprocess_body_part(results['hands'][0], frameSize)\n",
    "                results['hands'].append(generate_empty_hand('Right'))\n",
    "            else:\n",
    "                results['hands'][0] = preprocess_body_part(results['hands'][0], frameSize)\n",
    "                results['hands'].insert(0, generate_empty_hand('Left'))                         \n",
    "        else:\n",
    "            results['hands'][0] = preprocess_body_part(results['hands'][0], frameSize)\n",
    "            results['hands'][1] = preprocess_body_part(results['hands'][1], frameSize)\n",
    "\n",
    "        # Pose Detection\n",
    "        # * We only use the first 23 out of the total 33 landmark points \n",
    "        #   as those represent the lower half body and are irrelevant\n",
    "        frame = poseDetector.findPose(frame, draw=True)\n",
    "        results['pose'] = {}\n",
    "        results['pose']['lmList'], tempPoseBbox = poseDetector.findPosition(frame, bboxWithHands=False)\n",
    "        if results['pose']['lmList'] and tempPoseBbox:\n",
    "            results['pose']['lmList'] = results['pose']['lmList'][:23]\n",
    "            results['pose']['bbox'] = tempPoseBbox['bbox']\n",
    "            results['pose']['center'] = tempPoseBbox['center']\n",
    "            results['pose'] = preprocess_body_part(results['pose'], frameSize)\n",
    "        else:\n",
    "            results['pose']['lmList'] = np.zeros(23 * 3, dtype=int)\n",
    "            results['pose']['bbox'] = np.zeros(4, dtype=float)\n",
    "            results['pose']['center'] = np.zeros(2, dtype=float)\n",
    "            \n",
    "        # Face Detection\n",
    "        frame, results['face'] = faceDetector.findFaces(frame, draw=True)\n",
    "        if results['face']:\n",
    "            results['face'] = select_best_matching_face(results['face'], frameSize)\n",
    "            results['face']['bbox'] = preprocess_bbox(results['face']['bbox'], frameSize)\n",
    "            results['face']['center'] = preprocess_center(results['face']['center'], frameSize)\n",
    "        else:\n",
    "            results['face'] = {\n",
    "                'bbox': np.zeros(4, dtype=float), \n",
    "                'center': np.zeros(2, dtype=float)\n",
    "            }\n",
    "\n",
    "        # Convert results into 1D-array\n",
    "        detectionResults = np.concatenate([\n",
    "            flattenDetectionResult(results['hands'][0]), \n",
    "            flattenDetectionResult(results['hands'][1]), \n",
    "            flattenDetectionResult(results['pose']), \n",
    "            results['face']['bbox'], \n",
    "            results['face']['center']\n",
    "        ])\n",
    "\n",
    "        # Show frame\n",
    "        cv2.imshow(\"Sign Language Recognition Prototype\", frame)\n",
    "\n",
    "        keyPressed = cv2.waitKey(10)\n",
    "        if (keyPressed == ord('q')):\n",
    "            pass\n",
    "\n",
    "        # Pressed 'Esc'\n",
    "        if (keyPressed == 27):\n",
    "            break\n",
    "\n",
    "except Exception as e:\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    raise e\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each output is now a 219 length 1D-array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.25      , -0.09558824, -0.05147059,\n",
       "        0.44852941, -0.28676471, -0.02941176,  0.54411765, -0.45588235,\n",
       "       -0.01470588,  0.52205882, -0.58088235,  0.        ,  0.38235294,\n",
       "       -0.51470588,  0.07352941,  0.47794118, -0.71323529,  0.04411765,\n",
       "        0.54411765, -0.82352941,  0.        ,  0.60294118, -0.90441176,\n",
       "       -0.02941176,  0.26470588, -0.55882353,  0.07352941,  0.34558824,\n",
       "       -0.76470588,  0.04411765,  0.39705882, -0.89705882,  0.        ,\n",
       "        0.44852941, -1.        , -0.03676471,  0.14705882, -0.55882353,\n",
       "        0.05147059,  0.21323529, -0.75735294,  0.01470588,  0.25735294,\n",
       "       -0.88970588, -0.01470588,  0.29411765, -0.99264706, -0.03676471,\n",
       "        0.00735294, -0.52205882,  0.02205882,  0.05147059, -0.67647059,\n",
       "       -0.00735294,  0.07352941, -0.77941176, -0.01470588,  0.09558824,\n",
       "       -0.875     , -0.02205882,  0.1625    ,  0.57291667,  0.290625  ,\n",
       "        0.85625   ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.00409836,  0.04918033, -0.10245902,  0.00819672,\n",
       "        0.09016393, -0.10655738,  0.01229508,  0.12704918, -0.10245902,\n",
       "        0.01639344, -0.07786885, -0.0942623 ,  0.0204918 , -0.11885246,\n",
       "       -0.08606557,  0.02459016, -0.15163934, -0.07786885,  0.02868852,\n",
       "        0.18442623, -0.05327869,  0.03278689, -0.19262295, -0.01229508,\n",
       "        0.03688525,  0.08606557,  0.11885246,  0.04098361, -0.06967213,\n",
       "        0.12704918,  0.04508197,  0.54918033,  0.45081967,  0.04918033,\n",
       "       -0.41803279,  0.45491803,  0.05327869,  0.87704918,  1.        ,\n",
       "        0.05737705, -0.91393443,  0.86065574,  0.06147541,  0.82786885,\n",
       "        0.97540984,  0.06557377, -0.73360656,  0.35245902,  0.06967213,\n",
       "        0.8442623 ,  0.99180328,  0.07377049, -0.70901639,  0.12295082,\n",
       "        0.07786885,  0.80327869,  0.90983607,  0.08196721, -0.67622951,\n",
       "        0.08606557,  0.08606557,  0.7704918 ,  0.9057377 ,  0.09016393,\n",
       "       -0.65163934,  0.17213115,  0.1125    ,  0.45416667,  0.85      ,\n",
       "        2.95625   ,  0.        ,  1.        ,  0.35      ,  0.63333333,\n",
       "        0.546875  ,  0.89583333,  0.        ,  0.        ])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "detectionResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparation for Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'thank you', 'help']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from files_io import readActionLabels, initActionLabelFolders\n",
    "\n",
    "action_labels = readActionLabels()\n",
    "initActionLabelFolders(action_labels)\n",
    "action_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detects hands, face & pose, \n",
    "# convert them into normalized landmark/keypoint coordinates in a 1D-array, \n",
    "# and also returns the frame with the landmark connections drawn onto it\n",
    "\n",
    "# Serial/Unparallelised version (Old version)\n",
    "def featureExtraction(handDetector, faceDetector, poseDetector, frame):\n",
    "    results = {}\n",
    "    frameSize = (frame.shape[1], frame.shape[0])\n",
    "\n",
    "    # Hand Detection\n",
    "    results['hands'], frame = handDetector.findHands(frame, draw=True)\n",
    "    if not results['hands']:\n",
    "        results['hands'] = [generate_empty_hand('Left'), generate_empty_hand('Right')]\n",
    "    elif len(results['hands']) == 1:\n",
    "        if (results['hands'][0]['type'] == 'Left'):\n",
    "            results['hands'][0] = preprocess_body_part(results['hands'][0], frameSize)\n",
    "            results['hands'].append(generate_empty_hand('Right'))\n",
    "        else:\n",
    "            results['hands'][0] = preprocess_body_part(results['hands'][0], frameSize)\n",
    "            results['hands'].insert(0, generate_empty_hand('Left'))                         \n",
    "    else:\n",
    "        results['hands'][0] = preprocess_body_part(results['hands'][0], frameSize)\n",
    "        results['hands'][1] = preprocess_body_part(results['hands'][1], frameSize)\n",
    "\n",
    "    # Pose Detection\n",
    "    # * We only use the first 23 out of the total 33 landmark points \n",
    "    #   as those represent the lower half body and are irrelevant\n",
    "    frame = poseDetector.findPose(frame, draw=True)\n",
    "    results['pose'] = {}\n",
    "    results['pose']['lmList'], tempPoseBbox = poseDetector.findPosition(frame, bboxWithHands=False)\n",
    "    if results['pose']['lmList'] and tempPoseBbox:\n",
    "        results['pose']['lmList'] = results['pose']['lmList'][:23]\n",
    "        results['pose']['bbox'] = tempPoseBbox['bbox']\n",
    "        results['pose']['center'] = tempPoseBbox['center']\n",
    "        results['pose'] = preprocess_body_part(results['pose'], frameSize)\n",
    "    else:\n",
    "        results['pose']['lmList'] = np.zeros(23 * 3, dtype=int)\n",
    "        results['pose']['bbox'] = np.zeros(4, dtype=float)\n",
    "        results['pose']['center'] = np.zeros(2, dtype=float)\n",
    "        \n",
    "    # Face Detection\n",
    "    frame, results['face'] = faceDetector.findFaces(frame, draw=True)\n",
    "    if results['face']:\n",
    "        results['face'] = select_best_matching_face(results['face'], frameSize)\n",
    "        results['face']['bbox'] = preprocess_bbox(results['face']['bbox'], frameSize)\n",
    "        results['face']['center'] = preprocess_center(results['face']['center'], frameSize)\n",
    "    else:\n",
    "        results['face'] = {\n",
    "            'bbox': np.zeros(4, dtype=float), \n",
    "            'center': np.zeros(2, dtype=float)\n",
    "        }\n",
    "\n",
    "    # Convert results into 1D-array\n",
    "    detectionResults = np.concatenate([\n",
    "        flattenDetectionResult(results['hands'][0]), \n",
    "        flattenDetectionResult(results['hands'][1]), \n",
    "        flattenDetectionResult(results['pose']), \n",
    "        results['face']['bbox'], \n",
    "        results['face']['center']\n",
    "    ])\n",
    "\n",
    "    return detectionResults, frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real Time Detection UI Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read one frame from camera\n",
    "def readFrame():\n",
    "    success, frame = cam.read()\n",
    "    if not success: \n",
    "        raise Exception(\"No Frames Read\")\n",
    "    return cv2.flip(frame, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pause recording upon \"Space\"\n",
    "def pauseWhenSpace(trainingNum, actionStr):\n",
    "    while True:\n",
    "        frame = readFrame()\n",
    "        cv2.putText(frame, f'Training #{trainingNum + 1} for \\'{actionStr}\\'', (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "        cv2.putText(frame, f'Pausing...', (40, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (20, 255, 125), 3)\n",
    "        cv2.imshow(\"Sign Language Recognition Prototype\", frame)\n",
    "\n",
    "        # If pressed resume, do countdown\n",
    "        keyPressed = cv2.waitKey(100)   # Read key every 100ms, i.e. lock fps to 10fps\n",
    "        if keyPressed == 32:    # 32 == Space\n",
    "            resume = False\n",
    "            for i in range(3):\n",
    "                for _ in range(10):\n",
    "                    temp_frame = readFrame()\n",
    "                    cv2.putText(temp_frame, f'Training #{trainingNum + 1} for \\'{actionStr}\\'', (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "                    cv2.putText(temp_frame, f'Resuming in {3 - i}', (20, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 120), 3)\n",
    "                    cv2.imshow(\"Sign Language Recognition Prototype\", temp_frame)\n",
    "                    tempKey = cv2.waitKey(100)\n",
    "                    if (tempKey == 27):\n",
    "                        raise Exception(\"Finished\")\n",
    "                    # If pressed paused again, stop resuming and continue pausing\n",
    "                    elif tempKey == 32:\n",
    "                        resume = True\n",
    "                        break\n",
    "                if resume:\n",
    "                    break\n",
    "            if not resume:\n",
    "                return\n",
    "            \n",
    "        elif keyPressed == 27:\n",
    "            raise Exception(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display countdown (1, 2, 3)\n",
    "def countdownFromThree(trainingNum, actionStr):\n",
    "    # Count down 3 seconds on every new training\n",
    "    for i in range(3):\n",
    "\n",
    "        # Using iteration of 10 frames (100ms each) so that the display is still going on 10fps\n",
    "        for _ in range(10):\n",
    "            frame = readFrame()\n",
    "            \n",
    "            cv2.putText(frame, f'Training #{trainingNum + 1} for \\'{actionStr}\\'', (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "            cv2.putText(frame, f'Next Training in {3 - i}', (20, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 3)\n",
    "            cv2.imshow(\"Sign Language Recognition Prototype\", frame)\n",
    "            \n",
    "            tempKey = cv2.waitKey(100)\n",
    "            if (tempKey == 27):     # Pressed Esc\n",
    "                raise Exception(\"Finished\")\n",
    "            elif tempKey == 32:     # Pressed Space\n",
    "                pauseWhenSpace(trainingNum, actionStr)\n",
    "                return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recording Label (Create Training Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which action to record\n",
    "action = action_labels[2]\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "cam = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "cam.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cam.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "# Detectors\n",
    "handDetector = HandDetector(detectionCon=0.5, maxHands=2)\n",
    "faceDetector = FaceDetector(minDetectionCon=0.5)\n",
    "poseDetector = PoseDetector(detectionCon=0.5)\n",
    "\n",
    "try:\n",
    "    startTime = time()\n",
    "\n",
    "    for training in range(TRAININGS_PER_LABEL): \n",
    "        trainingResults = []\n",
    "        for frame_num in range(TRAININGS_PER_LABEL):\n",
    "\n",
    "            # Countdown\n",
    "            if frame_num == 0:\n",
    "                countdownFromThree(training, action)\n",
    "                startTime = time()\n",
    "        \n",
    "            # Read from camera\n",
    "            frame = readFrame()\n",
    "\n",
    "            detectionResults, frame = featureExtraction(\n",
    "                handDetector, faceDetector, poseDetector, frame)\n",
    "            \n",
    "            # Show resulting frame\n",
    "            cv2.putText(frame, f'Training #{training + 1} for \\'{action}\\'', (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "            cv2.imshow(\"Sign Language Recognition Prototype\", frame)     \n",
    "\n",
    "            # Save the results\n",
    "            trainingResults.append(detectionResults)\n",
    "\n",
    "            keyPressed = cv2.waitKey(10)\n",
    "            # Stop Program when pressed 'Esc'\n",
    "            if (keyPressed == 27):\n",
    "                raise Exception(\"Finished\")\n",
    "\n",
    "        # After all frames are finished for each training:\n",
    "        # save as .npy\n",
    "        \n",
    "        # IMPORTANT: THIS LINE IS DISABLED IN CASE OF ACCIDENTALLY OVERWRITING DATA\n",
    "        # Enable it ONLY during data collection\n",
    "        # np.save(os.path.join(KEYPOINTS_PATH, action, str(training)), np.array(trainingResults))\n",
    "\n",
    "except Exception as e:\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(e)\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 219)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.load(os.path.join(KEYPOINTS_PATH, action, '0.npy')).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Time Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from time import time\n",
    "from cvzone import FPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name 'featureExtraction' is not defined\n"
     ]
    }
   ],
   "source": [
    "cam = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "cam.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cam.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "# Detectors\n",
    "handDetector = HandDetector(detectionCon=0.5, maxHands=2)\n",
    "faceDetector = FaceDetector(minDetectionCon=0.5)\n",
    "poseDetector = PoseDetector(detectionCon=0.5)\n",
    "\n",
    "fpsReader = FPS()\n",
    "\n",
    "timeStats = []\n",
    "\n",
    "try:\n",
    "\n",
    "    keypointsHistory = deque()\n",
    "    predictionHistory = deque()\n",
    "    detectionThreshold = 1.0\n",
    "\n",
    "    lastPredictionTime = time()\n",
    "    predictionCooldown = 1\n",
    "\n",
    "    while True:\n",
    "        startTime = time()\n",
    "\n",
    "        # Read from camera\n",
    "        success, frame = cam.read()\n",
    "        if not success: \n",
    "            raise Exception(\"No Frames Read\")\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # Pose Detection\n",
    "        detectionResults, frame = featureExtraction(\n",
    "            handDetector, faceDetector, poseDetector, frame)\n",
    "        \n",
    "        # Semantic Prediction\n",
    "        keypointsHistory.append(detectionResults)\n",
    "        if len(keypointsHistory) > FRAMES_PER_TRAINING:\n",
    "            keypointsHistory.popleft()\n",
    "\n",
    "            \n",
    "            # if time() > lastPredictionTime + predictionCooldown:\n",
    "            #     predictionResults = model.predict(\n",
    "            #         np.expand_dims(keypointsHistory, axis=0), \n",
    "            #         verbose=0, \n",
    "            #         use_multiprocessing=True, \n",
    "            #         workers=4\n",
    "            #         )[0]\n",
    "            #     predWord = action_labels[np.argmax(predictionResults)]\n",
    "            #     predAccuracy = predictionResults[np.argmax(predictionResults)]\n",
    "\n",
    "            #     if predAccuracy >= detectionThreshold:\n",
    "            #         lastPredictionTime = time()\n",
    "                    \n",
    "            #         predictionHistory.append(predWord)\n",
    "            #         if len(predictionHistory) > 5:\n",
    "            #             predictionHistory.popleft()\n",
    "        \n",
    "        cv2.putText(frame, ', '.join(predictionHistory), (15, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 3)\n",
    "        \n",
    "        fps, frame = fpsReader.update(frame,pos=(950,80),color=(0,255,0),scale=5,thickness=5)\n",
    "\n",
    "\n",
    "        # Show resulting frame\n",
    "        # cv2.putText(frame, f'Training #{training + 1} for \\'{action}\\'', (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "        cv2.imshow(\"Sign Language Recognition Prototype\", frame)     \n",
    "\n",
    "        timeStats.append(time() - startTime)\n",
    "\n",
    "        keyPressed = cv2.waitKey(15)\n",
    "        # Stop Program when pressed 'Esc'\n",
    "        if (keyPressed == 27):\n",
    "            raise Exception(\"Finished\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(e)\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.07875680923461914,\n",
       " 0.07371330261230469,\n",
       " 0.0756688117980957,\n",
       " 0.07382822036743164,\n",
       " 0.07182097434997559,\n",
       " 0.14654779434204102,\n",
       " 0.13663458824157715,\n",
       " 0.13762927055358887,\n",
       " 0.13364148139953613,\n",
       " 0.173537015914917,\n",
       " 0.16253042221069336,\n",
       " 0.16656756401062012,\n",
       " 0.1545863151550293,\n",
       " 0.1565384864807129,\n",
       " 0.16048026084899902,\n",
       " 0.1515054702758789,\n",
       " 0.1635282039642334,\n",
       " 0.17353558540344238,\n",
       " 0.15356874465942383,\n",
       " 0.15557503700256348,\n",
       " 0.15255475044250488,\n",
       " 0.1715106964111328,\n",
       " 0.15854263305664062,\n",
       " 0.15158939361572266,\n",
       " 0.1595776081085205,\n",
       " 0.16945433616638184]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "timeStats[10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "# Detects hands, face & pose, \n",
    "# convert them into normalized landmark/keypoint coordinates in a 1D-array, \n",
    "# and also returns the frame with the landmark connections drawn onto it\n",
    "def featureExtractionV2(handDetector, faceDetector, poseDetector, frame):\n",
    "    results = {}\n",
    "    frameSize = (frame.shape[1], frame.shape[0])\n",
    "\n",
    "    def detectHands(frame, handDetector, frameSize):\n",
    "        results = {}\n",
    "        # Hand Detection\n",
    "        results['hands'], frame = handDetector.findHands(frame, draw=True)\n",
    "        if not results['hands']:\n",
    "            results['hands'] = [generate_empty_hand('Left'), generate_empty_hand('Right')]\n",
    "        elif len(results['hands']) == 1:\n",
    "            results['hands'][0] = preprocess_body_part(results['hands'][0], frameSize)\n",
    "            \n",
    "            if (results['hands'][0]['type'] == 'Left'):\n",
    "                results['hands'].append(generate_empty_hand('Right'))\n",
    "            else:\n",
    "                results['hands'].insert(0, generate_empty_hand('Left'))                         \n",
    "        else:\n",
    "            results['hands'][0] = preprocess_body_part(results['hands'][0], frameSize)\n",
    "            results['hands'][1] = preprocess_body_part(results['hands'][1], frameSize)\n",
    "        return results['hands']\n",
    "\n",
    "    def detectPose(frame, poseDetector, frameSize):\n",
    "        results = {}\n",
    "        # Pose Detection\n",
    "        # * We only use the first 23 out of the total 33 landmark points \n",
    "        #   as those represent the lower half body and are irrelevant\n",
    "        frame = poseDetector.findPose(frame, draw=True)\n",
    "        results['pose'] = {}\n",
    "        results['pose']['lmList'], tempPoseBbox = poseDetector.findPosition(frame, bboxWithHands=False)\n",
    "        if results['pose']['lmList'] and tempPoseBbox:\n",
    "            results['pose']['lmList'] = results['pose']['lmList'][:23]\n",
    "            results['pose']['bbox'] = tempPoseBbox['bbox']\n",
    "            results['pose']['center'] = tempPoseBbox['center']\n",
    "            results['pose'] = preprocess_body_part(results['pose'], frameSize)\n",
    "        else:\n",
    "            results['pose']['lmList'] = np.zeros(23 * 3, dtype=int)\n",
    "            results['pose']['bbox'] = np.zeros(4, dtype=float)\n",
    "            results['pose']['center'] = np.zeros(2, dtype=float)\n",
    "        return results['pose']\n",
    "            \n",
    "\n",
    "    def detectFace(frame, faceDetector, frameSize):\n",
    "        results = {}\n",
    "        # Face Detection\n",
    "        frame, results['face'] = faceDetector.findFaces(frame, draw=True)\n",
    "        if results['face']:\n",
    "            results['face'] = select_best_matching_face(results['face'], frameSize)\n",
    "            results['face']['bbox'] = preprocess_bbox(results['face']['bbox'], frameSize)\n",
    "            results['face']['center'] = preprocess_center(results['face']['center'], frameSize)\n",
    "        else:\n",
    "            results['face'] = {\n",
    "                'bbox': np.zeros(4, dtype=float), \n",
    "                'center': np.zeros(2, dtype=float)\n",
    "            }\n",
    "        return results['face']\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        # t1 = executor.submit(detectHands, frame, handDetector, frameSize)\n",
    "        # t2 = executor.submit(detectPose, frame, poseDetector, frameSize)\n",
    "        t3 = executor.submit(detectFace, frame, faceDetector, frameSize)\n",
    "\n",
    "\n",
    "        # Convert results into 1D-array\n",
    "        detectionResults = np.concatenate([\n",
    "            flattenDetectionResult(t1.result()[0]), \n",
    "            flattenDetectionResult(t1.result()[1]), \n",
    "            flattenDetectionResult(t2.result()), \n",
    "            t3.result()['bbox'], \n",
    "            t3.result()['center']\n",
    "        ])\n",
    "\n",
    "        return detectionResults, frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "cam = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "cam.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cam.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "fpsReader = FPS()\n",
    "\n",
    "timeStats = []\n",
    "\n",
    "try:\n",
    "    initialTime = time()\n",
    "    while True:\n",
    "        startTime = time()\n",
    "\n",
    "        # Read from camera\n",
    "        success, frame = cam.read()\n",
    "        \n",
    "        # Pose Detection\n",
    "        detectionResults, frame = featureExtractionV2(\n",
    "            handDetector, faceDetector, poseDetector, frame)\n",
    "        \n",
    "        # fps, frame = fpsReader.update(frame,pos=(50,80),color=(0,255,0),scale=5,thickness=5)\n",
    "\n",
    "\n",
    "        # Show resulting frame\n",
    "        # cv2.putText(frame, f'Training #{training + 1} for \\'{action}\\'', (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "        cv2.imshow(\"Sign Language Recognition Prototype\", frame)     \n",
    "\n",
    "        timeStats.append(time() - startTime)\n",
    "\n",
    "        keyPressed = cv2.waitKey(15)\n",
    "        # Stop Program when pressed 'Esc'\n",
    "        if (keyPressed == 27):\n",
    "            raise Exception(\"Finished\")\n",
    "        \n",
    "        # if time() - initialTime > 10:\n",
    "        #     raise Exception()\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(e)\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05556477281384002"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.array(timeStats[1:]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016924326368373075"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.array(timeStats[1:]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09001387300945464"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.array(timeStats[1:]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06750456676926724"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.array(timeStats[1:]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "cam.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cam.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "success, frame = cam.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "type(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
