{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries / Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvzone.HandTrackingModule import HandDetector\n",
    "from cvzone.FaceDetectionModule import FaceDetector\n",
    "from cvzone.PoseModule import PoseDetector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Extraction (Hand+Face+Pose Detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten a 2d np array into 1d array\n",
    "def flatten2dList(arr, dataType=int):\n",
    "    return np.fromiter(chain.from_iterable(arr), dataType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the largest absolute value in an np array\n",
    "def getAbsLargestVal(arr):\n",
    "    return np.max(np.abs(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offset and normalize the landmark list\n",
    "# Returns a 1d numpy array\n",
    "def preprocess_landmarks(landmark_list):\n",
    "    if not landmark_list:\n",
    "        return []\n",
    "    \n",
    "    landmark_list = np.array(landmark_list, dtype=float)\n",
    "    origin = landmark_list[0]\n",
    "    \n",
    "    # Offset every point with respect to the first point\n",
    "    # Convert to 1D-array\n",
    "    new_landmark_list = (landmark_list - origin).ravel()\n",
    "    \n",
    "    # Get highest absolute value\n",
    "    largest_value = getAbsLargestVal(new_landmark_list)\n",
    "    \n",
    "    # Normalization\n",
    "    return new_landmark_list / largest_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offset and normalize a BBOX list (BBOX = Bounding Box, used in face and hand detection)\n",
    "# Returns a 1d numpy array\n",
    "def preprocess_bbox(bbox, frameSize):\n",
    "    bbox = np.array(bbox, dtype=float)\n",
    "    # Convert 3rd and 4th element into coordinates instead of width/height\n",
    "    bbox[2] = bbox[0] + bbox[2]\n",
    "    bbox[3] = bbox[1] + bbox[3]\n",
    "\n",
    "    # Normalize against frame size\n",
    "    bbox[0] /= frameSize[0]\n",
    "    bbox[1] /= frameSize[1]\n",
    "    bbox[2] /= frameSize[0]\n",
    "    bbox[3] /= frameSize[1]\n",
    "\n",
    "    return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize a center vertex (a list of 2 elements)\n",
    "# Returns a 1d numpy array\n",
    "def preprocess_center(center, frameSize):\n",
    "    center = np.array(center, dtype=float)\n",
    "    center[0] /= frameSize[0]\n",
    "    center[1] /= frameSize[1]\n",
    "    return center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess (Offset and normalize) the body's landmark list, bbox and center\n",
    "def preprocess_body_part(bodyPart, frameSize):\n",
    "    bodyPart['lmList'] = preprocess_landmarks(bodyPart['lmList'])\n",
    "    bodyPart['bbox'] = preprocess_bbox(bodyPart['bbox'], frameSize)\n",
    "    bodyPart['center'] = preprocess_center(bodyPart['center'], frameSize)\n",
    "    return bodyPart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate empty/placeholder data for a hand \n",
    "# Used when a hand is not detected in frame\n",
    "def generate_empty_hand(type):\n",
    "    return {\n",
    "        'lmList': np.zeros(21 * 3, dtype=int), \n",
    "        'bbox': np.zeros(4, dtype=float), \n",
    "        'center': np.zeros(2, dtype=float), \n",
    "        'type': type\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best matching face, aka the one with the best score (clarity)\n",
    "# and closest to the center of the screen\n",
    "# Since the Neural network will be design to only accept one face\n",
    "def select_best_matching_face(faces, frameSize):\n",
    "    if not faces:\n",
    "        return False\n",
    "    elif len(faces) == 1:\n",
    "        return faces[0]\n",
    "    \n",
    "    def difference(a, b):\n",
    "        return (a[0] - b[0])**2 + (a[1] - b[1])**2\n",
    "    \n",
    "    frameCenter = (frameSize[0] / 2, frameSize[1] / 2)\n",
    "\n",
    "    best_score = faces[0]\n",
    "    best_center = faces[0]\n",
    "    center_diff = difference(faces[0]['center'], frameCenter)\n",
    "\n",
    "    for each in faces:\n",
    "        if difference(each['center'], frameCenter) < center_diff:\n",
    "            best_center = each\n",
    "        if each['score'][0] > best_score['score'][0]:\n",
    "            best_score = each\n",
    "    \n",
    "    if best_center['score'][0] > 0.5:\n",
    "        return best_center\n",
    "    return best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten everything\n",
    "def flattenDetectionResult(obj):\n",
    "    return np.fromiter(chain.from_iterable([obj['lmList'], obj['bbox'], obj['center']]), float)\n",
    "    # return np.concatenate([obj['lmList'], obj['bbox'], obj['center']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparation for Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'thank you', 'help']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from files_io import readActionLabels, initActionLabelFolders\n",
    "\n",
    "action_labels = readActionLabels()\n",
    "initActionLabelFolders(action_labels)\n",
    "action_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Feature Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detects hands, face & pose, \n",
    "# convert them into normalized landmark/keypoint coordinates in a 1D-array, \n",
    "# and also returns the frame with the landmark connections drawn onto it\n",
    "\n",
    "# Serial/Unparallelised version (Old version)\n",
    "def featureExtraction(handDetector, faceDetector, poseDetector, frame):\n",
    "    results = {}\n",
    "    frameSize = (frame.shape[1], frame.shape[0])\n",
    "\n",
    "    tempResults = {}\n",
    "\n",
    "    # Hand Detection\n",
    "    results['hands'], frame = handDetector.findHands(frame, draw=True)\n",
    "    tempResults['hands'] = results['hands'].copy()\n",
    "    if not results['hands']:\n",
    "        results['hands'] = [generate_empty_hand('Left'), generate_empty_hand('Right')]\n",
    "    elif len(results['hands']) == 1:\n",
    "        if (results['hands'][0]['type'] == 'Left'):\n",
    "            results['hands'][0] = preprocess_body_part(results['hands'][0], frameSize)\n",
    "            results['hands'].append(generate_empty_hand('Right'))\n",
    "        else:\n",
    "            results['hands'][0] = preprocess_body_part(results['hands'][0], frameSize)\n",
    "            results['hands'].insert(0, generate_empty_hand('Left'))                         \n",
    "    else:\n",
    "        results['hands'][0] = preprocess_body_part(results['hands'][0], frameSize)\n",
    "        results['hands'][1] = preprocess_body_part(results['hands'][1], frameSize)\n",
    "\n",
    "    # Pose Detection\n",
    "    # **We only use the first 23 out of the total 33 landmark points \n",
    "    #   as those represent the lower half body and are irrelevant to sign language interpretation\n",
    "    frame = poseDetector.findPose(frame, draw=True)\n",
    "    results['pose'] = {}\n",
    "    results['pose']['lmList'], tempPoseBbox = poseDetector.findPosition(frame, bboxWithHands=False)\n",
    "    if results['pose']['lmList'] and tempPoseBbox:\n",
    "        results['pose']['lmList'] = results['pose']['lmList'][:23]\n",
    "        results['pose']['bbox'] = tempPoseBbox['bbox']\n",
    "        results['pose']['center'] = tempPoseBbox['center']\n",
    "        \n",
    "        tempResults['pose'] = results['pose'].copy()\n",
    "\n",
    "        results['pose'] = preprocess_body_part(results['pose'], frameSize)\n",
    "    else:\n",
    "        results['pose']['lmList'] = np.zeros(23 * 3, dtype=int)\n",
    "        results['pose']['bbox'] = np.zeros(4, dtype=float)\n",
    "        results['pose']['center'] = np.zeros(2, dtype=float)\n",
    "        \n",
    "\n",
    "    \n",
    "    # Face Detection\n",
    "    frame, results['face'] = faceDetector.findFaces(frame, draw=True)\n",
    "    if results['face']:\n",
    "        results['face'] = select_best_matching_face(results['face'], frameSize)\n",
    "\n",
    "        \n",
    "        tempResults['face'] = results['face'].copy()\n",
    "\n",
    "        results['face']['bbox'] = preprocess_bbox(results['face']['bbox'], frameSize)\n",
    "        results['face']['center'] = preprocess_center(results['face']['center'], frameSize)\n",
    "    else:\n",
    "        results['face'] = {\n",
    "            'bbox': np.zeros(4, dtype=float), \n",
    "            'center': np.zeros(2, dtype=float)\n",
    "        }\n",
    "\n",
    "    # Calculate relative distance between body parts\n",
    "    results['relative'] = {}\n",
    "    results['relative']['faceHand0'] = results['face']['center'] - results['hands'][0]['center']\n",
    "    results['relative']['faceHand1'] = results['face']['center'] - results['hands'][1]['center']\n",
    "    results['relative']['facePose'] = results['face']['center'] - results['pose']['center']\n",
    "\n",
    "\n",
    "    \n",
    "    # return ([\n",
    "    #     flattenDetectionResult(results['hands'][0]), \n",
    "    #     flattenDetectionResult(results['hands'][1]), \n",
    "    #     flattenDetectionResult(results['pose']), \n",
    "    #     results['face']['bbox'], \n",
    "    #     results['face']['center'],\n",
    "    #     results['relative']['faceHand0'],\n",
    "    #     results['relative']['faceHand1'],\n",
    "    #     results['relative']['facePose']\n",
    "    # ]), frame, tempResults\n",
    "\n",
    "    # Convert results into 1D-array\n",
    "    detectionResults = flatten2dList([\n",
    "        flattenDetectionResult(results['hands'][0]), \n",
    "        flattenDetectionResult(results['hands'][1]), \n",
    "        flattenDetectionResult(results['pose']), \n",
    "        results['face']['bbox'], \n",
    "        results['face']['center'],\n",
    "        results['relative']['faceHand0'],\n",
    "        results['relative']['faceHand1'],\n",
    "        results['relative']['facePose']\n",
    "    ], dataType=float)\n",
    "\n",
    "    return detectionResults, frame, tempResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2  UI Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read one frame from camera\n",
    "def readFrame():\n",
    "    success, frame = cam.read()\n",
    "    if not success: \n",
    "        raise Exception(\"No Frames Read\")\n",
    "    return cv2.flip(frame, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pause recording upon \"Space\"\n",
    "def pauseWhenSpace(trainingNum, actionStr):\n",
    "    while True:\n",
    "        frame = readFrame()\n",
    "        cv2.putText(frame, f'Training #{trainingNum + 1} for \\'{actionStr}\\'', (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "        cv2.putText(frame, f'Pausing...', (40, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (20, 255, 125), 3)\n",
    "        cv2.imshow(\"Sign Language Recognition Prototype\", frame)\n",
    "\n",
    "        # If pressed resume, do countdown\n",
    "        keyPressed = cv2.waitKey(100)   # Read key every 100ms, i.e. lock fps to 10fps\n",
    "        if keyPressed == 32:    # 32 == Space\n",
    "            resume = False\n",
    "\n",
    "            \n",
    "            for i in range(3):\n",
    "                for _ in range(10):\n",
    "                    temp_frame = readFrame()\n",
    "                    cv2.putText(temp_frame, f'Training #{trainingNum + 1} for \\'{actionStr}\\'', (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "                    cv2.putText(temp_frame, f'Resuming in {3 - i}', (20, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 120), 3)\n",
    "                    cv2.imshow(\"Sign Language Recognition Prototype\", temp_frame)\n",
    "                    tempKey = cv2.waitKey(100)\n",
    "                    if (tempKey == 27):\n",
    "                        raise Exception(\"Finished\")\n",
    "                    # If pressed paused again, stop resuming and continue pausing\n",
    "                    elif tempKey == 32:\n",
    "                        resume = True\n",
    "                        break\n",
    "                if resume:\n",
    "                    break\n",
    "            if not resume:\n",
    "                return\n",
    "            \n",
    "        elif keyPressed == 27:\n",
    "            raise Exception(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display countdown (1, 2, 3)\n",
    "pausing = False\n",
    "def countdownFromThree(trainingNum, actionStr):\n",
    "    # Count down 3 seconds on every new training\n",
    "    for i in range(3):\n",
    "\n",
    "        # Using iteration of 10 frames (100ms each) so that the display is still going on 10fps\n",
    "        for _ in range(10):\n",
    "            frame = readFrame()\n",
    "            \n",
    "            cv2.putText(frame, f'Training #{trainingNum + 1} for \\'{actionStr}\\'', (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "            cv2.putText(frame, f'Next Training in {3 - i}', (20, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 3)\n",
    "            cv2.imshow(\"Sign Language Recognition Prototype\", frame)\n",
    "            \n",
    "            tempKey = cv2.waitKey(100)\n",
    "            if (tempKey == 27):     # Pressed Esc\n",
    "                raise Exception(\"Finished\")\n",
    "            elif tempKey == 32:     # Pressed Space\n",
    "                pauseWhenSpace(trainingNum, actionStr)\n",
    "                return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Recording Label (Create Training Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'help'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify which action to record\n",
    "action = action_labels[2]\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lmList': [[882, 515, 0], [872, 489, 0], [849, 462, -3], [837, 441, -7], [835, 422, -10], [834, 455, -7], [801, 472, -14], [812, 479, -17], [823, 476, -18], [833, 470, -10], [802, 489, -15], [814, 494, -14], [825, 490, -13], [833, 488, -14], [809, 504, -18], [820, 509, -13], [830, 506, -9], [833, 508, -18], [814, 518, -21], [826, 521, -16], [835, 518, -12]], 'bbox': (801, 422, 81, 99), 'center': (841, 471), 'type': 'Left'}\n",
      "{'lmList': [[522, 524, 0], [532, 497, 0], [545, 471, -3], [550, 448, -8], [549, 430, -13], [579, 476, -5], [598, 481, -10], [592, 491, -14], [581, 495, -17], [584, 493, -9], [600, 501, -9], [592, 508, -9], [580, 508, -11], [584, 512, -13], [595, 520, -11], [587, 524, -8], [576, 523, -8], [581, 530, -18], [587, 537, -16], [579, 538, -11], [569, 536, -9]], 'bbox': (522, 430, 78, 108), 'center': (561, 484), 'type': 'Right'}\n",
      "{'lmList': [[0, 705, 555, -444], [1, 715, 530, -397], [2, 724, 532, -396], [3, 733, 534, -396], [4, 685, 529, -399], [5, 673, 531, -398], [6, 661, 533, -398], [7, 748, 554, -153], [8, 642, 557, -156], [9, 723, 587, -360], [10, 684, 586, -362], [11, 816, 682, -39], [12, 572, 696, -94], [13, 976, 770, -324], [14, 451, 793, -473], [15, 900, 562, -540], [16, 504, 597, -855], [17, 876, 513, -597], [18, 522, 540, -942], [19, 857, 503, -511], [20, 533, 529, -862], [21, 858, 524, -520], [22, 538, 556, -846]], 'bbox': (450, 408, 488, 1271), 'center': (694, 1043)}\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_16744\\2204461021.py\", line 33, in <module>\n",
      "    raise Exception(\"Done\")\n",
      "Exception: Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cam = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "cam.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cam.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "# Detectors\n",
    "handDetector = HandDetector(detectionCon=0.5, maxHands=2)\n",
    "faceDetector = FaceDetector(minDetectionCon=0.5)\n",
    "poseDetector = PoseDetector(detectionCon=0.5)\n",
    "\n",
    "try:\n",
    "    startTime = time()\n",
    "\n",
    "    for training in range(TRAININGS_PER_LABEL): \n",
    "        trainingResults = []\n",
    "        for frame_num in range(FRAMES_PER_TRAINING):\n",
    "\n",
    "            # Countdown\n",
    "            # if frame_num == 0:\n",
    "            #     countdownFromThree(training, action)\n",
    "            #     startTime = time()\n",
    "        \n",
    "            # Read from camera\n",
    "            frame = readFrame()\n",
    "\n",
    "            detectionResults, frame, ogResults = featureExtraction(\n",
    "                handDetector, faceDetector, poseDetector, frame)\n",
    "            \n",
    "            # Show resulting frame\n",
    "            cv2.putText(frame, f'Training #{training + 1} for \\'{action}\\'', (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "            cv2.imshow(\"Sign Language Recognition Prototype\", frame)     \n",
    "\n",
    "            # print(detectionResults)\n",
    "            raise Exception(\"Done\")\n",
    "            # Save the results\n",
    "            trainingResults.append(detectionResults)\n",
    "\n",
    "            keyPressed = cv2.waitKey(10)\n",
    "            # Stop Program when pressed 'Esc'\n",
    "            if (keyPressed == 27):\n",
    "                raise Exception(\"Finished\")\n",
    "\n",
    "        # After all frames are finished for each training:\n",
    "        # save as .npy\n",
    "        \n",
    "        # IMPORTANT: THIS LINE IS DISABLED IN CASE OF ACCIDENTALLY OVERWRITING DATA\n",
    "        # Enable it ONLY during data collection\n",
    "        # np.save(os.path.join(KEYPOINTS_PATH, action, str(training)), np.array(trainingResults))\n",
    "\n",
    "except Exception as e:\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(e)\n",
    "    traceback.print_exc()\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(69,), (69,), (75,), (4,), (2,), (2,), (2,), (2,)]\n"
     ]
    }
   ],
   "source": [
    "print(list(i.shape for i in detectionResults))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  3.33333333e-02,\n",
       "        1.90476190e-02, -1.85714286e-01,  1.90476190e-01,  1.42857143e-02,\n",
       "       -2.47619048e-01,  3.42857143e-01, -9.04761905e-02, -2.76190476e-01,\n",
       "        4.52380952e-01, -1.95238095e-01, -2.95238095e-01,  4.04761905e-01,\n",
       "        2.00000000e-01, -1.33333333e-01,  5.85714286e-01,  3.52380952e-01,\n",
       "       -2.14285714e-01,  7.00000000e-01,  4.42857143e-01, -2.71428571e-01,\n",
       "        7.90476190e-01,  5.19047619e-01, -3.00000000e-01,  4.47619048e-01,\n",
       "        1.00000000e-01, -7.61904762e-02,  6.95238095e-01,  1.76190476e-01,\n",
       "       -1.57142857e-01,  8.61904762e-01,  2.00000000e-01, -2.23809524e-01,\n",
       "        1.00000000e+00,  2.38095238e-01, -2.52380952e-01,  4.23809524e-01,\n",
       "       -4.76190476e-03, -3.33333333e-02,  5.57142857e-01, -4.28571429e-02,\n",
       "       -1.66666667e-01,  4.57142857e-01, -7.14285714e-02, -2.09523810e-01,\n",
       "        3.71428571e-01, -7.61904762e-02, -1.95238095e-01,  3.71428571e-01,\n",
       "       -1.04761905e-01, -4.76190476e-03,  4.95238095e-01, -1.76190476e-01,\n",
       "       -1.19047619e-01,  4.42857143e-01, -1.95238095e-01, -1.33333333e-01,\n",
       "        3.71428571e-01, -1.90476190e-01, -1.09523810e-01,  2.77343750e-01,\n",
       "        3.25000000e-01,  4.41406250e-01,  5.33333333e-01,  3.59375000e-01,\n",
       "        4.29166667e-01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -2.01342282e-01, -1.00671141e-01, -2.01342282e-02, -4.83221477e-01,\n",
       "       -2.08053691e-01, -6.04026846e-02, -6.71140940e-01, -3.08724832e-01,\n",
       "       -1.07382550e-01, -7.65100671e-01, -4.56375839e-01, -1.61073826e-01,\n",
       "       -5.16778523e-01, -6.04026846e-02, -1.54362416e-01, -7.44966443e-01,\n",
       "       -1.20805369e-01, -2.34899329e-01, -8.92617450e-01, -1.81208054e-01,\n",
       "       -2.81879195e-01, -1.00000000e+00, -2.48322148e-01, -3.08724832e-01,\n",
       "       -4.49664430e-01,  2.68456376e-02, -1.81208054e-01, -7.04697987e-01,\n",
       "        2.48322148e-01, -2.48322148e-01, -8.38926174e-01,  3.69127517e-01,\n",
       "       -2.68456376e-01, -9.39597315e-01,  4.63087248e-01, -2.81879195e-01,\n",
       "       -3.35570470e-01,  1.27516779e-01, -2.01342282e-01, -5.43624161e-01,\n",
       "        3.35570470e-01, -2.61744966e-01, -4.63087248e-01,  2.68456376e-01,\n",
       "       -2.28187919e-01, -3.75838926e-01,  1.94630872e-01, -1.94630872e-01,\n",
       "       -2.28187919e-01,  2.14765101e-01, -2.28187919e-01, -3.89261745e-01,\n",
       "        3.75838926e-01, -2.75167785e-01, -3.35570470e-01,  3.28859060e-01,\n",
       "       -2.48322148e-01, -2.75167785e-01,  2.55033557e-01, -2.14765101e-01,\n",
       "        6.56250000e-01,  5.61111111e-01,  7.72656250e-01,  7.51388889e-01,\n",
       "        7.14062500e-01,  6.55555556e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  2.38095238e-03,  2.38095238e-02,\n",
       "       -4.04761905e-02,  9.76190476e-02,  4.76190476e-03,  4.76190476e-02,\n",
       "       -3.80952381e-02,  9.76190476e-02,  7.14285714e-03,  7.14285714e-02,\n",
       "       -3.57142857e-02,  1.00000000e-01,  9.52380952e-03, -4.76190476e-02,\n",
       "       -3.09523810e-02,  1.42857143e-01,  1.19047619e-02, -7.38095238e-02,\n",
       "       -2.14285714e-02,  1.45238095e-01,  1.42857143e-02, -1.00000000e-01,\n",
       "       -7.14285714e-03,  1.42857143e-01,  1.66666667e-02,  1.07142857e-01,\n",
       "        2.14285714e-02,  5.00000000e-01,  1.90476190e-02, -1.42857143e-01,\n",
       "        7.38095238e-02,  7.57142857e-01,  2.14285714e-02,  5.47619048e-02,\n",
       "        7.61904762e-02,  1.28571429e-01,  2.38095238e-02, -3.80952381e-02,\n",
       "        7.85714286e-02,  1.95238095e-01,  2.61904762e-02,  3.54761905e-01,\n",
       "        4.21428571e-01,  5.21428571e-01,  2.85714286e-02, -4.00000000e-01,\n",
       "        4.09523810e-01,  1.00000000e+00,  3.09523810e-02,  8.33333333e-01,\n",
       "        3.30952381e-01, -6.42857143e-02,  3.33333333e-02, -9.45238095e-01,\n",
       "        1.85714286e-01,  6.45238095e-01,  3.57142857e-02,  5.80952381e-01,\n",
       "        6.66666667e-02, -1.19047619e-01,  3.80952381e-02, -8.59523810e-01,\n",
       "       -3.23809524e-01,  3.45238095e-01,  4.04761905e-02,  4.95238095e-01,\n",
       "        2.38095238e-03, -2.09523810e-01,  4.28571429e-02, -8.28571429e-01,\n",
       "       -4.57142857e-01,  2.02380952e-01,  4.52380952e-02,  4.52380952e-01,\n",
       "       -7.14285714e-03,  1.42857143e-02,  4.76190476e-02, -7.61904762e-01,\n",
       "       -4.42857143e-01,  3.80952381e-01,  5.00000000e-02,  4.69047619e-01,\n",
       "        2.38095238e-02, -5.95238095e-02,  5.23809524e-02, -7.61904762e-01,\n",
       "       -3.92857143e-01,  3.47619048e-01,  3.12500000e-01,  3.84722222e-01,\n",
       "        8.07031250e-01,  2.60833333e+00,  5.59375000e-01,  1.49583333e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -3.59375000e-01, -4.29166667e-01,\n",
       "       -7.14062500e-01, -6.55555556e-01, -5.59375000e-01, -1.49583333e+00])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detectionResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lmList': [[896, 328, 0],\n",
       "  [878, 304, -12],\n",
       "  [869, 270, -22],\n",
       "  [862, 245, -32],\n",
       "  [853, 224, -42],\n",
       "  [908, 260, -30],\n",
       "  [924, 223, -45],\n",
       "  [932, 199, -55],\n",
       "  [942, 178, -61],\n",
       "  [927, 272, -33],\n",
       "  [950, 255, -52],\n",
       "  [970, 244, -61],\n",
       "  [986, 234, -66],\n",
       "  [940, 289, -37],\n",
       "  [932, 296, -54],\n",
       "  [909, 310, -51],\n",
       "  [891, 319, -46],\n",
       "  [947, 307, -41],\n",
       "  [939, 313, -54],\n",
       "  [920, 323, -50],\n",
       "  [905, 330, -45]],\n",
       " 'bbox': (853, 178, 133, 152),\n",
       " 'center': (919, 254),\n",
       " 'type': 'Left'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ogResults['hands'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 219)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.load(os.path.join(KEYPOINTS_PATH, action, '0.npy')).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Real Time Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from time import time\n",
    "from cvzone import FPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name 'featureExtraction' is not defined\n"
     ]
    }
   ],
   "source": [
    "cam = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "cam.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cam.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "# Detectors\n",
    "handDetector = HandDetector(detectionCon=0.5, maxHands=2)\n",
    "faceDetector = FaceDetector(minDetectionCon=0.5)\n",
    "poseDetector = PoseDetector(detectionCon=0.5)\n",
    "\n",
    "fpsReader = FPS()\n",
    "\n",
    "timeStats = []\n",
    "\n",
    "try:\n",
    "\n",
    "    keypointsHistory = deque()\n",
    "    predictionHistory = deque()\n",
    "    detectionThreshold = 1.0\n",
    "\n",
    "    lastPredictionTime = time()\n",
    "    predictionCooldown = 1\n",
    "\n",
    "    while True:\n",
    "        startTime = time()\n",
    "\n",
    "        # Read from camera\n",
    "        success, frame = cam.read()\n",
    "        if not success: \n",
    "            raise Exception(\"No Frames Read\")\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # Pose Detection\n",
    "        detectionResults, frame = featureExtraction(\n",
    "            handDetector, faceDetector, poseDetector, frame)\n",
    "        \n",
    "        # Semantic Prediction\n",
    "        keypointsHistory.append(detectionResults)\n",
    "        if len(keypointsHistory) > FRAMES_PER_TRAINING:\n",
    "            keypointsHistory.popleft()\n",
    "\n",
    "            \n",
    "            # if time() > lastPredictionTime + predictionCooldown:\n",
    "            #     predictionResults = model.predict(\n",
    "            #         np.expand_dims(keypointsHistory, axis=0), \n",
    "            #         verbose=0, \n",
    "            #         use_multiprocessing=True, \n",
    "            #         workers=4\n",
    "            #         )[0]\n",
    "            #     predWord = action_labels[np.argmax(predictionResults)]\n",
    "            #     predAccuracy = predictionResults[np.argmax(predictionResults)]\n",
    "\n",
    "            #     if predAccuracy >= detectionThreshold:\n",
    "            #         lastPredictionTime = time()\n",
    "                    \n",
    "            #         predictionHistory.append(predWord)\n",
    "            #         if len(predictionHistory) > 5:\n",
    "            #             predictionHistory.popleft()\n",
    "        \n",
    "        cv2.putText(frame, ', '.join(predictionHistory), (15, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 3)\n",
    "        \n",
    "        fps, frame = fpsReader.update(frame,pos=(950,80),color=(0,255,0),scale=5,thickness=5)\n",
    "\n",
    "\n",
    "        # Show resulting frame\n",
    "        # cv2.putText(frame, f'Training #{training + 1} for \\'{action}\\'', (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "        cv2.imshow(\"Sign Language Recognition Prototype\", frame)     \n",
    "\n",
    "        timeStats.append(time() - startTime)\n",
    "\n",
    "        keyPressed = cv2.waitKey(15)\n",
    "        # Stop Program when pressed 'Esc'\n",
    "        if (keyPressed == 27):\n",
    "            raise Exception(\"Finished\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(e)\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.07875680923461914,\n",
       " 0.07371330261230469,\n",
       " 0.0756688117980957,\n",
       " 0.07382822036743164,\n",
       " 0.07182097434997559,\n",
       " 0.14654779434204102,\n",
       " 0.13663458824157715,\n",
       " 0.13762927055358887,\n",
       " 0.13364148139953613,\n",
       " 0.173537015914917,\n",
       " 0.16253042221069336,\n",
       " 0.16656756401062012,\n",
       " 0.1545863151550293,\n",
       " 0.1565384864807129,\n",
       " 0.16048026084899902,\n",
       " 0.1515054702758789,\n",
       " 0.1635282039642334,\n",
       " 0.17353558540344238,\n",
       " 0.15356874465942383,\n",
       " 0.15557503700256348,\n",
       " 0.15255475044250488,\n",
       " 0.1715106964111328,\n",
       " 0.15854263305664062,\n",
       " 0.15158939361572266,\n",
       " 0.1595776081085205,\n",
       " 0.16945433616638184]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "timeStats[10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "# Detects hands, face & pose, \n",
    "# convert them into normalized landmark/keypoint coordinates in a 1D-array, \n",
    "# and also returns the frame with the landmark connections drawn onto it\n",
    "def featureExtractionV2(handDetector, faceDetector, poseDetector, frame):\n",
    "    results = {}\n",
    "    frameSize = (frame.shape[1], frame.shape[0])\n",
    "\n",
    "    def detectHands(frame, handDetector, frameSize):\n",
    "        results = {}\n",
    "        # Hand Detection\n",
    "        results['hands'], frame = handDetector.findHands(frame, draw=True)\n",
    "        if not results['hands']:\n",
    "            results['hands'] = [generate_empty_hand('Left'), generate_empty_hand('Right')]\n",
    "        elif len(results['hands']) == 1:\n",
    "            results['hands'][0] = preprocess_body_part(results['hands'][0], frameSize)\n",
    "            \n",
    "            if (results['hands'][0]['type'] == 'Left'):\n",
    "                results['hands'].append(generate_empty_hand('Right'))\n",
    "            else:\n",
    "                results['hands'].insert(0, generate_empty_hand('Left'))                         \n",
    "        else:\n",
    "            results['hands'][0] = preprocess_body_part(results['hands'][0], frameSize)\n",
    "            results['hands'][1] = preprocess_body_part(results['hands'][1], frameSize)\n",
    "        return results['hands']\n",
    "\n",
    "    def detectPose(frame, poseDetector, frameSize):\n",
    "        results = {}\n",
    "        # Pose Detection\n",
    "        # * We only use the first 23 out of the total 33 landmark points \n",
    "        #   as those represent the lower half body and are irrelevant\n",
    "        frame = poseDetector.findPose(frame, draw=True)\n",
    "        results['pose'] = {}\n",
    "        results['pose']['lmList'], tempPoseBbox = poseDetector.findPosition(frame, bboxWithHands=False)\n",
    "        if results['pose']['lmList'] and tempPoseBbox:\n",
    "            results['pose']['lmList'] = results['pose']['lmList'][:23]\n",
    "            results['pose']['bbox'] = tempPoseBbox['bbox']\n",
    "            results['pose']['center'] = tempPoseBbox['center']\n",
    "            results['pose'] = preprocess_body_part(results['pose'], frameSize)\n",
    "        else:\n",
    "            results['pose']['lmList'] = np.zeros(23 * 3, dtype=int)\n",
    "            results['pose']['bbox'] = np.zeros(4, dtype=float)\n",
    "            results['pose']['center'] = np.zeros(2, dtype=float)\n",
    "        return results['pose']\n",
    "            \n",
    "\n",
    "    def detectFace(frame, faceDetector, frameSize):\n",
    "        results = {}\n",
    "        # Face Detection\n",
    "        frame, results['face'] = faceDetector.findFaces(frame, draw=True)\n",
    "        if results['face']:\n",
    "            results['face'] = select_best_matching_face(results['face'], frameSize)\n",
    "            results['face']['bbox'] = preprocess_bbox(results['face']['bbox'], frameSize)\n",
    "            results['face']['center'] = preprocess_center(results['face']['center'], frameSize)\n",
    "        else:\n",
    "            results['face'] = {\n",
    "                'bbox': np.zeros(4, dtype=float), \n",
    "                'center': np.zeros(2, dtype=float)\n",
    "            }\n",
    "        return results['face']\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        # t1 = executor.submit(detectHands, frame, handDetector, frameSize)\n",
    "        # t2 = executor.submit(detectPose, frame, poseDetector, frameSize)\n",
    "        t3 = executor.submit(detectFace, frame, faceDetector, frameSize)\n",
    "\n",
    "\n",
    "        # Convert results into 1D-array\n",
    "        detectionResults = np.concatenate([\n",
    "            flattenDetectionResult(t1.result()[0]), \n",
    "            flattenDetectionResult(t1.result()[1]), \n",
    "            flattenDetectionResult(t2.result()), \n",
    "            t3.result()['bbox'], \n",
    "            t3.result()['center']\n",
    "        ])\n",
    "\n",
    "        return detectionResults, frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "cam = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "cam.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cam.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "fpsReader = FPS()\n",
    "\n",
    "timeStats = []\n",
    "\n",
    "try:\n",
    "    initialTime = time()\n",
    "    while True:\n",
    "        startTime = time()\n",
    "\n",
    "        # Read from camera\n",
    "        success, frame = cam.read()\n",
    "        \n",
    "        # Pose Detection\n",
    "        detectionResults, frame = featureExtractionV2(\n",
    "            handDetector, faceDetector, poseDetector, frame)\n",
    "        \n",
    "        # fps, frame = fpsReader.update(frame,pos=(50,80),color=(0,255,0),scale=5,thickness=5)\n",
    "\n",
    "\n",
    "        # Show resulting frame\n",
    "        # cv2.putText(frame, f'Training #{training + 1} for \\'{action}\\'', (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "        cv2.imshow(\"Sign Language Recognition Prototype\", frame)     \n",
    "\n",
    "        timeStats.append(time() - startTime)\n",
    "\n",
    "        keyPressed = cv2.waitKey(15)\n",
    "        # Stop Program when pressed 'Esc'\n",
    "        if (keyPressed == 27):\n",
    "            raise Exception(\"Finished\")\n",
    "        \n",
    "        # if time() - initialTime > 10:\n",
    "        #     raise Exception()\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(e)\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05556477281384002"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.array(timeStats[1:]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016924326368373075"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.array(timeStats[1:]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09001387300945464"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.array(timeStats[1:]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06750456676926724"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.array(timeStats[1:]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "cam.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cam.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "success, frame = cam.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "type(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
