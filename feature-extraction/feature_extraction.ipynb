{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries / Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvzone.HandTrackingModule import HandDetector\n",
    "from cvzone.FaceDetectionModule import FaceDetector\n",
    "from cvzone.PoseModule import PoseDetector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Extraction (Hand+Face+Pose Detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten a 2d np array into 1d array\n",
    "def flatten2dList(arr, dataType=int):\n",
    "    return np.fromiter(chain.from_iterable(arr), dataType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the largest absolute value in an np array\n",
    "def getAbsLargestVal(arr):\n",
    "    return np.max(np.abs(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offset and normalize the landmark list\n",
    "# Returns a 1d numpy array\n",
    "def preprocess_landmarks(landmark_list):\n",
    "    if not landmark_list:\n",
    "        return []\n",
    "    \n",
    "    landmark_list = np.array(landmark_list, dtype=float)\n",
    "    origin = landmark_list[0]\n",
    "    \n",
    "    # Offset every point with respect to the first point\n",
    "    # Convert to 1D-array\n",
    "    new_landmark_list = (landmark_list - origin).ravel()\n",
    "    \n",
    "    # Get highest absolute value\n",
    "    largest_value = getAbsLargestVal(new_landmark_list)\n",
    "    \n",
    "    # Normalization\n",
    "    return new_landmark_list / largest_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offset and normalize a BBOX list (BBOX = Bounding Box, used in face and hand detection)\n",
    "# Returns a 1d numpy array\n",
    "def preprocess_bbox(bbox, frameSize):\n",
    "    bbox = np.array(bbox, dtype=float)\n",
    "    # Convert 3rd and 4th element into coordinates instead of width/height\n",
    "    bbox[2] = bbox[0] + bbox[2]\n",
    "    bbox[3] = bbox[1] + bbox[3]\n",
    "\n",
    "    # Normalize against frame size\n",
    "    bbox[0] /= frameSize[0]\n",
    "    bbox[1] /= frameSize[1]\n",
    "    bbox[2] /= frameSize[0]\n",
    "    bbox[3] /= frameSize[1]\n",
    "\n",
    "    return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize a center vertex (a list of 2 elements)\n",
    "# Returns a 1d numpy array\n",
    "def preprocess_center(center, frameSize):\n",
    "    center = np.array(center, dtype=float)\n",
    "    center[0] /= frameSize[0]\n",
    "    center[1] /= frameSize[1]\n",
    "    return center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess (Offset and normalize) the body's landmark list, bbox and center\n",
    "def preprocess_body_part(bodyPart, frameSize):\n",
    "    bodyPart['lmList'] = preprocess_landmarks(bodyPart['lmList'])\n",
    "    bodyPart['bbox'] = preprocess_bbox(bodyPart['bbox'], frameSize)\n",
    "    bodyPart['center'] = preprocess_center(bodyPart['center'], frameSize)\n",
    "    return bodyPart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate empty/placeholder data for a hand \n",
    "# Used when a hand is not detected in frame\n",
    "def generate_empty_hand(type):\n",
    "    return {\n",
    "        'lmList': np.zeros(21 * 3, dtype=int), \n",
    "        'bbox': np.zeros(4, dtype=float), \n",
    "        'center': np.zeros(2, dtype=float), \n",
    "        'type': type\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best matching face, aka the one with the best score (clarity)\n",
    "# and closest to the center of the screen\n",
    "# Since the Neural network will be design to only accept one face\n",
    "def select_best_matching_face(faces, frameSize):\n",
    "    if not faces:\n",
    "        return False\n",
    "    elif len(faces) == 1:\n",
    "        return faces[0]\n",
    "    \n",
    "    def difference(a, b):\n",
    "        return (a[0] - b[0])**2 + (a[1] - b[1])**2\n",
    "    \n",
    "    frameCenter = (frameSize[0] / 2, frameSize[1] / 2)\n",
    "\n",
    "    best_score = faces[0]\n",
    "    best_center = faces[0]\n",
    "    center_diff = difference(faces[0]['center'], frameCenter)\n",
    "\n",
    "    for each in faces:\n",
    "        if difference(each['center'], frameCenter) < center_diff:\n",
    "            best_center = each\n",
    "        if each['score'][0] > best_score['score'][0]:\n",
    "            best_score = each\n",
    "    \n",
    "    if best_center['score'][0] > 0.5:\n",
    "        return best_center\n",
    "    return best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten everything\n",
    "def flattenDetectionResult(obj):\n",
    "    return np.fromiter(chain.from_iterable([obj['lmList'], obj['bbox'], obj['center']]), float)\n",
    "    # return np.concatenate([obj['lmList'], obj['bbox'], obj['center']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparation for Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import TRAININGS_PER_LABEL, FRAMES_PER_TRAINING, KEYPOINTS_PER_FRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'thank you', 'help']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from files_io import readActionLabels, initActionLabelFolders\n",
    "\n",
    "action_labels = readActionLabels()\n",
    "initActionLabelFolders(action_labels)\n",
    "action_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Feature Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detects hands, face & pose, \n",
    "# convert them into normalized landmark/keypoint coordinates in a 1D-array, \n",
    "# and also returns the frame with the landmark connections drawn onto it\n",
    "\n",
    "# Serial/Unparallelised version (Old version)\n",
    "def featureExtraction(handDetector, faceDetector, poseDetector, frame):\n",
    "    results = {}\n",
    "    frameSize = (frame.shape[1], frame.shape[0])\n",
    "\n",
    "    tempResults = {}\n",
    "\n",
    "    # Hand Detection\n",
    "    results['hands'], frame = handDetector.findHands(frame, draw=True)\n",
    "    tempResults['hands'] = results['hands'].copy()\n",
    "    if not results['hands']:\n",
    "        results['hands'] = [generate_empty_hand('Left'), generate_empty_hand('Right')]\n",
    "    elif len(results['hands']) == 1:\n",
    "        if (results['hands'][0]['type'] == 'Left'):\n",
    "            results['hands'][0] = preprocess_body_part(results['hands'][0], frameSize)\n",
    "            results['hands'].append(generate_empty_hand('Right'))\n",
    "        else:\n",
    "            results['hands'][0] = preprocess_body_part(results['hands'][0], frameSize)\n",
    "            results['hands'].insert(0, generate_empty_hand('Left'))                         \n",
    "    else:\n",
    "        results['hands'][0] = preprocess_body_part(results['hands'][0], frameSize)\n",
    "        results['hands'][1] = preprocess_body_part(results['hands'][1], frameSize)\n",
    "\n",
    "    # Pose Detection\n",
    "    # **We only use the first 23 out of the total 33 landmark points \n",
    "    #   as those represent the lower half body and are irrelevant to sign language interpretation\n",
    "    frame = poseDetector.findPose(frame, draw=True)\n",
    "    results['pose'] = {}\n",
    "    results['pose']['lmList'], tempPoseBbox = poseDetector.findPosition(frame, bboxWithHands=False)\n",
    "    if results['pose']['lmList'] and tempPoseBbox:\n",
    "        results['pose']['lmList'] = results['pose']['lmList'][:23]\n",
    "        results['pose']['bbox'] = tempPoseBbox['bbox']\n",
    "        results['pose']['center'] = tempPoseBbox['center']\n",
    "        \n",
    "        tempResults['pose'] = results['pose'].copy()\n",
    "\n",
    "        results['pose'] = preprocess_body_part(results['pose'], frameSize)\n",
    "    else:\n",
    "        results['pose']['lmList'] = np.zeros(23 * 3, dtype=int)\n",
    "        results['pose']['bbox'] = np.zeros(4, dtype=float)\n",
    "        results['pose']['center'] = np.zeros(2, dtype=float)\n",
    "        \n",
    "\n",
    "    \n",
    "    # Face Detection\n",
    "    frame, results['face'] = faceDetector.findFaces(frame, draw=True)\n",
    "    if results['face']:\n",
    "        results['face'] = select_best_matching_face(results['face'], frameSize)\n",
    "\n",
    "        \n",
    "        tempResults['face'] = results['face'].copy()\n",
    "\n",
    "        results['face']['bbox'] = preprocess_bbox(results['face']['bbox'], frameSize)\n",
    "        results['face']['center'] = preprocess_center(results['face']['center'], frameSize)\n",
    "    else:\n",
    "        results['face'] = {\n",
    "            'bbox': np.zeros(4, dtype=float), \n",
    "            'center': np.zeros(2, dtype=float)\n",
    "        }\n",
    "\n",
    "    # Calculate relative distance between body parts\n",
    "    results['relative'] = {}\n",
    "    results['relative']['faceHand0'] = results['face']['center'] - results['hands'][0]['center']\n",
    "    results['relative']['faceHand1'] = results['face']['center'] - results['hands'][1]['center']\n",
    "    results['relative']['facePose'] = results['face']['center'] - results['pose']['center']\n",
    "\n",
    "    # Convert results into 1D-array\n",
    "    detectionResults = flatten2dList([\n",
    "        flattenDetectionResult(results['hands'][0]), \n",
    "        flattenDetectionResult(results['hands'][1]), \n",
    "        flattenDetectionResult(results['pose']), \n",
    "        results['face']['bbox'], \n",
    "        results['face']['center'],\n",
    "        results['relative']['faceHand0'],\n",
    "        results['relative']['faceHand1'],\n",
    "        results['relative']['facePose']\n",
    "    ], dataType=float)\n",
    "\n",
    "    return detectionResults, frame, tempResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2  UI Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read one frame from camera\n",
    "def readFrame():\n",
    "    success, frame = cam.read()\n",
    "    if not success: \n",
    "        raise Exception(\"No Frames Read\")\n",
    "    return cv2.flip(frame, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pause recording upon \"Space\"\n",
    "def pauseWhenSpace(trainingNum, actionStr):\n",
    "    while True:\n",
    "        frame = readFrame()\n",
    "        cv2.putText(frame, f'Training #{trainingNum + 1} for \\'{actionStr}\\'', (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "        cv2.putText(frame, f'Pausing...', (40, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (20, 255, 125), 3)\n",
    "        cv2.imshow(\"Sign Language Recognition Prototype\", frame)\n",
    "\n",
    "        # If pressed resume, do countdown\n",
    "        keyPressed = cv2.waitKey(100)   # Read key every 100ms, i.e. lock fps to 10fps\n",
    "        if keyPressed == 32:    # 32 == Space\n",
    "            resume = False\n",
    "            \n",
    "            for i in range(3):\n",
    "                for _ in range(10):\n",
    "                    temp_frame = readFrame()\n",
    "                    cv2.putText(temp_frame, f'Training #{trainingNum + 1} for \\'{actionStr}\\'', (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "                    cv2.putText(temp_frame, f'Resuming in {3 - i}', (20, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 120), 3)\n",
    "                    cv2.imshow(\"Sign Language Recognition Prototype\", temp_frame)\n",
    "                    tempKey = cv2.waitKey(100)\n",
    "                    if (tempKey == 27):\n",
    "                        raise Exception(\"Finished\")\n",
    "                    # If pressed paused again, stop resuming and continue pausing\n",
    "                    elif tempKey == 32:\n",
    "                        resume = True\n",
    "                        break\n",
    "                if resume:\n",
    "                    break\n",
    "            if not resume:\n",
    "                return\n",
    "            \n",
    "        elif keyPressed == 27:\n",
    "            raise Exception(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display countdown (1, 2, 3)\n",
    "pausing = False\n",
    "def countdownFromThree(trainingNum, actionStr):\n",
    "    # Count down 3 seconds on every new training\n",
    "    for i in range(3):\n",
    "\n",
    "        # Using iteration of 10 frames (100ms each) so that the display is still going on 10fps\n",
    "        for _ in range(10):\n",
    "            frame = readFrame()\n",
    "            \n",
    "            cv2.putText(frame, f'Training #{trainingNum + 1} for \\'{actionStr}\\'', (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "            cv2.putText(frame, f'Next Training in {3 - i}', (20, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 3)\n",
    "            cv2.imshow(\"Sign Language Recognition Prototype\", frame)\n",
    "            \n",
    "            tempKey = cv2.waitKey(100)\n",
    "            if (tempKey == 27):     # Pressed Esc\n",
    "                raise Exception(\"Finished\")\n",
    "            elif tempKey == 32:     # Pressed Space\n",
    "                pauseWhenSpace(trainingNum, actionStr)\n",
    "                return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Recording Label (Create Training Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'help'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify which action to record\n",
    "action = action_labels[2]\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2896\\4034106895.py\", line 20, in <module>\n",
      "    countdownFromThree(training_num, action)\n",
      "  File \"C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2896\\3147981487.py\", line 17, in countdownFromThree\n",
      "    raise Exception(\"Finished\")\n",
      "Exception: Finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cam = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "cam.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cam.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "# Detectors\n",
    "handDetector = HandDetector(detectionCon=0.5, maxHands=2)\n",
    "faceDetector = FaceDetector(minDetectionCon=0.5)\n",
    "poseDetector = PoseDetector(detectionCon=0.5)\n",
    "\n",
    "try:\n",
    "    startTime = time()\n",
    "\n",
    "    trainingResults = np.zeros((TRAININGS_PER_LABEL, FRAMES_PER_TRAINING, KEYPOINTS_PER_FRAME))\n",
    "    \n",
    "    for training_num in range(TRAININGS_PER_LABEL): \n",
    "        for frame_num in range(FRAMES_PER_TRAINING):\n",
    "\n",
    "            # Countdown\n",
    "            if frame_num == 0:\n",
    "                countdownFromThree(training_num, action)\n",
    "                startTime = time()\n",
    "        \n",
    "            # Read from camera\n",
    "            frame = readFrame()\n",
    "\n",
    "            detectionResults, frame, ogResults = featureExtraction(\n",
    "                handDetector, faceDetector, poseDetector, frame)\n",
    "            \n",
    "            # Show resulting frame\n",
    "            cv2.putText(frame, f'Training #{training_num + 1} for \\'{action}\\'', (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "            cv2.imshow(\"Sign Language Recognition Prototype\", frame)     \n",
    "\n",
    "            # Save the results\n",
    "            trainingResults[training_num][frame_num] = detectionResults\n",
    "\n",
    "            keyPressed = cv2.waitKey(1)\n",
    "            # Stop Program when pressed 'Esc'\n",
    "            if (keyPressed == 27):\n",
    "                raise Exception(\"Finished\")\n",
    "\n",
    "    # After all frames are finished for each training:\n",
    "    # save as .npy\n",
    "    \n",
    "    # IMPORTANT: THIS LINE IS DISABLED IN CASE OF ACCIDENTALLY OVERWRITING DATA\n",
    "    # Enable it ONLY during data collection\n",
    "    # np.save(os.path.join(KEYPOINTS_PATH, action), trainingResults)\n",
    "\n",
    "except Exception as e:\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(e)\n",
    "    traceback.print_exc()\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Documents\\TARC\\FINAL YEAR PROJECT\\Code\\sign-language-recognition\\feature-extraction\\feature_extraction.ipynb Cell 29\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/TARC/FINAL%20YEAR%20PROJECT/Code/sign-language-recognition/feature-extraction/feature_extraction.ipynb#X66sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m cam\u001b[39m.\u001b[39mset(cv2\u001b[39m.\u001b[39mCAP_PROP_FRAME_HEIGHT, \u001b[39m720\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/TARC/FINAL%20YEAR%20PROJECT/Code/sign-language-recognition/feature-extraction/feature_extraction.ipynb#X66sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Documents/TARC/FINAL%20YEAR%20PROJECT/Code/sign-language-recognition/feature-extraction/feature_extraction.ipynb#X66sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     frame \u001b[39m=\u001b[39m readFrame()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/TARC/FINAL%20YEAR%20PROJECT/Code/sign-language-recognition/feature-extraction/feature_extraction.ipynb#X66sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     cv2\u001b[39m.\u001b[39mimshow(\u001b[39m\"\u001b[39m\u001b[39mSign Language Recognition Prototype\u001b[39m\u001b[39m\"\u001b[39m, frame) \n",
      "\u001b[1;32md:\\Documents\\TARC\\FINAL YEAR PROJECT\\Code\\sign-language-recognition\\feature-extraction\\feature_extraction.ipynb Cell 29\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/TARC/FINAL%20YEAR%20PROJECT/Code/sign-language-recognition/feature-extraction/feature_extraction.ipynb#X66sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreadFrame\u001b[39m():\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Documents/TARC/FINAL%20YEAR%20PROJECT/Code/sign-language-recognition/feature-extraction/feature_extraction.ipynb#X66sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     success, frame \u001b[39m=\u001b[39m cam\u001b[39m.\u001b[39;49mread()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/TARC/FINAL%20YEAR%20PROJECT/Code/sign-language-recognition/feature-extraction/feature_extraction.ipynb#X66sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m success: \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/TARC/FINAL%20YEAR%20PROJECT/Code/sign-language-recognition/feature-extraction/feature_extraction.ipynb#X66sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo Frames Read\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cam = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "cam.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cam.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "while True:\n",
    "    frame = readFrame()\n",
    "    cv2.imshow(\"Sign Language Recognition Prototype\", frame) \n",
    "    \n",
    "    keyPressed = cv2.waitKey(1)\n",
    "    # Stop Program when pressed 'Esc'\n",
    "    if (keyPressed == 27):\n",
    "        raise Exception(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(69,), (69,), (75,), (4,), (2,), (2,), (2,), (2,)]\n"
     ]
    }
   ],
   "source": [
    "print(list(i.shape for i in detectionResults))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'detectionResults' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Documents\\TARC\\FINAL YEAR PROJECT\\Code\\sign-language-recognition\\feature-extraction\\feature_extraction.ipynb Cell 30\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Documents/TARC/FINAL%20YEAR%20PROJECT/Code/sign-language-recognition/feature-extraction/feature_extraction.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m detectionResults\u001b[39m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'detectionResults' is not defined"
     ]
    }
   ],
   "source": [
    "detectionResults.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lmList': [[896, 328, 0],\n",
       "  [878, 304, -12],\n",
       "  [869, 270, -22],\n",
       "  [862, 245, -32],\n",
       "  [853, 224, -42],\n",
       "  [908, 260, -30],\n",
       "  [924, 223, -45],\n",
       "  [932, 199, -55],\n",
       "  [942, 178, -61],\n",
       "  [927, 272, -33],\n",
       "  [950, 255, -52],\n",
       "  [970, 244, -61],\n",
       "  [986, 234, -66],\n",
       "  [940, 289, -37],\n",
       "  [932, 296, -54],\n",
       "  [909, 310, -51],\n",
       "  [891, 319, -46],\n",
       "  [947, 307, -41],\n",
       "  [939, 313, -54],\n",
       "  [920, 323, -50],\n",
       "  [905, 330, -45]],\n",
       " 'bbox': (853, 178, 133, 152),\n",
       " 'center': (919, 254),\n",
       " 'type': 'Left'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ogResults['hands'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 219)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.load(os.path.join(KEYPOINTS_PATH, action, '0.npy')).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Real Time Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from time import time\n",
    "from cvzone import FPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name 'featureExtraction' is not defined\n"
     ]
    }
   ],
   "source": [
    "cam = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "cam.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cam.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "# Detectors\n",
    "handDetector = HandDetector(detectionCon=0.5, maxHands=2)\n",
    "faceDetector = FaceDetector(minDetectionCon=0.5)\n",
    "poseDetector = PoseDetector(detectionCon=0.5)\n",
    "\n",
    "fpsReader = FPS()\n",
    "\n",
    "timeStats = []\n",
    "\n",
    "try:\n",
    "\n",
    "    keypointsHistory = deque()\n",
    "    predictionHistory = deque()\n",
    "    detectionThreshold = 1.0\n",
    "\n",
    "    lastPredictionTime = time()\n",
    "    predictionCooldown = 1\n",
    "\n",
    "    while True:\n",
    "        startTime = time()\n",
    "\n",
    "        # Read from camera\n",
    "        success, frame = cam.read()\n",
    "        if not success: \n",
    "            raise Exception(\"No Frames Read\")\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # Pose Detection\n",
    "        detectionResults, frame = featureExtraction(\n",
    "            handDetector, faceDetector, poseDetector, frame)\n",
    "        \n",
    "        # Semantic Prediction\n",
    "        keypointsHistory.append(detectionResults)\n",
    "        if len(keypointsHistory) > FRAMES_PER_TRAINING:\n",
    "            keypointsHistory.popleft()\n",
    "\n",
    "            \n",
    "            # if time() > lastPredictionTime + predictionCooldown:\n",
    "            #     predictionResults = model.predict(\n",
    "            #         np.expand_dims(keypointsHistory, axis=0), \n",
    "            #         verbose=0, \n",
    "            #         use_multiprocessing=True, \n",
    "            #         workers=4\n",
    "            #         )[0]\n",
    "            #     predWord = action_labels[np.argmax(predictionResults)]\n",
    "            #     predAccuracy = predictionResults[np.argmax(predictionResults)]\n",
    "\n",
    "            #     if predAccuracy >= detectionThreshold:\n",
    "            #         lastPredictionTime = time()\n",
    "                    \n",
    "            #         predictionHistory.append(predWord)\n",
    "            #         if len(predictionHistory) > 5:\n",
    "            #             predictionHistory.popleft()\n",
    "        \n",
    "        cv2.putText(frame, ', '.join(predictionHistory), (15, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 3)\n",
    "        \n",
    "        fps, frame = fpsReader.update(frame,pos=(950,80),color=(0,255,0),scale=5,thickness=5)\n",
    "\n",
    "\n",
    "        # Show resulting frame\n",
    "        # cv2.putText(frame, f'Training #{training + 1} for \\'{action}\\'', (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "        cv2.imshow(\"Sign Language Recognition Prototype\", frame)     \n",
    "\n",
    "        timeStats.append(time() - startTime)\n",
    "\n",
    "        keyPressed = cv2.waitKey(15)\n",
    "        # Stop Program when pressed 'Esc'\n",
    "        if (keyPressed == 27):\n",
    "            raise Exception(\"Finished\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(e)\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.07875680923461914,\n",
       " 0.07371330261230469,\n",
       " 0.0756688117980957,\n",
       " 0.07382822036743164,\n",
       " 0.07182097434997559,\n",
       " 0.14654779434204102,\n",
       " 0.13663458824157715,\n",
       " 0.13762927055358887,\n",
       " 0.13364148139953613,\n",
       " 0.173537015914917,\n",
       " 0.16253042221069336,\n",
       " 0.16656756401062012,\n",
       " 0.1545863151550293,\n",
       " 0.1565384864807129,\n",
       " 0.16048026084899902,\n",
       " 0.1515054702758789,\n",
       " 0.1635282039642334,\n",
       " 0.17353558540344238,\n",
       " 0.15356874465942383,\n",
       " 0.15557503700256348,\n",
       " 0.15255475044250488,\n",
       " 0.1715106964111328,\n",
       " 0.15854263305664062,\n",
       " 0.15158939361572266,\n",
       " 0.1595776081085205,\n",
       " 0.16945433616638184]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "timeStats[10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "# Detects hands, face & pose, \n",
    "# convert them into normalized landmark/keypoint coordinates in a 1D-array, \n",
    "# and also returns the frame with the landmark connections drawn onto it\n",
    "def featureExtractionV2(handDetector, faceDetector, poseDetector, frame):\n",
    "    results = {}\n",
    "    frameSize = (frame.shape[1], frame.shape[0])\n",
    "\n",
    "    def detectHands(frame, handDetector, frameSize):\n",
    "        results = {}\n",
    "        # Hand Detection\n",
    "        results['hands'], frame = handDetector.findHands(frame, draw=True)\n",
    "        if not results['hands']:\n",
    "            results['hands'] = [generate_empty_hand('Left'), generate_empty_hand('Right')]\n",
    "        elif len(results['hands']) == 1:\n",
    "            results['hands'][0] = preprocess_body_part(results['hands'][0], frameSize)\n",
    "            \n",
    "            if (results['hands'][0]['type'] == 'Left'):\n",
    "                results['hands'].append(generate_empty_hand('Right'))\n",
    "            else:\n",
    "                results['hands'].insert(0, generate_empty_hand('Left'))                         \n",
    "        else:\n",
    "            results['hands'][0] = preprocess_body_part(results['hands'][0], frameSize)\n",
    "            results['hands'][1] = preprocess_body_part(results['hands'][1], frameSize)\n",
    "        return results['hands']\n",
    "\n",
    "    def detectPose(frame, poseDetector, frameSize):\n",
    "        results = {}\n",
    "        # Pose Detection\n",
    "        # * We only use the first 23 out of the total 33 landmark points \n",
    "        #   as those represent the lower half body and are irrelevant\n",
    "        frame = poseDetector.findPose(frame, draw=True)\n",
    "        results['pose'] = {}\n",
    "        results['pose']['lmList'], tempPoseBbox = poseDetector.findPosition(frame, bboxWithHands=False)\n",
    "        if results['pose']['lmList'] and tempPoseBbox:\n",
    "            results['pose']['lmList'] = results['pose']['lmList'][:23]\n",
    "            results['pose']['bbox'] = tempPoseBbox['bbox']\n",
    "            results['pose']['center'] = tempPoseBbox['center']\n",
    "            results['pose'] = preprocess_body_part(results['pose'], frameSize)\n",
    "        else:\n",
    "            results['pose']['lmList'] = np.zeros(23 * 3, dtype=int)\n",
    "            results['pose']['bbox'] = np.zeros(4, dtype=float)\n",
    "            results['pose']['center'] = np.zeros(2, dtype=float)\n",
    "        return results['pose']\n",
    "            \n",
    "\n",
    "    def detectFace(frame, faceDetector, frameSize):\n",
    "        results = {}\n",
    "        # Face Detection\n",
    "        frame, results['face'] = faceDetector.findFaces(frame, draw=True)\n",
    "        if results['face']:\n",
    "            results['face'] = select_best_matching_face(results['face'], frameSize)\n",
    "            results['face']['bbox'] = preprocess_bbox(results['face']['bbox'], frameSize)\n",
    "            results['face']['center'] = preprocess_center(results['face']['center'], frameSize)\n",
    "        else:\n",
    "            results['face'] = {\n",
    "                'bbox': np.zeros(4, dtype=float), \n",
    "                'center': np.zeros(2, dtype=float)\n",
    "            }\n",
    "        return results['face']\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        # t1 = executor.submit(detectHands, frame, handDetector, frameSize)\n",
    "        # t2 = executor.submit(detectPose, frame, poseDetector, frameSize)\n",
    "        t3 = executor.submit(detectFace, frame, faceDetector, frameSize)\n",
    "\n",
    "\n",
    "        # Convert results into 1D-array\n",
    "        detectionResults = np.concatenate([\n",
    "            flattenDetectionResult(t1.result()[0]), \n",
    "            flattenDetectionResult(t1.result()[1]), \n",
    "            flattenDetectionResult(t2.result()), \n",
    "            t3.result()['bbox'], \n",
    "            t3.result()['center']\n",
    "        ])\n",
    "\n",
    "        return detectionResults, frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "cam = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "cam.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cam.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "fpsReader = FPS()\n",
    "\n",
    "timeStats = []\n",
    "\n",
    "try:\n",
    "    initialTime = time()\n",
    "    while True:\n",
    "        startTime = time()\n",
    "\n",
    "        # Read from camera\n",
    "        success, frame = cam.read()\n",
    "        \n",
    "        # Pose Detection\n",
    "        detectionResults, frame = featureExtractionV2(\n",
    "            handDetector, faceDetector, poseDetector, frame)\n",
    "        \n",
    "        # fps, frame = fpsReader.update(frame,pos=(50,80),color=(0,255,0),scale=5,thickness=5)\n",
    "\n",
    "\n",
    "        # Show resulting frame\n",
    "        # cv2.putText(frame, f'Training #{training + 1} for \\'{action}\\'', (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "        cv2.imshow(\"Sign Language Recognition Prototype\", frame)     \n",
    "\n",
    "        timeStats.append(time() - startTime)\n",
    "\n",
    "        keyPressed = cv2.waitKey(15)\n",
    "        # Stop Program when pressed 'Esc'\n",
    "        if (keyPressed == 27):\n",
    "            raise Exception(\"Finished\")\n",
    "        \n",
    "        # if time() - initialTime > 10:\n",
    "        #     raise Exception()\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(e)\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05556477281384002"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.array(timeStats[1:]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016924326368373075"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.array(timeStats[1:]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09001387300945464"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.array(timeStats[1:]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06750456676926724"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.array(timeStats[1:]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "cam.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cam.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "success, frame = cam.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "type(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
