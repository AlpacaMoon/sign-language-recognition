{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import Libraries / Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvzone.HandTrackingModule import HandDetector\n",
    "from cvzone.FaceDetectionModule import FaceDetector\n",
    "from cvzone.PoseModule import PoseDetector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Hand, Face and Pose Detection + Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten2dList(arr, dataType=int):\n",
    "    return np.fromiter(chain.from_iterable(arr), dataType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAbsLargestVal(arr):\n",
    "    return max(np.max(arr), abs(np.min(arr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_landmarks(landmark_list):\n",
    "    if not landmark_list:\n",
    "        return []\n",
    "    \n",
    "    # Offset every point with respect to the first point\n",
    "    new_landmark_list = []\n",
    "    origin_x = landmark_list[0][0] \n",
    "    origin_y = landmark_list[0][1]\n",
    "    origin_z = landmark_list[0][2]\n",
    "    for each in landmark_list:\n",
    "        updated_point = [\n",
    "            each[0] - origin_x, \n",
    "            each[1] - origin_y, \n",
    "            each[2] - origin_z\n",
    "        ]\n",
    "        new_landmark_list.append(updated_point)\n",
    "    \n",
    "    # Convert to 1D-array\n",
    "    new_landmark_list = flatten2dList(new_landmark_list)\n",
    "    \n",
    "    # Get highest absolute value\n",
    "    largest_value = getAbsLargestVal(new_landmark_list)\n",
    "    \n",
    "    # Normalization\n",
    "    return new_landmark_list / largest_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_bbox(bbox, frameSize):\n",
    "    bbox = np.array(bbox, dtype=float)\n",
    "    # Convert 3rd and 4th element into coordinates instead of width/height\n",
    "    bbox[2] = bbox[0] + bbox[2]\n",
    "    bbox[3] = bbox[1] + bbox[3]\n",
    "\n",
    "    # Normalize against frame size\n",
    "    bbox[0] /= frameSize[0]\n",
    "    bbox[2] /= frameSize[0]\n",
    "    bbox[1] /= frameSize[1]\n",
    "    bbox[3] /= frameSize[1]\n",
    "\n",
    "    return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_center(center, frameSize):\n",
    "    center = np.array(center)\n",
    "    center[0] /= frameSize[0]\n",
    "    center[1] /= frameSize[1]\n",
    "    return center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_body_part(bodyPart, frameSize):\n",
    "    bodyPart['lmList'] = preprocess_landmarks(bodyPart['lmList'])\n",
    "    bodyPart['bbox'] = preprocess_bbox(bodyPart['bbox'], frameSize)\n",
    "    bodyPart['center'] = preprocess_center(bodyPart['center'], frameSize)\n",
    "    return bodyPart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_empty_hand(type):\n",
    "    return {\n",
    "        'lmList': np.zeros(21 * 3, dtype=int), \n",
    "        'bbox': np.zeros(4, dtype=float), \n",
    "        'center': np.zeros(2, dtype=float), \n",
    "        'type': type\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_matching_face(faces, frameSize):\n",
    "    if not faces:\n",
    "        return False\n",
    "    elif len(faces) == 1:\n",
    "        return faces[0]\n",
    "    \n",
    "    def difference(a, b):\n",
    "        return (a[0] - b[0])**2 + (a[1] - b[1])**2\n",
    "    \n",
    "    frameCenter = (frameSize[0] / 2, frameSize[1] / 2)\n",
    "\n",
    "    best_score = faces[0]\n",
    "    best_center = faces[0]\n",
    "    center_diff = difference(faces[0]['center'], frameCenter)\n",
    "\n",
    "    for each in faces:\n",
    "        if difference(each['center'], frameCenter) < center_diff:\n",
    "            best_center = each\n",
    "        if each['score'][0] > best_score['score'][0]:\n",
    "            best_score = each\n",
    "    \n",
    "    if best_center['score'][0] > 0.5:\n",
    "        return best_center\n",
    "    return best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flattenDetectionResult(obj):\n",
    "    return np.concatenate([obj['lmList'], obj['bbox'], obj['center']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prototype of pose detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "# Detectors\n",
    "handDetector = HandDetector(detectionCon=0.5, maxHands=2)\n",
    "faceDetector = FaceDetector(minDetectionCon=0.5)\n",
    "poseDetector = PoseDetector(detectionCon=0.5)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Read from camera\n",
    "        success, frame = cam.read()\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        results = {}\n",
    "        frameSize = (frame.shape[1], frame.shape[0])\n",
    "\n",
    "        # Hand Detection\n",
    "        results['hands'], frame = handDetector.findHands(frame, draw=True)\n",
    "        if not results['hands']:\n",
    "            results['hands'] = [generate_empty_hand('Left'), generate_empty_hand('Right')]\n",
    "        elif len(results['hands']) == 1:\n",
    "            if (results['hands'][0]['type'] == 'Left'):\n",
    "                results['hands'][0] = preprocess_body_part(results['hands'][0], frameSize)\n",
    "                results['hands'].append(generate_empty_hand('Right'))\n",
    "            else:\n",
    "                results['hands'][0] = preprocess_body_part(results['hands'][0], frameSize)\n",
    "                results['hands'].insert(0, generate_empty_hand('Left'))                         \n",
    "        else:\n",
    "            results['hands'][0] = preprocess_body_part(results['hands'][0], frameSize)\n",
    "            results['hands'][1] = preprocess_body_part(results['hands'][1], frameSize)\n",
    "\n",
    "        # Pose Detection\n",
    "        # * We only use the first 23 out of the total 33 landmark points \n",
    "        #   as those represent the lower half body and are irrelevant\n",
    "        frame = poseDetector.findPose(frame, draw=True)\n",
    "        results['pose'] = {}\n",
    "        results['pose']['lmList'], tempPoseBbox = poseDetector.findPosition(frame, bboxWithHands=False)\n",
    "        if results['pose']['lmList'] and tempPoseBbox:\n",
    "            results['pose']['lmList'] = results['pose']['lmList'][:23]\n",
    "            results['pose']['bbox'] = tempPoseBbox['bbox']\n",
    "            results['pose']['center'] = tempPoseBbox['center']\n",
    "            results['pose'] = preprocess_body_part(results['pose'], frameSize)\n",
    "        else:\n",
    "            results['pose']['lmList'] = np.zeros(23 * 3, dtype=int)\n",
    "            results['pose']['bbox'] = np.zeros(4, dtype=float)\n",
    "            results['pose']['center'] = np.zeros(2, dtype=float)\n",
    "            \n",
    "        # Face Detection\n",
    "        frame, results['face'] = faceDetector.findFaces(frame, draw=True)\n",
    "        if results['face']:\n",
    "            results['face'] = select_best_matching_face(results['face'], frameSize)\n",
    "            results['face']['bbox'] = preprocess_bbox(results['face']['bbox'], frameSize)\n",
    "            results['face']['center'] = preprocess_center(results['face']['center'], frameSize)\n",
    "        else:\n",
    "            results['face'] = {\n",
    "                'bbox': np.zeros(4, dtype=float), \n",
    "                'center': np.zeros(2, dtype=float)\n",
    "            }\n",
    "\n",
    "        # Convert results into 1D-array\n",
    "        detectionResults = np.concatenate([\n",
    "            flattenDetectionResult(results['hands'][0]), \n",
    "            flattenDetectionResult(results['hands'][1]), \n",
    "            flattenDetectionResult(results['pose']), \n",
    "            results['face']['bbox'], \n",
    "            results['face']['center']\n",
    "        ])\n",
    "\n",
    "        # Show frame\n",
    "        cv2.imshow(\"Sign Language Recognition Prototype\", frame)\n",
    "\n",
    "        keyPressed = cv2.waitKey(10)\n",
    "        if (keyPressed == ord('q')):\n",
    "            pass\n",
    "\n",
    "        # Pressed 'Esc'\n",
    "        if (keyPressed == 27):\n",
    "            break\n",
    "\n",
    "except Exception as e:\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    raise e\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each output is now a 219 length 1D-array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.25      , -0.09558824, -0.05147059,\n",
       "        0.44852941, -0.28676471, -0.02941176,  0.54411765, -0.45588235,\n",
       "       -0.01470588,  0.52205882, -0.58088235,  0.        ,  0.38235294,\n",
       "       -0.51470588,  0.07352941,  0.47794118, -0.71323529,  0.04411765,\n",
       "        0.54411765, -0.82352941,  0.        ,  0.60294118, -0.90441176,\n",
       "       -0.02941176,  0.26470588, -0.55882353,  0.07352941,  0.34558824,\n",
       "       -0.76470588,  0.04411765,  0.39705882, -0.89705882,  0.        ,\n",
       "        0.44852941, -1.        , -0.03676471,  0.14705882, -0.55882353,\n",
       "        0.05147059,  0.21323529, -0.75735294,  0.01470588,  0.25735294,\n",
       "       -0.88970588, -0.01470588,  0.29411765, -0.99264706, -0.03676471,\n",
       "        0.00735294, -0.52205882,  0.02205882,  0.05147059, -0.67647059,\n",
       "       -0.00735294,  0.07352941, -0.77941176, -0.01470588,  0.09558824,\n",
       "       -0.875     , -0.02205882,  0.1625    ,  0.57291667,  0.290625  ,\n",
       "        0.85625   ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.00409836,  0.04918033, -0.10245902,  0.00819672,\n",
       "        0.09016393, -0.10655738,  0.01229508,  0.12704918, -0.10245902,\n",
       "        0.01639344, -0.07786885, -0.0942623 ,  0.0204918 , -0.11885246,\n",
       "       -0.08606557,  0.02459016, -0.15163934, -0.07786885,  0.02868852,\n",
       "        0.18442623, -0.05327869,  0.03278689, -0.19262295, -0.01229508,\n",
       "        0.03688525,  0.08606557,  0.11885246,  0.04098361, -0.06967213,\n",
       "        0.12704918,  0.04508197,  0.54918033,  0.45081967,  0.04918033,\n",
       "       -0.41803279,  0.45491803,  0.05327869,  0.87704918,  1.        ,\n",
       "        0.05737705, -0.91393443,  0.86065574,  0.06147541,  0.82786885,\n",
       "        0.97540984,  0.06557377, -0.73360656,  0.35245902,  0.06967213,\n",
       "        0.8442623 ,  0.99180328,  0.07377049, -0.70901639,  0.12295082,\n",
       "        0.07786885,  0.80327869,  0.90983607,  0.08196721, -0.67622951,\n",
       "        0.08606557,  0.08606557,  0.7704918 ,  0.9057377 ,  0.09016393,\n",
       "       -0.65163934,  0.17213115,  0.1125    ,  0.45416667,  0.85      ,\n",
       "        2.95625   ,  0.        ,  1.        ,  0.35      ,  0.63333333,\n",
       "        0.546875  ,  0.89583333,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detectionResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Preparation for Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'thank you', 'help']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ACTION_LABELS_PATH = os.path.join('../action-recognition/action-labels.csv')\n",
    "action_labels = []\n",
    "with open(ACTION_LABELS_PATH) as f:\n",
    "    csv_reader = csv.reader(f, delimiter=\",\")\n",
    "    action_labels = [each[1] for each in csv_reader]\n",
    "action_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data paths\n",
    "KEYPOINTS_PATH = os.path.join(\"../action-recognition/keypoints_data\")\n",
    "for action in action_labels:\n",
    "    if not os.path.exists(os.path.join(KEYPOINTS_PATH, action)):\n",
    "        os.makedirs(os.path.join(KEYPOINTS_PATH, action))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detects hands, face & pose, \n",
    "# convert them into normalized landmark/keypoint coordinates in a 1D-array, \n",
    "# and also returns the frame with the landmark connections drawn onto it\n",
    "def featureExtraction(handDetector, faceDetector, poseDetector, frame):\n",
    "    results = {}\n",
    "    frameSize = (frame.shape[1], frame.shape[0])\n",
    "\n",
    "    # Hand Detection\n",
    "    results['hands'], frame = handDetector.findHands(frame, draw=True)\n",
    "    if not results['hands']:\n",
    "        results['hands'] = [generate_empty_hand('Left'), generate_empty_hand('Right')]\n",
    "    elif len(results['hands']) == 1:\n",
    "        if (results['hands'][0]['type'] == 'Left'):\n",
    "            results['hands'][0] = preprocess_body_part(results['hands'][0], frameSize)\n",
    "            results['hands'].append(generate_empty_hand('Right'))\n",
    "        else:\n",
    "            results['hands'][0] = preprocess_body_part(results['hands'][0], frameSize)\n",
    "            results['hands'].insert(0, generate_empty_hand('Left'))                         \n",
    "    else:\n",
    "        results['hands'][0] = preprocess_body_part(results['hands'][0], frameSize)\n",
    "        results['hands'][1] = preprocess_body_part(results['hands'][1], frameSize)\n",
    "\n",
    "    # Pose Detection\n",
    "    # * We only use the first 23 out of the total 33 landmark points \n",
    "    #   as those represent the lower half body and are irrelevant\n",
    "    frame = poseDetector.findPose(frame, draw=True)\n",
    "    results['pose'] = {}\n",
    "    results['pose']['lmList'], tempPoseBbox = poseDetector.findPosition(frame, bboxWithHands=False)\n",
    "    if results['pose']['lmList'] and tempPoseBbox:\n",
    "        results['pose']['lmList'] = results['pose']['lmList'][:23]\n",
    "        results['pose']['bbox'] = tempPoseBbox['bbox']\n",
    "        results['pose']['center'] = tempPoseBbox['center']\n",
    "        results['pose'] = preprocess_body_part(results['pose'], frameSize)\n",
    "    else:\n",
    "        results['pose']['lmList'] = np.zeros(23 * 3, dtype=int)\n",
    "        results['pose']['bbox'] = np.zeros(4, dtype=float)\n",
    "        results['pose']['center'] = np.zeros(2, dtype=float)\n",
    "        \n",
    "    # Face Detection\n",
    "    frame, results['face'] = faceDetector.findFaces(frame, draw=True)\n",
    "    if results['face']:\n",
    "        results['face'] = select_best_matching_face(results['face'], frameSize)\n",
    "        results['face']['bbox'] = preprocess_bbox(results['face']['bbox'], frameSize)\n",
    "        results['face']['center'] = preprocess_center(results['face']['center'], frameSize)\n",
    "    else:\n",
    "        results['face'] = {\n",
    "            'bbox': np.zeros(4, dtype=float), \n",
    "            'center': np.zeros(2, dtype=float)\n",
    "        }\n",
    "\n",
    "    # Convert results into 1D-array\n",
    "    detectionResults = np.concatenate([\n",
    "        flattenDetectionResult(results['hands'][0]), \n",
    "        flattenDetectionResult(results['hands'][1]), \n",
    "        flattenDetectionResult(results['pose']), \n",
    "        results['face']['bbox'], \n",
    "        results['face']['center']\n",
    "    ])\n",
    "\n",
    "    return detectionResults, frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many video records / training data each label should have\n",
    "trainings_per_label = 100\n",
    "\n",
    "# How many frames each video record / training data should have\n",
    "frames_per_training = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def readFrame():\n",
    "    success, frame = cam.read()\n",
    "    if not success: \n",
    "        raise Exception(\"No Frames Read\")\n",
    "    return cv2.flip(frame, 1)\n",
    "    \n",
    "\n",
    "def countdownFromThree(trainingNum, actionStr):\n",
    "    # Count down 3 seconds on every new training\n",
    "    for i in range(3):\n",
    "        for _ in range(10):\n",
    "            frame = readFrame()\n",
    "            cv2.putText(frame, f'Training #{trainingNum + 1} for \\'{actionStr}\\'', (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "            cv2.putText(frame, f'Next Training in {3 - i}', (20, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 3)\n",
    "            cv2.imshow(\"Sign Language Recognition Prototype\", frame)\n",
    "            tempKey = cv2.waitKey(100)\n",
    "            if (tempKey == 27):\n",
    "                raise Exception(\"Finished\")\n",
    "            elif tempKey == 32:\n",
    "                pauseWhenSpace(trainingNum, actionStr)\n",
    "                return\n",
    "\n",
    "def pauseWhenSpace(trainingNum, actionStr):\n",
    "    while True:\n",
    "        frame = readFrame()\n",
    "        cv2.putText(frame, f'Training #{trainingNum + 1} for \\'{actionStr}\\'', (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "        cv2.putText(frame, f'Pausing...', (40, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (20, 255, 125), 3)\n",
    "        cv2.imshow(\"Sign Language Recognition Prototype\", frame)\n",
    "\n",
    "        # If pressed resume, do countdown\n",
    "        keyPressed = cv2.waitKey(100)\n",
    "        if keyPressed == 32:\n",
    "            resume = False\n",
    "            for i in range(3):\n",
    "                for _ in range(10):\n",
    "                    temp_frame = readFrame()\n",
    "                    cv2.putText(temp_frame, f'Training #{trainingNum + 1} for \\'{actionStr}\\'', (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "                    cv2.putText(temp_frame, f'Resuming in {3 - i}', (20, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 120), 3)\n",
    "                    cv2.imshow(\"Sign Language Recognition Prototype\", temp_frame)\n",
    "                    tempKey = cv2.waitKey(100)\n",
    "                    if (tempKey == 27):\n",
    "                        raise Exception(\"Finished\")\n",
    "                    # If pressed paused again, stop resuming and continue pausing\n",
    "                    elif tempKey == 32:\n",
    "                        resume = True\n",
    "                        break\n",
    "                if resume:\n",
    "                    break\n",
    "            if not resume:\n",
    "                return\n",
    "            \n",
    "        elif keyPressed == 27:\n",
    "            raise Exception(\"Finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "cam = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "cam.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cam.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "# Detectors\n",
    "handDetector = HandDetector(detectionCon=0.5, maxHands=2)\n",
    "faceDetector = FaceDetector(minDetectionCon=0.5)\n",
    "poseDetector = PoseDetector(detectionCon=0.5)\n",
    "\n",
    "from time import time\n",
    "startTime = time()\n",
    "\n",
    "try:\n",
    "    action = action_labels[2]\n",
    "\n",
    "    for training in range(trainings_per_label): \n",
    "        trainingResults = []\n",
    "        for frame_num in range(frames_per_training):\n",
    "\n",
    "            # Additional GUI\n",
    "            if frame_num == 0:\n",
    "                countdownFromThree(training, action)\n",
    "                startTime = time()\n",
    "        \n",
    "            # Read from camera\n",
    "            frame = readFrame()\n",
    "\n",
    "            detectionResults, frame = featureExtraction(\n",
    "                handDetector, faceDetector, poseDetector, frame)\n",
    "            \n",
    "            # Show resulting frame\n",
    "            cv2.putText(frame, f'Training #{training + 1} for \\'{action}\\'', (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "            cv2.imshow(\"Sign Language Recognition Prototype\", frame)     \n",
    "\n",
    "            # Save the results\n",
    "            trainingResults.append(detectionResults)\n",
    "\n",
    "            keyPressed = cv2.waitKey(10)\n",
    "            # Stop Program when pressed 'Esc'\n",
    "            if (keyPressed == 27):\n",
    "                raise Exception(\"Finished\")\n",
    "\n",
    "        # After all frames are finished for each training:\n",
    "        # save as .npy\n",
    "        \n",
    "        # IMPORTANT: THIS LINE IS DISABLED IN CASE OF ACCIDENTALLY OVERWRITING DATA\n",
    "        # Renable it ONLY during data collection\n",
    "        # np.save(os.path.join(KEYPOINTS_PATH, action, str(training)), np.array(trainingResults))\n",
    "\n",
    "except Exception as e:\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(e)\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 219)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(os.path.join(KEYPOINTS_PATH, action, '0.npy')).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Recognition Model Training & Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': '0', 'thank you': '1', 'help': '2'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_mapping = {}\n",
    "with open(ACTION_LABELS_PATH) as f:\n",
    "    csv_reader = csv.reader(f, delimiter=\",\")\n",
    "    action_mapping = {each[1]: each[0] for each in csv_reader}\n",
    "    \n",
    "action_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = [], []\n",
    "\n",
    "for action in action_labels:\n",
    "    for trainingNum in range(trainings_per_label):\n",
    "        sequence = np.load(os.path.join(KEYPOINTS_PATH, action, f\"{trainingNum}.npy\"))\n",
    "        features.append(sequence)\n",
    "        labels.append(action_mapping[action])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 15, 219)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(features)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = to_categorical(labels).astype(int)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7884000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.nbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(225, 15, 219)\n",
      "(75, 15, 219)\n",
      "(225, 3)\n",
      "(75, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Model Compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_PATH = os.path.join(\"log\")\n",
    "tb_callback = TensorBoard(log_dir=LOG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 219)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelInputSize = (X.shape[1], X.shape[2])\n",
    "modelInputSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(units=64, activation='relu', return_sequences=True, input_shape=modelInputSize))\n",
    "model.add(LSTM(units=128, activation='relu', return_sequences=True))\n",
    "model.add(LSTM(units=64, activation='relu', return_sequences=False))\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(units=y.shape[1], activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "8/8 [==============================] - 5s 26ms/step - loss: 1.0369 - categorical_accuracy: 0.4356\n",
      "Epoch 2/30\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 0.5446 - categorical_accuracy: 0.8800\n",
      "Epoch 3/30\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.3801 - categorical_accuracy: 0.8756\n",
      "Epoch 4/30\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.3209 - categorical_accuracy: 0.9378\n",
      "Epoch 5/30\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.2694 - categorical_accuracy: 0.9378\n",
      "Epoch 6/30\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.6229 - categorical_accuracy: 0.9111\n",
      "Epoch 7/30\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.2242 - categorical_accuracy: 0.9600\n",
      "Epoch 8/30\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.1236 - categorical_accuracy: 0.9911\n",
      "Epoch 9/30\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.0277 - categorical_accuracy: 0.9956\n",
      "Epoch 10/30\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.0164 - categorical_accuracy: 0.9956\n",
      "Epoch 11/30\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.0225 - categorical_accuracy: 0.9911\n",
      "Epoch 12/30\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.9712 - categorical_accuracy: 0.8756\n",
      "Epoch 13/30\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.1537 - categorical_accuracy: 0.9511\n",
      "Epoch 14/30\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.1563 - categorical_accuracy: 0.9644\n",
      "Epoch 15/30\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.1155 - categorical_accuracy: 0.9778\n",
      "Epoch 16/30\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.0787 - categorical_accuracy: 0.9822\n",
      "Epoch 17/30\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 0.0331 - categorical_accuracy: 0.9911\n",
      "Epoch 18/30\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.0121 - categorical_accuracy: 0.9956\n",
      "Epoch 19/30\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.0058 - categorical_accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.0029 - categorical_accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.0014 - categorical_accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 6.9683e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 4.6501e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 3.3077e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 2.4641e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 2.0439e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.5943e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.3734e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.1602e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.0018e-04 - categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20fe44aa490>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=30, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = os.path.join(\"../action-recognition/models\")\n",
    "model.load_weights(os.path.join(MODEL_PATH, 'model_1.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_3 (LSTM)               (None, 15, 64)            72704     \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 15, 128)           98816     \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 227,267\n",
      "Trainable params: 227,267\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "test_results = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = np.argmax(y_test, axis=1)\n",
    "y_predict = np.argmax(test_results, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_actual, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[47,  0],\n",
       "        [ 0, 28]],\n",
       "\n",
       "       [[53,  0],\n",
       "        [ 0, 22]],\n",
       "\n",
       "       [[50,  0],\n",
       "        [ 0, 25]]], dtype=int64)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_confusion_matrix(y_actual, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "cm = confusion_matrix(y_actual, y_predict)\n",
    "cmDisp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "cmDisp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Saving Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = os.path.join(\"../action-recognition/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join(MODEL_PATH, \"model_1.h5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Time Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from time import time\n",
    "from cvzone import FPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "cam = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "cam.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cam.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "# Detectors\n",
    "handDetector = HandDetector(detectionCon=0.5, maxHands=2)\n",
    "faceDetector = FaceDetector(minDetectionCon=0.5)\n",
    "poseDetector = PoseDetector(detectionCon=0.5)\n",
    "\n",
    "fpsReader = FPS()\n",
    "\n",
    "timeStats = []\n",
    "\n",
    "try:\n",
    "\n",
    "    keypointsHistory = deque()\n",
    "    predictionHistory = deque()\n",
    "    detectionThreshold = 1.0\n",
    "\n",
    "    lastPredictionTime = time()\n",
    "    predictionCooldown = 1\n",
    "\n",
    "    while True:\n",
    "        startTime = time()\n",
    "\n",
    "        # Read from camera\n",
    "        success, frame = cam.read()\n",
    "        if not success: \n",
    "            raise Exception(\"No Frames Read\")\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # Pose Detection\n",
    "        detectionResults, frame = featureExtraction(\n",
    "            handDetector, faceDetector, poseDetector, frame)\n",
    "        \n",
    "        # Semantic Prediction\n",
    "        keypointsHistory.append(detectionResults)\n",
    "        if len(keypointsHistory) > frames_per_training:\n",
    "            keypointsHistory.popleft()\n",
    "\n",
    "            \n",
    "            # if time() > lastPredictionTime + predictionCooldown:\n",
    "            #     predictionResults = model.predict(\n",
    "            #         np.expand_dims(keypointsHistory, axis=0), \n",
    "            #         verbose=0, \n",
    "            #         use_multiprocessing=True, \n",
    "            #         workers=4\n",
    "            #         )[0]\n",
    "            #     predWord = action_labels[np.argmax(predictionResults)]\n",
    "            #     predAccuracy = predictionResults[np.argmax(predictionResults)]\n",
    "\n",
    "            #     if predAccuracy >= detectionThreshold:\n",
    "            #         lastPredictionTime = time()\n",
    "                    \n",
    "            #         predictionHistory.append(predWord)\n",
    "            #         if len(predictionHistory) > 5:\n",
    "            #             predictionHistory.popleft()\n",
    "        \n",
    "        cv2.putText(frame, ', '.join(predictionHistory), (15, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 3)\n",
    "        \n",
    "        fps, frame = fpsReader.update(frame,pos=(950,80),color=(0,255,0),scale=5,thickness=5)\n",
    "\n",
    "\n",
    "        # Show resulting frame\n",
    "        # cv2.putText(frame, f'Training #{training + 1} for \\'{action}\\'', (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "        cv2.imshow(\"Sign Language Recognition Prototype\", frame)     \n",
    "\n",
    "        timeStats.append(time() - startTime)\n",
    "\n",
    "        keyPressed = cv2.waitKey(15)\n",
    "        # Stop Program when pressed 'Esc'\n",
    "        if (keyPressed == 27):\n",
    "            raise Exception(\"Finished\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(e)\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.07875680923461914,\n",
       " 0.07371330261230469,\n",
       " 0.0756688117980957,\n",
       " 0.07382822036743164,\n",
       " 0.07182097434997559,\n",
       " 0.14654779434204102,\n",
       " 0.13663458824157715,\n",
       " 0.13762927055358887,\n",
       " 0.13364148139953613,\n",
       " 0.173537015914917,\n",
       " 0.16253042221069336,\n",
       " 0.16656756401062012,\n",
       " 0.1545863151550293,\n",
       " 0.1565384864807129,\n",
       " 0.16048026084899902,\n",
       " 0.1515054702758789,\n",
       " 0.1635282039642334,\n",
       " 0.17353558540344238,\n",
       " 0.15356874465942383,\n",
       " 0.15557503700256348,\n",
       " 0.15255475044250488,\n",
       " 0.1715106964111328,\n",
       " 0.15854263305664062,\n",
       " 0.15158939361572266,\n",
       " 0.1595776081085205,\n",
       " 0.16945433616638184]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeStats[10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "# Detects hands, face & pose, \n",
    "# convert them into normalized landmark/keypoint coordinates in a 1D-array, \n",
    "# and also returns the frame with the landmark connections drawn onto it\n",
    "def featureExtractionV2(handDetector, faceDetector, poseDetector, frame):\n",
    "    results = {}\n",
    "    frameSize = (frame.shape[1], frame.shape[0])\n",
    "\n",
    "    def detectHands(frame, handDetector, frameSize):\n",
    "        results = {}\n",
    "        # Hand Detection\n",
    "        results['hands'], frame = handDetector.findHands(frame, draw=True)\n",
    "        if not results['hands']:\n",
    "            results['hands'] = [generate_empty_hand('Left'), generate_empty_hand('Right')]\n",
    "        elif len(results['hands']) == 1:\n",
    "            results['hands'][0] = preprocess_body_part(results['hands'][0], frameSize)\n",
    "            \n",
    "            if (results['hands'][0]['type'] == 'Left'):\n",
    "                results['hands'].append(generate_empty_hand('Right'))\n",
    "            else:\n",
    "                results['hands'].insert(0, generate_empty_hand('Left'))                         \n",
    "        else:\n",
    "            results['hands'][0] = preprocess_body_part(results['hands'][0], frameSize)\n",
    "            results['hands'][1] = preprocess_body_part(results['hands'][1], frameSize)\n",
    "        return results['hands']\n",
    "\n",
    "    def detectPose(frame, poseDetector, frameSize):\n",
    "        results = {}\n",
    "        # Pose Detection\n",
    "        # * We only use the first 23 out of the total 33 landmark points \n",
    "        #   as those represent the lower half body and are irrelevant\n",
    "        frame = poseDetector.findPose(frame, draw=True)\n",
    "        results['pose'] = {}\n",
    "        results['pose']['lmList'], tempPoseBbox = poseDetector.findPosition(frame, bboxWithHands=False)\n",
    "        if results['pose']['lmList'] and tempPoseBbox:\n",
    "            results['pose']['lmList'] = results['pose']['lmList'][:23]\n",
    "            results['pose']['bbox'] = tempPoseBbox['bbox']\n",
    "            results['pose']['center'] = tempPoseBbox['center']\n",
    "            results['pose'] = preprocess_body_part(results['pose'], frameSize)\n",
    "        else:\n",
    "            results['pose']['lmList'] = np.zeros(23 * 3, dtype=int)\n",
    "            results['pose']['bbox'] = np.zeros(4, dtype=float)\n",
    "            results['pose']['center'] = np.zeros(2, dtype=float)\n",
    "        return results['pose']\n",
    "            \n",
    "\n",
    "    def detectFace(frame, faceDetector, frameSize):\n",
    "        results = {}\n",
    "        # Face Detection\n",
    "        frame, results['face'] = faceDetector.findFaces(frame, draw=True)\n",
    "        if results['face']:\n",
    "            results['face'] = select_best_matching_face(results['face'], frameSize)\n",
    "            results['face']['bbox'] = preprocess_bbox(results['face']['bbox'], frameSize)\n",
    "            results['face']['center'] = preprocess_center(results['face']['center'], frameSize)\n",
    "        else:\n",
    "            results['face'] = {\n",
    "                'bbox': np.zeros(4, dtype=float), \n",
    "                'center': np.zeros(2, dtype=float)\n",
    "            }\n",
    "        return results['face']\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        t1 = executor.submit(detectHands, frame, handDetector, frameSize)\n",
    "        t2 = executor.submit(detectPose, frame, poseDetector, frameSize)\n",
    "        t3 = executor.submit(detectFace, frame, faceDetector, frameSize)\n",
    "\n",
    "\n",
    "        # Convert results into 1D-array\n",
    "        detectionResults = np.concatenate([\n",
    "            flattenDetectionResult(t1.result()[0]), \n",
    "            flattenDetectionResult(t1.result()[1]), \n",
    "            flattenDetectionResult(t2.result()), \n",
    "            t3.result()['bbox'], \n",
    "            t3.result()['center']\n",
    "        ])\n",
    "\n",
    "        return detectionResults, frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "cam = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "cam.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cam.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "fpsReader = FPS()\n",
    "\n",
    "timeStats = []\n",
    "\n",
    "try:\n",
    "    initialTime = time()\n",
    "    while True:\n",
    "        startTime = time()\n",
    "\n",
    "        # Read from camera\n",
    "        success, frame = cam.read()\n",
    "        \n",
    "        # Pose Detection\n",
    "        detectionResults, frame = featureExtractionV2(\n",
    "            handDetector, faceDetector, poseDetector, frame)\n",
    "        \n",
    "        fps, frame = fpsReader.update(frame,pos=(50,80),color=(0,255,0),scale=5,thickness=5)\n",
    "\n",
    "\n",
    "        # Show resulting frame\n",
    "        # cv2.putText(frame, f'Training #{training + 1} for \\'{action}\\'', (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "        cv2.imshow(\"Sign Language Recognition Prototype\", frame)     \n",
    "\n",
    "        timeStats.append(time() - startTime)\n",
    "\n",
    "        keyPressed = cv2.waitKey(15)\n",
    "        # Stop Program when pressed 'Esc'\n",
    "        if (keyPressed == 27):\n",
    "            raise Exception(\"Finished\")\n",
    "        \n",
    "        if time() - initialTime > 5:\n",
    "            raise Exception()\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(e)\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016924326368373075"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(timeStats[1:]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09001387300945464"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(timeStats[1:]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06750456676926724"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(timeStats[1:]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
